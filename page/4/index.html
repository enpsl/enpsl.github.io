<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 6.3.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"enpsl.github.io",root:"/",scheme:"Muse",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!1,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="my blog"><meta property="og:type" content="website"><meta property="og:title" content="彭诗亮的博客"><meta property="og:url" content="https://enpsl.github.io/page/4/index.html"><meta property="og:site_name" content="彭诗亮的博客"><meta property="og:description" content="my blog"><meta property="og:locale" content="zh_CN"><meta property="article:author" content="enpsl"><meta property="article:tag" content="彭诗亮 psl pengshiliang blog"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://enpsl.github.io/page/4/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!0,isPost:!1,lang:"zh-CN"}</script><title>彭诗亮的博客</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">彭诗亮的博客</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i> 首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i> 归档</a></li></ul></nav></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content index posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://enpsl.github.io/2021/03/02/2021-03-02-vagrant/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="enpsl"><meta itemprop="description" content="my blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="彭诗亮的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"> <a href="/2021/03/02/2021-03-02-vagrant/" class="post-title-link" itemprop="url">Vagrant基本命令</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-03-02 21:30:00" itemprop="dateCreated datePublished" datetime="2021-03-02T21:30:00+08:00">2021-03-02</time></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="vagrant-使用"><a href="#vagrant-使用" class="headerlink" title="vagrant 使用"></a>vagrant 使用</h2><h2 id="init-centos"><a href="#init-centos" class="headerlink" title="init centos"></a>init centos</h2><p>centos7 box 下载地址<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1kVlAz59">centos7</a></p><p><strong>添加vagrant box到box list</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant box add centos7 Vagrant-CentOS-7.box</span><br></pre></td></tr></table></figure><p><strong>初始化一个虚拟机使用刚才添加的vagrant box</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> centos</span><br><span class="line"><span class="built_in">cd</span> centos</span><br><span class="line">vim Vagrantfile</span><br></pre></td></tr></table></figure><p><strong>添加下面内容到Vagrantfile中</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- mode: ruby -*-</span></span><br><span class="line"><span class="comment"># vi: set ft=ruby :</span></span><br><span class="line"></span><br><span class="line">Vagrant.require_version <span class="string">&quot;&gt;= 1.6.0&quot;</span></span><br><span class="line"></span><br><span class="line">boxes = [</span><br><span class="line">    &#123;</span><br><span class="line">        :name =&gt; <span class="string">&quot;docker-node1&quot;</span>,</span><br><span class="line">        :eth1 =&gt; <span class="string">&quot;192.168.205.10&quot;</span>,</span><br><span class="line">        :mem =&gt; <span class="string">&quot;1024&quot;</span>,</span><br><span class="line">        :cpu =&gt; <span class="string">&quot;1&quot;</span></span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        :name =&gt; <span class="string">&quot;docker-node2&quot;</span>,</span><br><span class="line">        :eth1 =&gt; <span class="string">&quot;192.168.205.11&quot;</span>,</span><br><span class="line">        :mem =&gt; <span class="string">&quot;1024&quot;</span>,</span><br><span class="line">        :cpu =&gt; <span class="string">&quot;1&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">Vagrant.configure(2) <span class="keyword">do</span> |config|</span><br><span class="line"></span><br><span class="line">  config.vm.box = <span class="string">&quot;centos7&quot;</span></span><br><span class="line"></span><br><span class="line">  boxes.each <span class="keyword">do</span> |opts|</span><br><span class="line">      config.vm.define opts[:name] <span class="keyword">do</span> |config|</span><br><span class="line">        config.vm.hostname = opts[:name]</span><br><span class="line">        config.vm.provider <span class="string">&quot;vmware_fusion&quot;</span> <span class="keyword">do</span> |v|</span><br><span class="line">          v.vmx[<span class="string">&quot;memsize&quot;</span>] = opts[:mem]</span><br><span class="line">          v.vmx[<span class="string">&quot;numvcpus&quot;</span>] = opts[:cpu]</span><br><span class="line">        end</span><br><span class="line"></span><br><span class="line">        config.vm.provider <span class="string">&quot;virtualbox&quot;</span> <span class="keyword">do</span> |v|</span><br><span class="line">          v.customize [<span class="string">&quot;modifyvm&quot;</span>, :<span class="built_in">id</span>, <span class="string">&quot;--memory&quot;</span>, opts[:mem]]</span><br><span class="line">          v.customize [<span class="string">&quot;modifyvm&quot;</span>, :<span class="built_in">id</span>, <span class="string">&quot;--cpus&quot;</span>, opts[:cpu]]</span><br><span class="line">        end</span><br><span class="line"></span><br><span class="line">        config.vm.network :private_network, ip: opts[:eth1]</span><br><span class="line">      end</span><br><span class="line">  end</span><br><span class="line"></span><br><span class="line">  config.vm.provision <span class="string">&quot;shell&quot;</span>, privileged: <span class="literal">true</span>, path: <span class="string">&quot;./setup.sh&quot;</span></span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>install docker的setup.sh文件</strong></p><p>在当前目录创建setup.sh文件并添加如下内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#/bin/sh</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># install some tools</span></span><br><span class="line">sudo yum install -y git vim gcc glibc-static telnet bridge-utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># install docker</span></span><br><span class="line">curl -fsSL get.docker.com -o get-docker.sh</span><br><span class="line">sh get-docker.sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># start docker service</span></span><br><span class="line">sudo groupadd docker</span><br><span class="line">sudo usermod -aG docker vagrant</span><br><span class="line">sudo systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="built_in">rm</span> -rf get-docker.sh</span><br></pre></td></tr></table></figure><p><strong>启动安装</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br></pre></td></tr></table></figure><h2 id="vagrant-报unknown-filesystem-type-‘vboxsf’-解决方案"><a href="#vagrant-报unknown-filesystem-type-‘vboxsf’-解决方案" class="headerlink" title="vagrant 报unknown filesystem type ‘vboxsf’ 解决方案"></a>vagrant 报unknown filesystem type ‘vboxsf’ 解决方案</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vagrant plugin install vagrant-vbguest</span><br><span class="line">vagrant destroy &amp;&amp; vagrant up</span><br></pre></td></tr></table></figure><h2 id="init-ubuntu"><a href="#init-ubuntu" class="headerlink" title="init ubuntu"></a>init ubuntu</h2><p>使用<a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/ubuntu-cloud-images/bionic/current/">清华源</a></p><p><strong>ubuntu18的box，终端运行如下命令</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vagrant box add \</span><br><span class="line">https://mirrors.tuna.tsinghua.edu.cn/ubuntu-cloud-images/bionic/current/bionic-server-cloudimg-amd64-vagrant.box \</span><br><span class="line">--name ubuntu/bionic</span><br></pre></td></tr></table></figure><p>Vagrantfile这样写：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">config.vm.box = <span class="string">&quot;ubuntu/bionic&quot;</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>接着就是<code>vagrant up &amp;&amp; vagrant ssh</code>了</p><h2 id="基本命令"><a href="#基本命令" class="headerlink" title="基本命令"></a>基本命令</h2><h3 id="列出所有Box"><a href="#列出所有Box" class="headerlink" title="列出所有Box"></a>列出所有Box</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant box list</span><br></pre></td></tr></table></figure><h3 id="添加一个Box"><a href="#添加一个Box" class="headerlink" title="添加一个Box"></a>添加一个Box</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant box add [options] &lt;name, url, or path</span><br></pre></td></tr></table></figure><h3 id="可以从https-app-vagrantup-com-boxes-search下载各种Vagrant映像文件"><a href="#可以从https-app-vagrantup-com-boxes-search下载各种Vagrant映像文件" class="headerlink" title="可以从https://app.vagrantup.com/boxes/search下载各种Vagrant映像文件"></a>可以从<a target="_blank" rel="noopener" href="https://app.vagrantup.com/boxes/search%E4%B8%8B%E8%BD%BD%E5%90%84%E7%A7%8DVagrant%E6%98%A0%E5%83%8F%E6%96%87%E4%BB%B6">https://app.vagrantup.com/boxes/search下载各种Vagrant映像文件</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant box add ubuntu/trusty64</span><br></pre></td></tr></table></figure><h3 id="通过指定的URL添加远程box"><a href="#通过指定的URL添加远程box" class="headerlink" title="通过指定的URL添加远程box"></a>通过指定的URL添加远程box</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant box add https://atlas.hashicorp.com/ubuntu/boxes/trusty64</span><br></pre></td></tr></table></figure><h3 id="添加一个本地box"><a href="#添加一个本地box" class="headerlink" title="添加一个本地box"></a>添加一个本地box</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant box add &#123;box_name&#125; &#123;file_path&#125;</span><br></pre></td></tr></table></figure><h3 id="初始化一个新VM"><a href="#初始化一个新VM" class="headerlink" title="初始化一个新VM"></a>初始化一个新VM</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant init ubuntu/trustry64</span><br></pre></td></tr></table></figure><p>此命令会在当前目录创建一个名为Vagrantfile的配置文件，内容大致如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Vagrant.configure(<span class="string">&quot;2&quot;</span>) <span class="keyword">do</span> |config|</span><br><span class="line">  config.vm.box = <span class="string">&quot;ubuntu/trusty64&quot;</span></span><br><span class="line">end</span><br></pre></td></tr></table></figure><h3 id="初始化一个新VM-1"><a href="#初始化一个新VM-1" class="headerlink" title="初始化一个新VM"></a>初始化一个新VM</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant up</span><br></pre></td></tr></table></figure><h3 id="启用SSH登陆VM"><a href="#启用SSH登陆VM" class="headerlink" title="启用SSH登陆VM"></a>启用SSH登陆VM</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh &lt;node_name&gt;</span><br></pre></td></tr></table></figure><p>如果需要从虚拟机中退出，直接在虚拟机中的命令行输入exit命令即可</p><h3 id="查看VM当前的状态"><a href="#查看VM当前的状态" class="headerlink" title="查看VM当前的状态"></a>查看VM当前的状态</h3><p>进入Vagrantfile配置文件所在的目录，执行以下命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant status</span><br></pre></td></tr></table></figure><h3 id="关闭VM"><a href="#关闭VM" class="headerlink" title="关闭VM"></a>关闭VM</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant halt</span><br></pre></td></tr></table></figure><h3 id="销毁VM"><a href="#销毁VM" class="headerlink" title="销毁VM"></a>销毁VM</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant destory [name|<span class="built_in">id</span>]</span><br></pre></td></tr></table></figure></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://enpsl.github.io/2021/03/02/2021-03-02-dockerfile/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="enpsl"><meta itemprop="description" content="my blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="彭诗亮的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"> <a href="/2021/03/02/2021-03-02-dockerfile/" class="post-title-link" itemprop="url">Dockerfile 语法梳理</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-03-02 13:30:00" itemprop="dateCreated datePublished" datetime="2021-03-02T13:30:00+08:00">2021-03-02</time></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="Dockerfile语法梳理"><a href="#Dockerfile语法梳理" class="headerlink" title="Dockerfile语法梳理"></a>Dockerfile语法梳理</h2><h3 id="FROM"><a href="#FROM" class="headerlink" title="FROM"></a>FROM</h3><p>from 后面接base image</p><p>eg:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> scratch</span><br><span class="line"><span class="keyword">FROM</span> centos</span><br><span class="line"><span class="keyword">FROM</span> ubuntu</span><br></pre></td></tr></table></figure><blockquote><p>尽量使用官方的image 作为base image</p></blockquote><h3 id="LABEL"><a href="#LABEL" class="headerlink" title="LABEL"></a>LABEL</h3><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">LABEL</span><span class="language-bash"> maintainer=<span class="string">&quot;username@gmail.com&quot;</span> </span></span><br><span class="line"><span class="keyword">LABEL</span><span class="language-bash"> version=<span class="string">&quot;1.0&quot;</span> </span></span><br><span class="line"><span class="keyword">LABEL</span><span class="language-bash"> description=<span class="string">&quot;this is description&quot;</span> </span></span><br></pre></td></tr></table></figure><blockquote><p>MetaData 不可少</p></blockquote><h3 id="RUN"><a href="#RUN" class="headerlink" title="RUN"></a>RUN</h3><p>执行命令并创建新的IMAGE LAYER</p><p>为了美观，复杂的RUN用反斜杠换行，避免无用分层，合并多条命令成一行。</p><h4 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h4><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">RUN</span><span class="language-bash"> yum update &amp;&amp; yum install -y vim \</span></span><br><span class="line"><span class="language-bash">python-dev</span></span><br></pre></td></tr></table></figure><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">RUN</span><span class="language-bash"> apt-get update &amp;&amp; apt-get install -y perl \</span></span><br><span class="line"><span class="language-bash">pwgen --no-install-recommends &amp;&amp; <span class="built_in">rm</span> -rf \</span></span><br><span class="line"><span class="language-bash">/var/lib/apt/lists/*     <span class="comment">#清理cache</span></span></span><br></pre></td></tr></table></figure><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">RUN</span><span class="language-bash"> /bin/bash -c <span class="string">&#x27;source $HOME/.bashrc;echo #HOME&#x27;</span></span></span><br></pre></td></tr></table></figure><h3 id="WORKDIR"><a href="#WORKDIR" class="headerlink" title="WORKDIR"></a>WORKDIR</h3><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> <span class="built_in">test</span>       <span class="comment"># 如果没有会自动创建test文件夹</span></span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> demo</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> <span class="built_in">pwd</span>            <span class="comment"># 打印/test/demo</span></span></span><br></pre></td></tr></table></figure><blockquote><p>尽量使用WORKDIR,不要使用RUN cd,尽量使用绝对路径</p></blockquote><h3 id="ADD-and-COPY"><a href="#ADD-and-COPY" class="headerlink" title="ADD and COPY"></a>ADD and COPY</h3><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ADD</span><span class="language-bash"> hello /</span></span><br><span class="line"><span class="keyword">ADD</span><span class="language-bash"> test.tar.gz / <span class="comment"># 添加到根目录并解压缩</span></span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> /root</span></span><br><span class="line"><span class="keyword">ADD</span><span class="language-bash"> hello <span class="built_in">test</span> /    <span class="comment"># /root/test/hello</span></span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> /root</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> hello <span class="built_in">test</span> /</span></span><br></pre></td></tr></table></figure><blockquote><p>大部分情况COPY优于，ADD有额外的解压功能，添加远程文件或目录用curl或wget</p></blockquote><h3 id="ENV"><a href="#ENV" class="headerlink" title="ENV"></a>ENV</h3><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ENV</span> MYSQL_VERSION <span class="number">5.6</span> <span class="comment">#常量</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> apt-get install -y mysql-server= <span class="string">&quot;<span class="variable">$&#123;MYSQL_VERSION&#125;</span>&quot;</span> \</span></span><br><span class="line"><span class="language-bash">&amp;&amp; <span class="built_in">rm</span> -rf /var/lib/apt/lists/*</span></span><br></pre></td></tr></table></figure><h3 id="CMD-amp-ENTRYPOINT"><a href="#CMD-amp-ENTRYPOINT" class="headerlink" title="CMD &amp; ENTRYPOINT"></a>CMD &amp; ENTRYPOINT</h3><p>CMD:设置容器启动后默认执行的命令和参数<br>如果docker run指定了其它的命令，则忽略CMD命令<br>定义多个CMD,只有最后一个会执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run &lt;image&gt;</span><br><span class="line">docker run -it &lt;image&gt; /bin/bash    //此命令会忽略CMD中的命令</span><br></pre></td></tr></table></figure><p>ENTRYPOINT:设置容器启动时运行的命令<br>让容器已应用程序或者服务的方式执行<br>不会被忽略，一定会执行</p><h4 id="SHELL-amp-EXEC"><a href="#SHELL-amp-EXEC" class="headerlink" title="SHELL &amp; EXEC"></a>SHELL &amp; EXEC</h4><p>SHELL:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">RUN</span><span class="language-bash"> apt-get install -y vim</span></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> <span class="built_in">echo</span> <span class="string">&quot;hello docker&quot;</span></span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="language-bash"> <span class="built_in">echo</span> <span class="string">&quot;hello docker&quot;</span></span></span><br></pre></td></tr></table></figure><p>EXEC:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">RUN</span><span class="language-bash"> [<span class="string">&quot;apt-get&quot;</span>, <span class="string">&quot;install&quot;</span>, <span class="string">&quot;y&quot;</span>, <span class="string">&quot;vim&quot;</span>]</span></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [<span class="string">&quot;/bin/echo&quot;</span>, <span class="string">&quot;hello docker&quot;</span>]</span></span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="string">&quot;/bin/echo&quot;</span>, <span class="string">&quot;hello docker&quot;</span>]</span></span><br></pre></td></tr></table></figure><p>EXEC方式需要指明运行环境，eg:</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> centos</span><br><span class="line"><span class="keyword">ENV</span> name word</span><br><span class="line"><span class="keyword">ENTRYPOINT</span><span class="language-bash"> [<span class="string">&quot;/bin/bash&quot;</span>, <span class="string">&quot;c&quot;</span>, <span class="string">&quot;echo hello <span class="variable">$name</span>&quot;</span>]</span></span><br></pre></td></tr></table></figure><p>更多详见<a target="_blank" rel="noopener" href="https://docs.docker.com/engine/reference/builder/">扩展阅读</a></p><h2 id="Dockerfile实战"><a href="#Dockerfile实战" class="headerlink" title="Dockerfile实战"></a>Dockerfile实战</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> flask-hello-word</span><br><span class="line"><span class="built_in">cd</span> flask-hello-word</span><br><span class="line">vim app.py</span><br></pre></td></tr></table></figure><p>app.py内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line">app = Flask(__name__)</span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hello</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;hello docker&quot;</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app.run(host=<span class="string">&quot;0.0.0.0&quot;</span>, port=<span class="number">5000</span>)</span><br></pre></td></tr></table></figure><p>编写Dockerfile</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> python:<span class="number">2.7</span></span><br><span class="line"><span class="keyword">LABEL</span><span class="language-bash"> maintainer=<span class="string">&quot;peng.shiliang&lt;1390509500@qq.com&gt;&quot;</span></span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> pip install flask</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> app.py /app/       <span class="comment"># app后面必须接/，否则会当作文件</span></span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> /app</span></span><br><span class="line"><span class="keyword">EXPOSE</span> <span class="number">5000</span>             <span class="comment"># 端口映射,保证远程能够访问</span></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [<span class="string">&quot;python&quot;</span>, <span class="string">&quot;app.py&quot;</span>]</span></span><br></pre></td></tr></table></figure><p>执行</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker build -t pengshiliang/flask-hello-word .</span><br><span class="line">docker push pengshiliang/flask-hello-word:latest</span><br></pre></td></tr></table></figure><p>运行flask-hello-word</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name=demo pengshiliang/flask-hello-word     //--name 便于docker container 操作</span><br><span class="line">docker <span class="built_in">exec</span> -it demo ip a       //查看docker容器ip</span><br><span class="line">curl &lt;demo ip&gt;      //输出hello docker</span><br></pre></td></tr></table></figure></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://enpsl.github.io/2021/03/02/2021-03-02-docker/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="enpsl"><meta itemprop="description" content="my blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="彭诗亮的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"> <a href="/2021/03/02/2021-03-02-docker/" class="post-title-link" itemprop="url">Docker入门</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-03-02 10:30:00" itemprop="dateCreated datePublished" datetime="2021-03-02T10:30:00+08:00">2021-03-02</time></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="赋予docker权限"><a href="#赋予docker权限" class="headerlink" title="赋予docker权限"></a>赋予docker权限</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vagrant@ubuntu-bionic:~$ sudo groupadd docker</span><br><span class="line">groupadd: group <span class="string">&#x27;docker&#x27;</span> already exists</span><br><span class="line">vagrant@ubuntu-bionic:~$ sudo gpasswd -a vagrant docker</span><br><span class="line">Adding user vagrant to group docker</span><br><span class="line">vagrant@ubuntu-bionic:~$ sudo service docker restart</span><br></pre></td></tr></table></figure><p>退出vagrant在重新进入</p><h2 id="docker基本命令"><a href="#docker基本命令" class="headerlink" title="docker基本命令"></a>docker基本命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 列举所有镜像</span><br><span class="line">docker image <span class="built_in">ls</span></span><br><span class="line">// 查看image build 历史</span><br><span class="line">docker <span class="built_in">history</span> &lt;image <span class="built_in">id</span>&gt;</span><br><span class="line">// 运行一个image</span><br><span class="line">docker run &lt;image <span class="built_in">id</span>&gt;</span><br><span class="line">// 列举所有正在运行的容器</span><br><span class="line">docker container <span class="built_in">ls</span></span><br><span class="line">// 列举所有的容器</span><br><span class="line">docker container <span class="built_in">ls</span> -a</span><br><span class="line">// 交互式运行运行（常驻运行）</span><br><span class="line">docker run -it &lt;image&gt;</span><br></pre></td></tr></table></figure><h2 id="docker-image-命令"><a href="#docker-image-命令" class="headerlink" title="docker image 命令"></a>docker image 命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker images   (docker image <span class="built_in">ls</span>缩写)</span><br><span class="line">docker rmi &lt;image <span class="built_in">id</span>&gt;   (docker image <span class="built_in">rm</span> &lt;image <span class="built_in">id</span>&gt;缩写)//移除一个镜像</span><br></pre></td></tr></table></figure><h2 id="docker-container命令"><a href="#docker-container命令" class="headerlink" title="docker container命令"></a>docker container命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker ps -a    (docker container <span class="built_in">ls</span> -a缩写)</span><br><span class="line">docker <span class="built_in">rm</span> &lt;image <span class="built_in">id</span>&gt;    (docker container <span class="built_in">rm</span> &lt;image <span class="built_in">id</span>&gt;缩写)  //删除一个容器</span><br><span class="line">docker ps -aq   //列举所有容器<span class="built_in">id</span></span><br><span class="line">docker ps -f <span class="string">&quot;status=exited&quot;</span> -q     //列举所有已退出的容器</span><br><span class="line">docker <span class="built_in">rm</span> $(docker ps -aq)</span><br><span class="line">docker <span class="built_in">rm</span> $(docker ps -f <span class="string">&quot;status=exited&quot;</span> -q)</span><br></pre></td></tr></table></figure><p><img src="/img/in-post/2019-03-02/3.png" alt="avatar"><br><img src="/img/in-post/2019-03-02/4.png" alt="avatar"></p><h2 id="build一个hello-word-image"><a href="#build一个hello-word-image" class="headerlink" title="build一个hello word image"></a>build一个hello word image</h2><h3 id="生成hello-word程序"><a href="#生成hello-word程序" class="headerlink" title="生成hello-word程序"></a>生成hello-word程序</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> hello-word</span><br><span class="line"><span class="built_in">cd</span> hello-word</span><br><span class="line">vim hello.c</span><br></pre></td></tr></table></figure><p>hello.c内容</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;hello word\n&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install gcc</span><br><span class="line">sudo apt-get install build-essential</span><br><span class="line">gcc -static hello.c -o hello</span><br></pre></td></tr></table></figure><p><img src="/img/in-post/2019-03-02/1.png" alt="avatar"></p><h3 id="编写Dockerfile"><a href="#编写Dockerfile" class="headerlink" title="编写Dockerfile"></a>编写Dockerfile</h3><p>执行<code>vim Dockerfile</code></p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> scratch</span><br><span class="line"><span class="keyword">ADD</span><span class="language-bash"> hello /</span></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [<span class="string">&quot;/hello&quot;</span>]</span></span><br></pre></td></tr></table></figure><h3 id="build命令"><a href="#build命令" class="headerlink" title="build命令"></a>build命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t &lt;tag&gt; &lt;<span class="built_in">dir</span>&gt;</span><br></pre></td></tr></table></figure><p>eg:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t pengshiliang/hello-word .</span><br></pre></td></tr></table></figure><p><img src="/img/in-post/2019-03-02/2.png" alt="avatar"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run pengshiliang/hello-word</span><br></pre></td></tr></table></figure><p>出现hello word 即为正常build</p><h2 id="发布"><a href="#发布" class="headerlink" title="发布"></a>发布</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker login</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker push pengshiliang/hello-word:latest</span><br></pre></td></tr></table></figure><h2 id="Container"><a href="#Container" class="headerlink" title="Container"></a>Container</h2><ol><li>通过image创建</li><li>在Image layer之上建立一个Cotainer layer</li><li>类面向对象：类和实例</li><li>image复制存储和分发，container负责运行app</li></ol></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://enpsl.github.io/2021/02/17/2021-02-17-CAP&BASE/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="enpsl"><meta itemprop="description" content="my blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="彭诗亮的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"> <a href="/2021/02/17/2021-02-17-CAP&BASE/" class="post-title-link" itemprop="url">CAP&BASE理论</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-02-17 19:30:00" itemprop="dateCreated datePublished" datetime="2021-02-17T19:30:00+08:00">2021-02-17</time></span></div></header><div class="post-body" itemprop="articleBody"><p>经历过技术面试的小伙伴想必对 CAP &amp; BASE 这个两个理论已经再熟悉不过了！</p><p>我当年参加面试的时候，不夸张地说，只要问到分布式相关的内容，面试官几乎是必定会问这两个分布式相关的理论。一是因为这两个分布式基础理论是学习分布式知识的必备前置基础，二是因为很多面试官自己比较熟悉这两个理论（方便提问）。</p><p>我们非常有必要将这两个理论搞懂，并且能够用自己的理解给别人讲出来。</p><h2 id="CAP-理论"><a href="#CAP-理论" class="headerlink" title="# CAP 理论"></a><a href="#cap-%E7%90%86%E8%AE%BA">#</a> CAP 理论</h2><p><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/CAP%E5%AE%9A%E7%90%86">CAP 理论&#x2F;定理open in new window</a>起源于 2000 年，由加州大学伯克利分校的 Eric Brewer 教授在分布式计算原理研讨会（PODC）上提出，因此 CAP 定理又被称作 <strong>布鲁尔定理（Brewer’s theorem）</strong></p><p>2 年后，麻省理工学院的 Seth Gilbert 和 Nancy Lynch 发表了布鲁尔猜想的证明，CAP 理论正式成为分布式领域的定理。</p><h3 id="简介"><a href="#简介" class="headerlink" title="# 简介"></a><a href="#%E7%AE%80%E4%BB%8B">#</a> 简介</h3><p><strong>CAP</strong> 也就是 <strong>Consistency（一致性）</strong>、<strong>Availability（可用性）</strong>、<strong>Partition Tolerance（分区容错性）</strong> 这三个单词首字母组合。</p><p><img src="https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/2020-11/cap.png" alt="img"></p><p>CAP 理论的提出者布鲁尔在提出 CAP 猜想的时候，并没有详细定义 <strong>Consistency</strong>、<strong>Availability</strong>、<strong>Partition Tolerance</strong> 三个单词的明确定义。</p><p>因此，对于 CAP 的民间解读有很多，一般比较被大家推荐的是下面 👇 这种版本的解读。</p><p>在理论计算机科学中，CAP 定理（CAP theorem）指出对于一个分布式系统来说，当设计读写操作时，只能同时满足以下三点中的两个：</p><ul><li><strong>一致性（Consistency）</strong> : 所有节点访问同一份最新的数据副本</li><li><strong>可用性（Availability）</strong>: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。</li><li><strong>分区容错性（Partition Tolerance）</strong> : 分布式系统出现网络分区的时候，仍然能够对外提供服务。</li></ul><p><strong>什么是网络分区？</strong></p><p>分布式系统中，多个节点之前的网络本来是连通的，但是因为某些故障（比如部分节点网络出了问题）某些节点之间不连通了，整个网络就分成了几块区域，这就叫 <strong>网络分区</strong>。</p><p><img src="https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/2020-11/partition-tolerance.png" alt="partition-tolerance"></p><h3 id="不是所谓的“3-选-2”"><a href="#不是所谓的“3-选-2”" class="headerlink" title="# 不是所谓的“3 选 2”"></a><a href="#%E4%B8%8D%E6%98%AF%E6%89%80%E8%B0%93%E7%9A%84-3-%E9%80%89-2">#</a> 不是所谓的“3 选 2”</h3><p>大部分人解释这一定律时，常常简单的表述为：“一致性、可用性、分区容忍性三者你只能同时达到其中两个，不可能同时达到”。实际上这是一个非常具有误导性质的说法，而且在 CAP 理论诞生 12 年之后，CAP 之父也在 2012 年重写了之前的论文。</p><blockquote><p><strong>当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能 2 选 1。也就是说当网络分区之后 P 是前提，决定了 P 之后才有 C 和 A 的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。</strong></p><p>简而言之就是：CAP 理论中分区容错性 P 是一定要满足的，在此基础上，只能满足可用性 A 或者一致性 C。</p></blockquote><p>因此，<strong>分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。</strong> 比如 ZooKeeper、HBase 就是 CP 架构，Cassandra、Eureka 就是 AP 架构，Nacos 不仅支持 CP 架构也支持 AP 架构。</p><p><strong>为啥不可能选择 CA 架构呢？</strong> 举个例子：若系统出现“分区”，系统中的某个节点在进行写操作。为了保证 C， 必须要禁止其他节点的读写操作，这就和 A 发生冲突了。如果为了保证 A，其他节点的读写操作正常的话，那就和 C 发生冲突了。</p><p><strong>选择 CP 还是 AP 的关键在于当前的业务场景，没有定论，比如对于需要确保强一致性的场景如银行一般会选择保证 CP 。</strong></p><p>另外，需要补充说明的一点是： <strong>如果网络分区正常的话（系统在绝大部分时候所处的状态），也就说不需要保证 P 的时候，C 和 A 能够同时保证。</strong></p><h3 id="CAP-实际应用案例"><a href="#CAP-实际应用案例" class="headerlink" title="# CAP 实际应用案例"></a><a href="#cap-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B">#</a> CAP 实际应用案例</h3><p>我这里以注册中心来探讨一下 CAP 的实际应用。考虑到很多小伙伴不知道注册中心是干嘛的，这里简单以 Dubbo 为例说一说。</p><p>下图是 Dubbo 的架构图。<strong>注册中心 Registry 在其中扮演了什么角色呢？提供了什么服务呢？</strong></p><p>注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小。</p><p><img src="https://guide-blog-images.oss-cn-shenzhen.aliyuncs.com/2020-11/dubbo-architecture.png" alt="img"></p><p>常见的可以作为注册中心的组件有：ZooKeeper、Eureka、Nacos…。</p><ol><li><strong>ZooKeeper 保证的是 CP。</strong> 任何时刻对 ZooKeeper 的读请求都能得到一致性的结果，但是， ZooKeeper 不保证每次请求的可用性比如在 Leader 选举过程中或者半数以上的机器不可用的时候服务就是不可用的。</li><li><strong>Eureka 保证的则是 AP。</strong> Eureka 在设计的时候就是优先保证 A （可用性）。在 Eureka 中不存在什么 Leader 节点，每个节点都是一样的、平等的。因此 Eureka 不会像 ZooKeeper 那样出现选举过程中或者半数以上的机器不可用的时候服务就是不可用的情况。 Eureka 保证即使大部分节点挂掉也不会影响正常提供服务，只要有一个节点是可用的就行了。只不过这个节点上的数据可能并不是最新的。</li><li><strong>Nacos 不仅支持 CP 也支持 AP。</strong></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="# 总结"></a><a href="#%E6%80%BB%E7%BB%93">#</a> 总结</h3><p>在进行分布式系统设计和开发时，我们不应该仅仅局限在 CAP 问题上，还要关注系统的扩展性、可用性等等</p><p>在系统发生“分区”的情况下，CAP 理论只能满足 CP 或者 AP。要注意的是，这里的前提是系统发生了“分区”</p><p>如果系统没有发生“分区”的话，节点间的网络连接通信正常的话，也就不存在 P 了。这个时候，我们就可以同时保证 C 和 A 了。</p><p>总结：<strong>如果系统发生“分区”，我们要考虑选择 CP 还是 AP。如果系统没有发生“分区”的话，我们要思考如何保证 CA 。</strong></p><h3 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="# 推荐阅读"></a><a href="#%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB">#</a> 推荐阅读</h3><ol><li><a target="_blank" rel="noopener" href="https://medium.com/@ravindraprasad/cap-theorem-simplified-28499a67eab4">CAP 定理简化open in new window</a> （英文，有趣的案例）</li><li><a target="_blank" rel="noopener" href="https://juejin.im/post/6844903936718012430">神一样的 CAP 理论被应用在何方open in new window</a> （中文，列举了很多实际的例子）</li><li><a target="_blank" rel="noopener" href="https://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html">请停止呼叫数据库 CP 或 AP open in new window</a> （英文，带给你不一样的思考）</li></ol><h2 id="BASE-理论"><a href="#BASE-理论" class="headerlink" title="# BASE 理论"></a><a href="#base-%E7%90%86%E8%AE%BA">#</a> BASE 理论</h2><p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/1394127.1394128">BASE 理论open in new window</a>起源于 2008 年， 由 eBay 的架构师 Dan Pritchett 在 ACM 上发表。</p><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="# 简介"></a><a href="#%E7%AE%80%E4%BB%8B-1">#</a> 简介</h3><p><strong>BASE</strong> 是 <strong>Basically Available（基本可用）</strong> 、<strong>Soft-state（软状态）</strong> 和 <strong>Eventually Consistent（最终一致性）</strong> 三个短语的缩写。BASE 理论是对 CAP 中一致性 C 和可用性 A 权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于 CAP 定理逐步演化而来的，它大大降低了我们对系统的要求。</p><h3 id="BASE-理论的核心思想"><a href="#BASE-理论的核心思想" class="headerlink" title="# BASE 理论的核心思想"></a><a href="#base-%E7%90%86%E8%AE%BA%E7%9A%84%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3">#</a> BASE 理论的核心思想</h3><p>即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。</p><blockquote><p>也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。</p></blockquote><p><strong>BASE 理论本质上是对 CAP 的延伸和补充，更具体地说，是对 CAP 中 AP 方案的一个补充。</strong></p><p><strong>为什么这样说呢？</strong></p><p>CAP 理论这节我们也说过了：</p><blockquote><p>如果系统没有发生“分区”的话，节点间的网络连接通信正常的话，也就不存在 P 了。这个时候，我们就可以同时保证 C 和 A 了。因此，<strong>如果系统发生“分区”，我们要考虑选择 CP 还是 AP。如果系统没有发生“分区”的话，我们要思考如何保证 CA 。</strong></p></blockquote><p>因此，AP 方案只是在系统发生分区的时候放弃一致性，而不是永远放弃一致性。在分区故障恢复后，系统应该达到最终一致性。这一点其实就是 BASE 理论延伸的地方。</p><h3 id="BASE-理论三要素"><a href="#BASE-理论三要素" class="headerlink" title="# BASE 理论三要素"></a><a href="#base-%E7%90%86%E8%AE%BA%E4%B8%89%E8%A6%81%E7%B4%A0">#</a> BASE 理论三要素</h3><p><img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC81LzI0LzE2MzkxNDgwNmQ5ZTE1YzY?x-oss-process=image/format,png" alt="BASE理论三要素"></p><h4 id="基本可用"><a href="#基本可用" class="headerlink" title="# 基本可用"></a><a href="#%E5%9F%BA%E6%9C%AC%E5%8F%AF%E7%94%A8">#</a> 基本可用</h4><p>基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。</p><p><strong>什么叫允许损失部分可用性呢？</strong></p><ul><li><strong>响应时间上的损失</strong>: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。</li><li><strong>系统功能上的损失</strong>：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。</li></ul><h4 id="软状态"><a href="#软状态" class="headerlink" title="# 软状态"></a><a href="#%E8%BD%AF%E7%8A%B6%E6%80%81">#</a> 软状态</h4><p>软状态指允许系统中的数据存在中间状态（<strong>CAP 理论中的数据不一致</strong>），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。</p><h4 id="最终一致性"><a href="#最终一致性" class="headerlink" title="# 最终一致性"></a><a href="#%E6%9C%80%E7%BB%88%E4%B8%80%E8%87%B4%E6%80%A7">#</a> 最终一致性</h4><p>最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。</p><blockquote><p>分布式一致性的 3 种级别：</p><ol><li><strong>强一致性</strong> ：系统写入了什么，读出来的就是什么。</li><li><strong>弱一致性</strong> ：不一定可以读取到最新写入的值，也不保证多少时间之后读取到的数据是最新的，只是会尽量保证某个时刻达到数据一致的状态。</li><li><strong>最终一致性</strong> ：弱一致性的升级版，系统会保证在一定时间内达到数据一致的状态。</li></ol><p><strong>业界比较推崇是最终一致性级别，但是某些对数据一致要求十分严格的场景比如银行转账还是要保证强一致性。</strong></p></blockquote><p>那实现最终一致性的具体方式是什么呢? <a target="_blank" rel="noopener" href="http://gk.link/a/10rZM">《分布式协议与算法实战》open in new window</a> 中是这样介绍：</p><blockquote><ul><li><strong>读时修复</strong> : 在读取数据时，检测数据的不一致，进行修复。比如 Cassandra 的 Read Repair 实现，具体来说，在向 Cassandra 系统查询数据的时候，如果检测到不同节点 的副本数据不一致，系统就自动修复数据。</li><li><strong>写时修复</strong> : 在写入数据，检测数据的不一致时，进行修复。比如 Cassandra 的 Hinted Handoff 实现。具体来说，Cassandra 集群的节点之间远程写数据的时候，如果写失败 就将数据缓存下来，然后定时重传，修复数据的不一致性。</li><li><strong>异步修复</strong> : 这个是最常用的方式，通过定时对账检测副本数据的一致性，并修复。</li></ul></blockquote><p>比较推荐 <strong>写时修复</strong>，这种方式对性能消耗比较低。</p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="# 总结"></a><a href="#%E6%80%BB%E7%BB%93-1">#</a> 总结</h3><p><strong>ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。</strong></p></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://enpsl.github.io/2021/02/17/2021-12-29-ZooKeeper%20%E5%AE%9E%E6%88%98/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="enpsl"><meta itemprop="description" content="my blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="彭诗亮的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"> <a href="/2021/02/17/2021-12-29-ZooKeeper%20%E5%AE%9E%E6%88%98/" class="post-title-link" itemprop="url">ZooKeeper实战</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-02-17 19:30:00" itemprop="dateCreated datePublished" datetime="2021-02-17T19:30:00+08:00">2021-02-17</time></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>这篇文章简单给演示一下 ZooKeeper 常见命令的使用以及 ZooKeeper Java客户端 Curator 的基本使用。介绍到的内容都是最基本的操作，能满足日常工作的基本需要。</p><p>如果文章有任何需要改善和完善的地方，欢迎在评论区指出，共同进步！</p><h2 id="2-ZooKeeper-安装和使用"><a href="#2-ZooKeeper-安装和使用" class="headerlink" title="2. ZooKeeper 安装和使用"></a>2. ZooKeeper 安装和使用</h2><h3 id="2-1-使用Docker-安装-zookeeper"><a href="#2-1-使用Docker-安装-zookeeper" class="headerlink" title="2.1. 使用Docker 安装 zookeeper"></a>2.1. 使用Docker 安装 zookeeper</h3><p><strong>a.使用 Docker 下载 ZooKeeper</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull zookeeper:3.5.8</span><br></pre></td></tr></table></figure><p><strong>b.运行 ZooKeeper</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name zookeeper -p 2181:2181 zookeeper:3.5.8</span><br></pre></td></tr></table></figure><h3 id="2-2-连接-ZooKeeper-服务"><a href="#2-2-连接-ZooKeeper-服务" class="headerlink" title="2.2. 连接 ZooKeeper 服务"></a>2.2. 连接 ZooKeeper 服务</h3><p><strong>a.进入ZooKeeper容器中</strong></p><p>先使用 <code>docker ps</code> 查看 ZooKeeper 的 ContainerID，然后使用 <code>docker exec -it ContainerID /bin/bash</code> 命令进入容器中。</p><p><strong>b.先进入 bin 目录,然后通过 <code>./zkCli.sh -server 127.0.0.1:2181</code>命令连接ZooKeeper 服务</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">root@eaf70fc620cb:/apache-zookeeper-3.5.8-bin# cd bin</span><br></pre></td></tr></table></figure><p>如果你看到控制台成功打印出如下信息的话，说明你已经成功连接 ZooKeeper 服务。</p><p><a target="_blank" rel="noopener" href="https://github.com/Snailclimb/JavaGuide/blob/main/docs/distributed-system/distributed-process-coordination/zookeeper/images/%E8%BF%9E%E6%8E%A5ZooKeeper%E6%9C%8D%E5%8A%A1.png"><img src="https://github.com/Snailclimb/JavaGuide/raw/main/docs/distributed-system/distributed-process-coordination/zookeeper/images/%E8%BF%9E%E6%8E%A5ZooKeeper%E6%9C%8D%E5%8A%A1.png" alt="img"></a></p><h3 id="2-3-常用命令演示"><a href="#2-3-常用命令演示" class="headerlink" title="2.3. 常用命令演示"></a>2.3. 常用命令演示</h3><h4 id="2-3-1-查看常用命令-help-命令"><a href="#2-3-1-查看常用命令-help-命令" class="headerlink" title="2.3.1. 查看常用命令(help 命令)"></a>2.3.1. 查看常用命令(help 命令)</h4><p>通过 <code>help</code> 命令查看 ZooKeeper 常用命令</p><h4 id="2-3-2-创建节点-create-命令"><a href="#2-3-2-创建节点-create-命令" class="headerlink" title="2.3.2. 创建节点(create 命令)"></a>2.3.2. 创建节点(create 命令)</h4><p>通过 <code>create</code> 命令在根目录创建了 node1 节点，与它关联的字符串是”node1”</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: 127.0.0.1:2181(CONNECTED) 34] create /node1 “node1”</span><br></pre></td></tr></table></figure><p>通过 <code>create</code> 命令在根目录创建了 node1 节点，与它关联的内容是数字 123</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: 127.0.0.1:2181(CONNECTED) 1] create /node1/node1.1 123</span><br><span class="line">Created /node1/node1.1</span><br></pre></td></tr></table></figure><h4 id="2-3-3-更新节点数据内容-set-命令"><a href="#2-3-3-更新节点数据内容-set-命令" class="headerlink" title="2.3.3. 更新节点数据内容(set 命令)"></a>2.3.3. 更新节点数据内容(set 命令)</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: 127.0.0.1:2181(CONNECTED) 11] set /node1 &quot;set node1&quot;</span><br></pre></td></tr></table></figure><h4 id="2-3-4-获取节点的数据-get-命令"><a href="#2-3-4-获取节点的数据-get-命令" class="headerlink" title="2.3.4. 获取节点的数据(get 命令)"></a>2.3.4. 获取节点的数据(get 命令)</h4><p><code>get</code> 命令可以获取指定节点的数据内容和节点的状态,可以看出我们通过 <code>set</code> 命令已经将节点数据内容改为 “set node1”。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">set node1</span><br><span class="line">cZxid = 0x47</span><br><span class="line">ctime = Sun Jan 20 10:22:59 CST 2019</span><br><span class="line">mZxid = 0x4b</span><br><span class="line">mtime = Sun Jan 20 10:41:10 CST 2019</span><br><span class="line">pZxid = 0x4a</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 1</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 9</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure><h4 id="2-3-5-查看某个目录下的子节点-ls-命令"><a href="#2-3-5-查看某个目录下的子节点-ls-命令" class="headerlink" title="2.3.5. 查看某个目录下的子节点(ls 命令)"></a>2.3.5. 查看某个目录下的子节点(ls 命令)</h4><p>通过 <code>ls</code> 命令查看根目录下的节点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: 127.0.0.1:2181(CONNECTED) 37] ls /</span><br><span class="line">[dubbo, ZooKeeper, node1]</span><br></pre></td></tr></table></figure><p>通过 <code>ls</code> 命令查看 node1 目录下的节点</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: 127.0.0.1:2181(CONNECTED) 5] ls /node1</span><br><span class="line">[node1.1]</span><br></pre></td></tr></table></figure><p>ZooKeeper 中的 ls 命令和 linux 命令中的 ls 类似， 这个命令将列出绝对路径 path 下的所有子节点信息（列出 1 级，并不递归）</p><h4 id="2-3-6-查看节点状态-stat-命令"><a href="#2-3-6-查看节点状态-stat-命令" class="headerlink" title="2.3.6. 查看节点状态(stat 命令)"></a>2.3.6. 查看节点状态(stat 命令)</h4><p>通过 <code>stat</code> 命令查看节点状态</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[zk: 127.0.0.1:2181(CONNECTED) 10] stat /node1</span><br><span class="line">cZxid = 0x47</span><br><span class="line">ctime = Sun Jan 20 10:22:59 CST 2019</span><br><span class="line">mZxid = 0x47</span><br><span class="line">mtime = Sun Jan 20 10:22:59 CST 2019</span><br><span class="line">pZxid = 0x4a</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 11</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure><p>上面显示的一些信息比如 cversion、aclVersion、numChildren 等等，我在上面 “<a target="_blank" rel="noopener" href="https://javaguide.cn/distributed-system/distributed-process-coordination/zookeeper/zookeeper-intro.html">ZooKeeper 相关概念总结(入门)</a>” 这篇文章中已经介绍到。</p><h4 id="2-3-7-查看节点信息和状态-ls2-命令"><a href="#2-3-7-查看节点信息和状态-ls2-命令" class="headerlink" title="2.3.7. 查看节点信息和状态(ls2 命令)"></a>2.3.7. 查看节点信息和状态(ls2 命令)</h4><p><code>ls2</code> 命令更像是 <code>ls</code> 命令和 <code>stat</code> 命令的结合。 <code>ls2</code> 命令返回的信息包括 2 部分：</p><ol><li>子节点列表</li><li>当前节点的 stat 信息。</li></ol><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[zk: 127.0.0.1:2181(CONNECTED) 7] ls2 /node1</span><br><span class="line">[node1.1]</span><br><span class="line">cZxid = 0x47</span><br><span class="line">ctime = Sun Jan 20 10:22:59 CST 2019</span><br><span class="line">mZxid = 0x47</span><br><span class="line">mtime = Sun Jan 20 10:22:59 CST 2019</span><br><span class="line">pZxid = 0x4a</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 11</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure><h4 id="2-3-8-删除节点-delete-命令"><a href="#2-3-8-删除节点-delete-命令" class="headerlink" title="2.3.8. 删除节点(delete 命令)"></a>2.3.8. 删除节点(delete 命令)</h4><p>这个命令很简单，但是需要注意的一点是如果你要删除某一个节点，那么这个节点必须无子节点才行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: 127.0.0.1:2181(CONNECTED) 3] delete /node1/node1.1</span><br></pre></td></tr></table></figure><p>在后面我会介绍到 Java 客户端 API 的使用以及开源 ZooKeeper 客户端 ZkClient 和 Curator 的使用。</p><h2 id="3-ZooKeeper-Java客户端-Curator简单使用"><a href="#3-ZooKeeper-Java客户端-Curator简单使用" class="headerlink" title="3. ZooKeeper Java客户端 Curator简单使用"></a>3. ZooKeeper Java客户端 Curator简单使用</h2><p>Curator 是Netflix公司开源的一套 ZooKeeper Java客户端框架，相比于 Zookeeper 自带的客户端 zookeeper 来说，Curator 的封装更加完善，各种 API 都可以比较方便地使用。</p><p><a target="_blank" rel="noopener" href="https://github.com/Snailclimb/JavaGuide/blob/main/docs/distributed-system/distributed-process-coordination/zookeeper/images/curator.png"><img src="https://github.com/Snailclimb/JavaGuide/raw/main/docs/distributed-system/distributed-process-coordination/zookeeper/images/curator.png" alt="img"></a></p><p>下面我们就来简单地演示一下 Curator 的使用吧！</p><p>Curator4.0+版本对ZooKeeper 3.5.x支持比较好。开始之前，请先将下面的依赖添加进你的项目。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;curator-framework&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;4.2.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.curator&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;4.2.0&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="3-1-连接-ZooKeeper-客户端"><a href="#3-1-连接-ZooKeeper-客户端" class="headerlink" title="3.1. 连接 ZooKeeper 客户端"></a>3.1. 连接 ZooKeeper 客户端</h3><p>通过 <code>CuratorFrameworkFactory</code> 创建 <code>CuratorFramework</code> 对象，然后再调用 <code>CuratorFramework</code> 对象的 <code>start()</code> 方法即可！</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">private static final int BASE_SLEEP_TIME = 1000;</span><br><span class="line">private static final int MAX_RETRIES = 3;</span><br><span class="line"></span><br><span class="line">// Retry strategy. Retry 3 times, and will increase the sleep time between retries.</span><br><span class="line">RetryPolicy retryPolicy = new ExponentialBackoffRetry(BASE_SLEEP_TIME, MAX_RETRIES);</span><br><span class="line">CuratorFramework zkClient = CuratorFrameworkFactory.builder()</span><br><span class="line">    // the server to connect to (can be a server list)</span><br><span class="line">    .connectString(&quot;127.0.0.1:2181&quot;)</span><br><span class="line">    .retryPolicy(retryPolicy)</span><br><span class="line">    .build();</span><br><span class="line">zkClient.start();</span><br></pre></td></tr></table></figure><p>对于一些基本参数的说明：</p><ul><li><code>baseSleepTimeMs</code>：重试之间等待的初始时间</li><li><code>maxRetries</code> ：最大重试次数</li><li><code>connectString</code> ：要连接的服务器列表</li><li><code>retryPolicy</code> ：重试策略</li></ul><h3 id="3-2-数据节点的增删改查"><a href="#3-2-数据节点的增删改查" class="headerlink" title="3.2. 数据节点的增删改查"></a>3.2. 数据节点的增删改查</h3><h4 id="3-2-1-创建节点"><a href="#3-2-1-创建节点" class="headerlink" title="3.2.1. 创建节点"></a>3.2.1. 创建节点</h4><p>我们在 <a target="_blank" rel="noopener" href="https://github.com/Snailclimb/JavaGuide/blob/main/docs/distributed-system/distributed-process-coordination/zookeeper/zookeeper-intro.md">ZooKeeper常见概念解读</a> 中介绍到，我们通常是将 znode 分为 4 大类：</p><ul><li><strong>持久（PERSISTENT）节点</strong> ：一旦创建就一直存在即使 ZooKeeper 集群宕机，直到将其删除。</li><li><strong>临时（EPHEMERAL）节点</strong> ：临时节点的生命周期是与 <strong>客户端会话（session）</strong> 绑定的，<strong>会话消失则节点消失</strong> 。并且，临时节点 <strong>只能做叶子节点</strong> ，不能创建子节点。</li><li><strong>持久顺序（PERSISTENT_SEQUENTIAL）节点</strong> ：除了具有持久（PERSISTENT）节点的特性之外， 子节点的名称还具有顺序性。比如 <code>/node1/app0000000001</code> 、<code>/node1/app0000000002</code> 。</li><li><strong>临时顺序（EPHEMERAL_SEQUENTIAL）节点</strong> ：除了具备临时（EPHEMERAL）节点的特性之外，子节点的名称还具有顺序性。</li></ul><p>你在使用的ZooKeeper 的时候，会发现 <code>CreateMode</code> 类中实际有 7种 znode 类型 ，但是用的最多的还是上面介绍的 4 种。</p><p><strong>a.创建持久化节点</strong></p><p>你可以通过下面两种方式创建持久化的节点。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//注意:下面的代码会报错，下文说了具体原因</span><br><span class="line">zkClient.create().forPath(&quot;/node1/00001&quot;);</span><br><span class="line">zkClient.create().withMode(CreateMode.PERSISTENT).forPath(&quot;/node1/00002&quot;);</span><br></pre></td></tr></table></figure><p>但是，你运行上面的代码会报错，这是因为的父节点<code>node1</code>还未创建。</p><p>你可以先创建父节点 <code>node1</code> ，然后再执行上面的代码就不会报错了。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkClient.create().forPath(&quot;/node1&quot;);</span><br></pre></td></tr></table></figure><p>更推荐的方式是通过下面这行代码， <strong><code>creatingParentsIfNeeded()</code> 可以保证父节点不存在的时候自动创建父节点，这是非常有用的。</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkClient.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(&quot;/node1/00001&quot;);</span><br></pre></td></tr></table></figure><p><strong>b.创建临时节点</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkClient.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(&quot;/node1/00001&quot;);</span><br></pre></td></tr></table></figure><p><strong>c.创建节点并指定数据内容</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zkClient.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(&quot;/node1/00001&quot;,&quot;java&quot;.getBytes());</span><br><span class="line">zkClient.getData().forPath(&quot;/node1/00001&quot;);//获取节点的数据内容，获取到的是 byte数组</span><br></pre></td></tr></table></figure><p><strong>d.检测节点是否创建成功</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkClient.checkExists().forPath(&quot;/node1/00001&quot;);//不为null的话，说明节点创建成功</span><br></pre></td></tr></table></figure><h4 id="3-2-2-删除节点"><a href="#3-2-2-删除节点" class="headerlink" title="3.2.2. 删除节点"></a>3.2.2. 删除节点</h4><p><strong>a.删除一个子节点</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkClient.delete().forPath(&quot;/node1/00001&quot;);</span><br></pre></td></tr></table></figure><p><strong>b.删除一个节点以及其下的所有子节点</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkClient.delete().deletingChildrenIfNeeded().forPath(&quot;/node1&quot;);</span><br></pre></td></tr></table></figure><h4 id="3-2-3-获取-x2F-更新节点数据内容"><a href="#3-2-3-获取-x2F-更新节点数据内容" class="headerlink" title="3.2.3. 获取&#x2F;更新节点数据内容"></a>3.2.3. 获取&#x2F;更新节点数据内容</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zkClient.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(&quot;/node1/00001&quot;,&quot;java&quot;.getBytes());</span><br><span class="line">zkClient.getData().forPath(&quot;/node1/00001&quot;);//获取节点的数据内容</span><br><span class="line">zkClient.setData().forPath(&quot;/node1/00001&quot;,&quot;c++&quot;.getBytes());//更新节点数据内容</span><br></pre></td></tr></table></figure><h4 id="3-2-4-获取某个节点的所有子节点路径"><a href="#3-2-4-获取某个节点的所有子节点路径" class="headerlink" title="3.2.4. 获取某个节点的所有子节点路径"></a>3.2.4. 获取某个节点的所有子节点路径</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; childrenPaths = zkClient.getChildren().forPath(&quot;/node1&quot;);</span><br></pre></td></tr></table></figure><details class="details-reset details-overlay details-overlay-dark" id="jumpto-line-details-dialog" style="box-sizing:border-box;display:block"><summary data-hotkey="l" aria-label="Jump to line" role="button" style="box-sizing:border-box;display:list-item;cursor:pointer;list-style:none;transition:color 80ms cubic-bezier(.33,1,.68,1) 0s,background-color,box-shadow,border-color"></summary></details></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://enpsl.github.io/2021/02/17/2021-12-30-ZooKeeper%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%80%EF%BC%89/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="enpsl"><meta itemprop="description" content="my blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="彭诗亮的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"> <a href="/2021/02/17/2021-12-30-ZooKeeper%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%B8%80%EF%BC%89/" class="post-title-link" itemprop="url">ZooKeeper基础知识（一）</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-02-17 19:30:00" itemprop="dateCreated datePublished" datetime="2021-02-17T19:30:00+08:00">2021-02-17</time></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="ZooKeeper-相关概念总结-入门"><a href="#ZooKeeper-相关概念总结-入门" class="headerlink" title="ZooKeeper 相关概念总结(入门)"></a>ZooKeeper 相关概念总结(入门)</h1><h2 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h2><p>相信大家对 ZooKeeper 应该不算陌生。但是你真的了解 ZooKeeper 到底有啥用不？如果别人&#x2F;面试官让你给他讲讲对于 ZooKeeper 的认识，你能回答到什么地步呢？</p><p>拿我自己来说吧！我本人曾经使用 Dubbo 来做分布式项目的时候，使用了 ZooKeeper 作为注册中心。为了保证分布式系统能够同步访问某个资源，我还使用 ZooKeeper 做过分布式锁。另外，我在学习 Kafka 的时候，知道 Kafka 很多功能的实现依赖了 ZooKeeper。</p><p>前几天，总结项目经验的时候，我突然问自己 ZooKeeper 到底是个什么东西？想了半天，脑海中只是简单的能浮现出几句话：</p><ol><li>ZooKeeper 可以被用作注册中心、分布式锁；</li><li>ZooKeeper 是 Hadoop 生态系统的一员；</li><li>构建 ZooKeeper 集群的时候，使用的服务器最好是奇数台。</li></ol><p>由此可见，我对于 ZooKeeper 的理解仅仅是停留在了表面。</p><p>所以，通过本文，希望带大家稍微详细的了解一下 ZooKeeper 。如果没有学过 ZooKeeper ，那么本文将会是你进入 ZooKeeper 大门的垫脚砖。如果你已经接触过 ZooKeeper ，那么本文将带你回顾一下 ZooKeeper 的一些基础概念。</p><p>另外，本文不光会涉及到 ZooKeeper 的一些概念，后面的文章会介绍到 ZooKeeper 常见命令的使用以及使用 Apache Curator 作为 ZooKeeper 的客户端。</p><p><em>如果文章有任何需要改善和完善的地方，欢迎在评论区指出，共同进步！</em></p><h2 id="2-ZooKeeper-介绍"><a href="#2-ZooKeeper-介绍" class="headerlink" title="2. ZooKeeper 介绍"></a>2. ZooKeeper 介绍</h2><h3 id="2-1-ZooKeeper-由来"><a href="#2-1-ZooKeeper-由来" class="headerlink" title="2.1. ZooKeeper 由来"></a>2.1. ZooKeeper 由来</h3><p>正式介绍 ZooKeeper 之前，我们先来看看 ZooKeeper 的由来，还挺有意思的。</p><p>下面这段内容摘自《从 Paxos 到 ZooKeeper 》第四章第一节，推荐大家阅读一下：</p><blockquote><p>ZooKeeper 最早起源于雅虎研究院的一个研究小组。在当时，研究人员发现，在雅虎内部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调，但是这些系统往往都存在分布式单点问题。所以，雅虎的开发人员就试图开发一个通用的无单点问题的分布式协调框架，以便让开发人员将精力集中在处理业务逻辑上。</p><p>关于“ZooKeeper”这个项目的名字，其实也有一段趣闻。在立项初期，考虑到之前内部很多项目都是使用动物的名字来命名的（例如著名的 Pig 项目),雅虎的工程师希望给这个项目也取一个动物的名字。时任研究院的首席科学家 RaghuRamakrishnan 开玩笑地说：“在这样下去，我们这儿就变成动物园了！”此话一出，大家纷纷表示就叫动物园管理员吧一一一因为各个以动物命名的分布式组件放在一起，雅虎的整个分布式系统看上去就像一个大型的动物园了，而 ZooKeeper 正好要用来进行分布式环境的协调一一于是，ZooKeeper 的名字也就由此诞生了。</p></blockquote><h3 id="2-2-ZooKeeper-概览"><a href="#2-2-ZooKeeper-概览" class="headerlink" title="2.2. ZooKeeper 概览"></a>2.2. ZooKeeper 概览</h3><p>ZooKeeper 是一个开源的<strong>分布式协调服务</strong>，它的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。</p><blockquote><p><strong>原语：</strong> 操作系统或计算机网络用语范畴。是由若干条指令组成的，用于完成一定功能的一个过程。具有不可分割性·即原语的执行必须是连续的，在执行过程中不允许被中断。</p></blockquote><p><strong>ZooKeeper 为我们提供了高可用、高性能、稳定的分布式数据一致性解决方案，通常被用于实现诸如数据发布&#x2F;订阅、负载均衡、命名服务、分布式协调&#x2F;通知、集群管理、Master 选举、分布式锁和分布式队列等功能。</strong></p><p>另外，<strong>ZooKeeper 将数据保存在内存中，性能是非常棒的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景）。</strong></p><h3 id="2-3-ZooKeeper-特点"><a href="#2-3-ZooKeeper-特点" class="headerlink" title="2.3. ZooKeeper 特点"></a>2.3. ZooKeeper 特点</h3><ul><li><strong>顺序一致性：</strong> 从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。</li><li><strong>原子性：</strong> 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。</li><li><strong>单一系统映像 ：</strong> 无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。</li><li><strong>可靠性：</strong> 一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。</li></ul><h3 id="2-4-ZooKeeper-典型应用场景"><a href="#2-4-ZooKeeper-典型应用场景" class="headerlink" title="2.4. ZooKeeper 典型应用场景"></a>2.4. ZooKeeper 典型应用场景</h3><p>ZooKeeper 概览中，我们介绍到使用其通常被用于实现诸如数据发布&#x2F;订阅、负载均衡、命名服务、分布式协调&#x2F;通知、集群管理、Master 选举、分布式锁和分布式队列等功能。</p><p>下面选 3 个典型的应用场景来专门说说：</p><ol><li><strong>分布式锁</strong> ： 通过创建唯一节点获得分布式锁，当获得锁的一方执行完相关代码或者是挂掉之后就释放锁。</li><li><strong>命名服务</strong> ：可以通过 ZooKeeper 的顺序节点生成全局唯一 ID</li><li><strong>数据发布&#x2F;订阅</strong> ：通过 <strong>Watcher 机制</strong> 可以很方便地实现数据发布&#x2F;订阅。当你将数据发布到 ZooKeeper 被监听的节点上，其他机器可通过监听 ZooKeeper 上节点的变化来实现配置的动态更新。</li></ol><p>实际上，这些功能的实现基本都得益于 ZooKeeper 可以保存数据的功能，但是 ZooKeeper 不适合保存大量数据，这一点需要注意。</p><h3 id="2-5-有哪些著名的开源项目用到了-ZooKeeper"><a href="#2-5-有哪些著名的开源项目用到了-ZooKeeper" class="headerlink" title="2.5. 有哪些著名的开源项目用到了 ZooKeeper?"></a>2.5. 有哪些著名的开源项目用到了 ZooKeeper?</h3><ol><li><strong>Kafka</strong> : ZooKeeper 主要为 Kafka 提供 Broker 和 Topic 的注册以及多个 Partition 的负载均衡等功能。</li><li><strong>Hbase</strong> : ZooKeeper 为 Hbase 提供确保整个集群只有一个 Master 以及保存和提供 regionserver 状态信息（是否在线）等功能。</li><li><strong>Hadoop</strong> : ZooKeeper 为 Namenode 提供高可用支持。</li></ol><h2 id="3-ZooKeeper-重要概念解读"><a href="#3-ZooKeeper-重要概念解读" class="headerlink" title="3. ZooKeeper 重要概念解读"></a>3. ZooKeeper 重要概念解读</h2><p><em>破音：拿出小本本，下面的内容非常重要哦！</em></p><h3 id="3-1-Data-model（数据模型）"><a href="#3-1-Data-model（数据模型）" class="headerlink" title="3.1. Data model（数据模型）"></a>3.1. Data model（数据模型）</h3><p>ZooKeeper 数据模型采用层次化的多叉树形结构，每个节点上都可以存储数据，这些数据可以是数字、字符串或者是二级制序列。并且。每个节点还可以拥有 N 个子节点，最上层是根节点以“&#x2F;”来代表。每个数据节点在 ZooKeeper 中被称为 <strong>znode</strong>，它是 ZooKeeper 中数据的最小单元。并且，每个 znode 都一个唯一的路径标识。</p><p>强调一句：<strong>ZooKeeper 主要是用来协调服务的，而不是用来存储业务数据的，所以不要放比较大的数据在 znode 上，ZooKeeper 给出的上限是每个结点的数据大小最大是 1M。</strong></p><p>从下图可以更直观地看出：ZooKeeper 节点路径标识方式和 Unix 文件系统路径非常相似，都是由一系列使用斜杠”&#x2F;“进行分割的路径表示，开发人员可以向这个节点中写入数据，也可以在节点下面创建子节点。这些操作我们后面都会介绍到。</p><p><img src="https://javaguide.cn/assets/znode-structure.19119dbd.png" alt="ZooKeeper 数据模型"></p><h3 id="3-2-znode（数据节点）"><a href="#3-2-znode（数据节点）" class="headerlink" title="3.2. znode（数据节点）"></a>3.2. znode（数据节点）</h3><p>介绍了 ZooKeeper 树形数据模型之后，我们知道每个数据节点在 ZooKeeper 中被称为 <strong>znode</strong>，它是 ZooKeeper 中数据的最小单元。你要存放的数据就放在上面，是你使用 ZooKeeper 过程中经常需要接触到的一个概念。</p><h4 id="3-2-1-znode-4-种类型"><a href="#3-2-1-znode-4-种类型" class="headerlink" title="3.2.1. znode 4 种类型"></a>3.2.1. znode 4 种类型</h4><p>我们通常是将 znode 分为 4 大类：</p><ul><li><strong>持久（PERSISTENT）节点</strong> ：一旦创建就一直存在即使 ZooKeeper 集群宕机，直到将其删除。</li><li><strong>临时（EPHEMERAL）节点</strong> ：临时节点的生命周期是与 <strong>客户端会话（session）</strong> 绑定的，<strong>会话消失则节点消失</strong> 。并且，<strong>临时节点只能做叶子节点</strong> ，不能创建子节点。</li><li><strong>持久顺序（PERSISTENT_SEQUENTIAL）节点</strong> ：除了具有持久（PERSISTENT）节点的特性之外， 子节点的名称还具有顺序性。比如 <code>/node1/app0000000001</code> 、<code>/node1/app0000000002</code> 。</li><li><strong>临时顺序（EPHEMERAL_SEQUENTIAL）节点</strong> ：除了具备临时（EPHEMERAL）节点的特性之外，子节点的名称还具有顺序性。</li></ul><h4 id="3-2-2-znode-数据结构"><a href="#3-2-2-znode-数据结构" class="headerlink" title="3.2.2. znode 数据结构"></a>3.2.2. znode 数据结构</h4><p>每个 znode 由 2 部分组成:</p><ul><li><strong>stat</strong> ：状态信息</li><li><strong>data</strong> ： 节点存放的数据的具体内容</li></ul><p>如下所示，我通过 get 命令来获取 根目录下的 dubbo 节点的内容。（get 命令在下面会介绍到）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[zk: 127.0.0.1:2181(CONNECTED) 6] get /dubbo</span><br><span class="line"># 该数据节点关联的数据内容为空</span><br><span class="line">null</span><br><span class="line"># 下面是该数据节点的一些状态信息，其实就是 Stat 对象的格式化输出</span><br><span class="line">cZxid = 0x2</span><br><span class="line">ctime = Tue Nov 27 11:05:34 CST 2018</span><br><span class="line">mZxid = 0x2</span><br><span class="line">mtime = Tue Nov 27 11:05:34 CST 2018</span><br><span class="line">pZxid = 0x3</span><br><span class="line">cversion = 1</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 0</span><br><span class="line">numChildren = 1</span><br></pre></td></tr></table></figure><p>Stat 类中包含了一个数据节点的所有状态信息的字段，包括事务 ID-cZxid、节点创建时间-ctime 和子节点个数-numChildren 等等。</p><p>下面我们来看一下每个 znode 状态信息究竟代表的是什么吧！（下面的内容来源于《从 Paxos 到 ZooKeeper 分布式一致性原理与实践》，因为 Guide 确实也不是特别清楚，要学会参考资料的嘛！ ） ：</p><table><thead><tr><th>znode 状态信息</th><th>解释</th></tr></thead><tbody><tr><td>cZxid</td><td>create ZXID，即该数据节点被创建时的事务 id</td></tr><tr><td>ctime</td><td>create time，即该节点的创建时间</td></tr><tr><td>mZxid</td><td>modified ZXID，即该节点最终一次更新时的事务 id</td></tr><tr><td>mtime</td><td>modified time，即该节点最后一次的更新时间</td></tr><tr><td>pZxid</td><td>该节点的子节点列表最后一次修改时的事务 id，只有子节点列表变更才会更新 pZxid，子节点内容变更不会更新</td></tr><tr><td>cversion</td><td>子节点版本号，当前节点的子节点每次变化时值增加 1</td></tr><tr><td>dataVersion</td><td>数据节点内容版本号，节点创建时为 0，每更新一次节点内容(不管内容有无变化)该版本号的值增加 1</td></tr><tr><td>aclVersion</td><td>节点的 ACL 版本号，表示该节点 ACL 信息变更次数</td></tr><tr><td>ephemeralOwner</td><td>创建该临时节点的会话的 sessionId；如果当前节点为持久节点，则 ephemeralOwner&#x3D;0</td></tr><tr><td>dataLength</td><td>数据节点内容长度</td></tr><tr><td>numChildren</td><td>当前节点的子节点个数</td></tr></tbody></table><h3 id="3-3-版本（version）"><a href="#3-3-版本（version）" class="headerlink" title="3.3. 版本（version）"></a>3.3. 版本（version）</h3><p>在前面我们已经提到，对应于每个 znode，ZooKeeper 都会为其维护一个叫作 <strong>Stat</strong> 的数据结构，Stat 中记录了这个 znode 的三个相关的版本：</p><ul><li><strong>dataVersion</strong> ：当前 znode 节点的版本号</li><li><strong>cversion</strong> ： 当前 znode 子节点的版本</li><li><strong>aclVersion</strong> ： 当前 znode 的 ACL 的版本。</li></ul><h3 id="3-4-ACL（权限控制）"><a href="#3-4-ACL（权限控制）" class="headerlink" title="3.4. ACL（权限控制）"></a>3.4. ACL（权限控制）</h3><p>ZooKeeper 采用 ACL（AccessControlLists）策略来进行权限控制，类似于 UNIX 文件系统的权限控制。</p><p>对于 znode 操作的权限，ZooKeeper 提供了以下 5 种：</p><ul><li><strong>CREATE</strong> : 能创建子节点</li><li><strong>READ</strong> ：能获取节点数据和列出其子节点</li><li><strong>WRITE</strong> : 能设置&#x2F;更新节点数据</li><li><strong>DELETE</strong> : 能删除子节点</li><li><strong>ADMIN</strong> : 能设置节点 ACL 的权限</li></ul><p>其中尤其需要注意的是，<strong>CREATE</strong> 和 <strong>DELETE</strong> 这两种权限都是针对 <strong>子节点</strong> 的权限控制。</p><p>对于身份认证，提供了以下几种方式：</p><ul><li><strong>world</strong> ： 默认方式，所有用户都可无条件访问。</li><li><strong>auth</strong> :不使用任何 id，代表任何已认证的用户。</li><li><strong>digest</strong> :用户名:密码认证方式： <em>username:password</em> 。</li><li><strong>ip</strong> : 对指定 ip 进行限制。</li></ul><h3 id="3-5-Watcher（事件监听器）"><a href="#3-5-Watcher（事件监听器）" class="headerlink" title="3.5. Watcher（事件监听器）"></a>3.5. Watcher（事件监听器）</h3><p>Watcher（事件监听器），是 ZooKeeper 中的一个很重要的特性。ZooKeeper 允许用户在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候，ZooKeeper 服务端会将事件通知到感兴趣的客户端上去，该机制是 ZooKeeper 实现分布式协调服务的重要特性。</p><p><img src="https://javaguide.cn/assets/watche%E6%9C%BA%E5%88%B6.f523bd89.png" alt="watcher机制"></p><p><em>破音：非常有用的一个特性，都拿出小本本记好了，后面用到 ZooKeeper 基本离不开 Watcher（事件监听器）机制。</em></p><h3 id="3-6-会话（Session）"><a href="#3-6-会话（Session）" class="headerlink" title="3.6. 会话（Session）"></a>3.6. 会话（Session）</h3><p>Session 可以看作是 ZooKeeper 服务器与客户端的之间的一个 TCP 长连接，通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向 ZooKeeper 服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的 Watcher 事件通知。</p><p>Session 有一个属性叫做：<code>sessionTimeout</code> ，<code>sessionTimeout</code> 代表会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在<code>sessionTimeout</code>规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。</p><p>另外，在为客户端创建会话之前，服务端首先会为每个客户端都分配一个 <code>sessionID</code>。由于 <code>sessionID</code>是 ZooKeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 <code>sessionID</code> 的，因此，无论是哪台服务器为客户端分配的 <code>sessionID</code>，都务必保证全局唯一。</p><h2 id="4-ZooKeeper-集群"><a href="#4-ZooKeeper-集群" class="headerlink" title="4. ZooKeeper 集群"></a>4. ZooKeeper 集群</h2><p>为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。通常 3 台服务器就可以构成一个 ZooKeeper 集群了。ZooKeeper 官方提供的架构图就是一个 ZooKeeper 集群整体对外提供服务。</p><p><img src="https://javaguide.cn/assets/zookeeper%E9%9B%86%E7%BE%A4.6fdcc61e.png" alt="img"></p><p>上图中每一个 Server 代表一个安装 ZooKeeper 服务的服务器。组成 ZooKeeper 服务的服务器都会在内存中维护当前的服务器状态，并且每台服务器之间都互相保持着通信。集群间通过 ZAB 协议（ZooKeeper Atomic Broadcast）来保持数据的一致性。</p><p><strong>最典型集群模式： Master&#x2F;Slave 模式（主备模式）</strong>。在这种模式中，通常 Master 服务器作为主服务器提供写服务，其他的 Slave 服务器从服务器通过异步复制的方式获取 Master 服务器最新的数据提供读服务。</p><h3 id="4-1-ZooKeeper-集群角色"><a href="#4-1-ZooKeeper-集群角色" class="headerlink" title="4.1. ZooKeeper 集群角色"></a>4.1. ZooKeeper 集群角色</h3><p>但是，在 ZooKeeper 中没有选择传统的 Master&#x2F;Slave 概念，而是引入了 Leader、Follower 和 Observer 三种角色。如下图所示</p><p><img src="https://javaguide.cn/assets/zookeeper%E9%9B%86%E7%BE%A4%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.ffff8ef5.png" alt="img"></p><p>ZooKeeper 集群中的所有机器通过一个 <strong>Leader 选举过程</strong> 来选定一台称为 “<strong>Leader</strong>” 的机器，Leader 既可以为客户端提供写服务又能提供读服务。除了 Leader 外，<strong>Follower</strong> 和 <strong>Observer</strong> 都只能提供读服务。Follower 和 Observer 唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。</p><table><thead><tr><th>角色</th><th>说明</th></tr></thead><tbody><tr><td>Leader</td><td>为客户端提供读和写的服务，负责投票的发起和决议，更新系统状态。</td></tr><tr><td>Follower</td><td>为客户端提供读服务，如果是写服务则转发给 Leader。参与选举过程中的投票。</td></tr><tr><td>Observer</td><td>为客户端提供读服务，如果是写服务则转发给 Leader。不参与选举过程中的投票，也不参与“过半写成功”策略。在不影响写性能的情况下提升集群的读性能。此角色于 ZooKeeper3.3 系列新增的角色。</td></tr></tbody></table><p>当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，就会进入 Leader 选举过程，这个过程会选举产生新的 Leader 服务器。</p><p>这个过程大致是这样的：</p><ol><li><strong>Leader election（选举阶段）</strong>：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。</li><li><strong>Discovery（发现阶段）</strong> ：在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。</li><li><strong>Synchronization（同步阶段）</strong> :同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后 准 leader 才会成为真正的 leader。</li><li><strong>Broadcast（广播阶段）</strong> :到了这个阶段，ZooKeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。</li></ol><h3 id="4-2-ZooKeeper-集群中的服务器状态"><a href="#4-2-ZooKeeper-集群中的服务器状态" class="headerlink" title="4.2. ZooKeeper 集群中的服务器状态"></a>4.2. ZooKeeper 集群中的服务器状态</h3><ul><li><strong>LOOKING</strong> ：寻找 Leader。</li><li><strong>LEADING</strong> ：Leader 状态，对应的节点为 Leader。</li><li><strong>FOLLOWING</strong> ：Follower 状态，对应的节点为 Follower。</li><li><strong>OBSERVING</strong> ：Observer 状态，对应节点为 Observer，该节点不参与 Leader 选举。</li></ul><h3 id="4-3-ZooKeeper-集群为啥最好奇数台？"><a href="#4-3-ZooKeeper-集群为啥最好奇数台？" class="headerlink" title="4.3. ZooKeeper 集群为啥最好奇数台？"></a>4.3. ZooKeeper 集群为啥最好奇数台？</h3><p>ZooKeeper 集群在宕掉几个 ZooKeeper 服务器之后，如果剩下的 ZooKeeper 服务器个数大于宕掉的个数的话整个 ZooKeeper 才依然可用。假如我们的集群中有 n 台 ZooKeeper 服务器，那么也就是剩下的服务数必须大于 n&#x2F;2。先说一下结论，2n 和 2n-1 的容忍度是一样的，都是 n-1，大家可以先自己仔细想一想，这应该是一个很简单的数学问题了。</p><p>比如假如我们有 3 台，那么最大允许宕掉 1 台 ZooKeeper 服务器，如果我们有 4 台的的时候也同样只允许宕掉 1 台。 假如我们有 5 台，那么最大允许宕掉 2 台 ZooKeeper 服务器，如果我们有 6 台的的时候也同样只允许宕掉 2 台。</p><p>综上，何必增加那一个不必要的 ZooKeeper 呢？</p><h3 id="4-4-ZooKeeper-选举的过半机制防止脑裂"><a href="#4-4-ZooKeeper-选举的过半机制防止脑裂" class="headerlink" title="4.4. ZooKeeper 选举的过半机制防止脑裂"></a>4.4. ZooKeeper 选举的过半机制防止脑裂</h3><p><strong>何为集群脑裂？</strong></p><p>对于一个集群，通常多台机器会部署在不同机房，来提高这个集群的可用性。保证可用性的同时，会发生一种机房间网络线路故障，导致机房间网络不通，而集群被割裂成几个小集群。这时候子集群各自选主导致“脑裂”的情况。</p><p>举例说明：比如现在有一个由 6 台服务器所组成的一个集群，部署在了 2 个机房，每个机房 3 台。正常情况下只有 1 个 leader，但是当两个机房中间网络断开的时候，每个机房的 3 台服务器都会认为另一个机房的 3 台服务器下线，而选出自己的 leader 并对外提供服务。若没有过半机制，当网络恢复的时候会发现有 2 个 leader。仿佛是 1 个大脑（leader）分散成了 2 个大脑，这就发生了脑裂现象。脑裂期间 2 个大脑都可能对外提供了服务，这将会带来数据一致性等问题。</p><p><strong>过半机制是如何防止脑裂现象产生的？</strong></p><p>ZooKeeper 的过半机制导致不可能产生 2 个 leader，因为少于等于一半是不可能产生 leader 的，这就使得不论机房的机器如何分配都不可能发生脑裂。</p><h2 id="5-ZAB-协议和-Paxos-算法"><a href="#5-ZAB-协议和-Paxos-算法" class="headerlink" title="5. ZAB 协议和 Paxos 算法"></a>5. ZAB 协议和 Paxos 算法</h2><p>Paxos 算法应该可以说是 ZooKeeper 的灵魂了。但是，ZooKeeper 并没有完全采用 Paxos 算法 ，而是使用 ZAB 协议作为其保证数据一致性的核心算法。另外，在 ZooKeeper 的官方文档中也指出，ZAB 协议并不像 Paxos 算法那样，是一种通用的分布式一致性算法，它是一种特别为 Zookeeper 设计的崩溃可恢复的原子消息广播算法。</p><h3 id="5-1-ZAB-协议介绍"><a href="#5-1-ZAB-协议介绍" class="headerlink" title="5.1. ZAB 协议介绍"></a>5.1. ZAB 协议介绍</h3><p>ZAB（ZooKeeper Atomic Broadcast 原子广播） 协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。 在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。</p><h3 id="5-2-ZAB-协议两种基本的模式：崩溃恢复和消息广播"><a href="#5-2-ZAB-协议两种基本的模式：崩溃恢复和消息广播" class="headerlink" title="5.2. ZAB 协议两种基本的模式：崩溃恢复和消息广播"></a>5.2. ZAB 协议两种基本的模式：崩溃恢复和消息广播</h3><p>ZAB 协议包括两种基本的模式，分别是</p><ul><li><strong>崩溃恢复</strong> ：当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进入恢复模式并选举产生新的 Leader 服务器。当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该 Leader 服务器完成了状态同步之后，ZAB 协议就会退出恢复模式。其中，<strong>所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和 Leader 服务器的数据状态保持一致</strong>。</li><li><strong>消息广播</strong> ：<strong>当集群中已经有过半的 Follower 服务器完成了和 Leader 服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。</strong> 当一台同样遵守 ZAB 协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个 Leader 服务器在负责进行消息广播，那么新加入的服务器就会自觉地进入数据恢复模式：找到 Leader 所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。</li></ul><p>关于 <strong>ZAB 协议&amp;Paxos 算法</strong> 需要讲和理解的东西太多了，具体可以看下面这两篇文章：</p><ul><li><a target="_blank" rel="noopener" href="http://codemacro.com/2014/10/15/explain-poxos/">图解 Paxos 一致性协议</a></li><li><a target="_blank" rel="noopener" href="https://dbaplus.cn/news-141-1875-1.html">Zookeeper ZAB 协议分析</a></li></ul><h2 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h2><ol><li>ZooKeeper 本身就是一个分布式程序（只要半数以上节点存活，ZooKeeper 就能正常服务）。</li><li>为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。</li><li>ZooKeeper 将数据保存在内存中，这也就保证了 高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，此限制也是保持 znode 中存储的数据量较小的进一步原因）。</li><li>ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地明显，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。）</li><li>ZooKeeper 有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个 znode 被创建了，除非主动进行 znode 的移除操作，否则这个 znode 将一直保存在 ZooKeeper 上。</li><li>ZooKeeper 底层其实只提供了两个功能：① 管理（存储、读取）用户程序提交的数据；② 为用户程序提供数据节点监听服务。</li></ol><h2 id="7-参考"><a href="#7-参考" class="headerlink" title="7. 参考"></a>7. 参考</h2><ol><li>《从 Paxos 到 ZooKeeper 分布式一致性原理与实践》</li></ol></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://enpsl.github.io/2021/02/17/2021-12-30-ZooKeeper%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="enpsl"><meta itemprop="description" content="my blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="彭诗亮的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"> <a href="/2021/02/17/2021-12-30-ZooKeeper%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%EF%BC%88%E4%BA%8C%EF%BC%89/" class="post-title-link" itemprop="url">ZooKeeper基础知识（二）</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-02-17 19:30:00" itemprop="dateCreated datePublished" datetime="2021-02-17T19:30:00+08:00">2021-02-17</time></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="1-好久不见"><a href="#1-好久不见" class="headerlink" title="1. 好久不见"></a>1. 好久不见</h2><blockquote><p>文章很长，先赞后看，养成习惯。❤️ 🧡 💛 💚 💙 💜</p></blockquote><h2 id="2-什么是ZooKeeper"><a href="#2-什么是ZooKeeper" class="headerlink" title="2. 什么是ZooKeeper"></a>2. 什么是ZooKeeper</h2><p><code>ZooKeeper</code> 由 <code>Yahoo</code> 开发，后来捐赠给了 <code>Apache</code> ，现已成为 <code>Apache</code> 顶级项目。<code>ZooKeeper</code> 是一个开源的分布式应用程序协调服务器，其为分布式系统提供一致性服务。其一致性是通过基于 <code>Paxos</code> 算法的 <code>ZAB</code> 协议完成的。其主要功能包括：配置维护、分布式同步、集群管理、分布式事务等。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/f61e695882d4e6820fc5e9b9c37dfd61f5d606c8b7385687777ea7fc995a32d9/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f37633334393839316237373336373135313461363866303537623265313466382e706e67"><img src="https://img-blog.csdnimg.cn/img_convert/7c349891b773671514a68f057b2e14f8.png" alt="zookeeper"></a></p><p>简单来说， <code>ZooKeeper</code> 是一个 <strong>分布式协调服务框架</strong> 。分布式？协调服务？这啥玩意？🤔🤔</p><p>其实解释到分布式这个概念的时候，我发现有些同学并不是能把 **分布式和集群 **这两个概念很好的理解透。前段时间有同学和我探讨起分布式的东西，他说分布式不就是加机器吗？一台机器不够用再加一台抗压呗。当然加机器这种说法也无可厚非，你一个分布式系统必定涉及到多个机器，但是你别忘了，计算机学科中还有一个相似的概念—— <code>Cluster</code> ，集群不也是加机器吗？但是 集群 和 分布式 其实就是两个完全不同的概念。</p><p>比如，我现在有一个秒杀服务，并发量太大单机系统承受不住，那我加几台服务器也 <strong>一样</strong> 提供秒杀服务，这个时候就是 <strong><code>Cluster</code> 集群</strong> 。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/ffcb080eb66f242ffcd8d2047a7f46aa.png" alt="cluster"></p><p>但是，我现在换一种方式，我将一个秒杀服务 <strong>拆分成多个子服务</strong> ，比如创建订单服务，增加积分服务，扣优惠券服务等等，<strong>然后我将这些子服务都部署在不同的服务器上</strong> ，这个时候就是 <strong><code>Distributed</code> 分布式</strong> 。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/07191f38aa947b0075e5c0a6a019a11d.png" alt="distributed"></p><p>而我为什么反驳同学所说的分布式就是加机器呢？因为我认为加机器更加适用于构建集群，因为它真是只有加机器。而对于分布式来说，你首先需要将业务进行拆分，然后再加机器（不仅仅是加机器那么简单），同时你还要去解决分布式带来的一系列问题。</p><p><img src="https://img-blog.csdnimg.cn/img_convert/2b2fbc21abfb3f6547a2121f28c6d00f.png" alt="img"></p><p>比如各个分布式组件如何协调起来，如何减少各个系统之间的耦合度，分布式事务的处理，如何去配置整个分布式系统等等。<code>ZooKeeper</code> 主要就是解决这些问题的。</p><h2 id="3-一致性问题"><a href="#3-一致性问题" class="headerlink" title="3. 一致性问题"></a>3. 一致性问题</h2><p>设计一个分布式系统必定会遇到一个问题—— <strong>因为分区容忍性（partition tolerance）的存在，就必定要求我们需要在系统可用性（availability）和数据一致性（consistency）中做出权衡</strong> 。这就是著名的 <code>CAP</code> 定理。</p><p>理解起来其实很简单，比如说把一个班级作为整个系统，而学生是系统中的一个个独立的子系统。这个时候班里的小红小明偷偷谈恋爱被班里的大嘴巴小花发现了，小花欣喜若狂告诉了周围的人，然后小红小明谈恋爱的消息在班级里传播起来了。当在消息的传播（散布）过程中，你抓到一个同学问他们的情况，如果回答你不知道，那么说明整个班级系统出现了数据不一致的问题（因为小花已经知道这个消息了）。而如果他直接不回答你，因为整个班级有消息在进行传播（为了保证一致性，需要所有人都知道才可提供服务），这个时候就出现了系统的可用性问题。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/d5cf7c19c674ced29a7a1d71795941b0961087c402a8017859bd7c457379de83/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f33346666666634316636636134663232316361396439616436663062353437302e706e67"><img src="https://camo.githubusercontent.com/d5cf7c19c674ced29a7a1d71795941b0961087c402a8017859bd7c457379de83/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f33346666666634316636636134663232316361396439616436663062353437302e706e67" alt="img"></a></p><p>而上述前者就是 <code>Eureka</code> 的处理方式，它保证了AP（可用性），后者就是我们今天所要讲的 <code>ZooKeeper</code> 的处理方式，它保证了CP（数据一致性）。</p><h2 id="4-一致性协议和算法"><a href="#4-一致性协议和算法" class="headerlink" title="4. 一致性协议和算法"></a>4. 一致性协议和算法</h2><p>而为了解决数据一致性问题，在科学家和程序员的不断探索中，就出现了很多的一致性协议和算法。比如 2PC（两阶段提交），3PC（三阶段提交），Paxos算法等等。</p><p>这时候请你思考一个问题，同学之间如果采用传纸条的方式去传播消息，那么就会出现一个问题——我咋知道我的小纸条有没有传到我想要传递的那个人手中呢？万一被哪个小家伙给劫持篡改了呢，对吧？</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/ccbffd4b7bfc60eb24b17ecb6deb520b1c21f44a3b557d3dfd4af38c94eca760/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f62306530316665333231336463633135333563333132393862613962646662632e706e67"><img src="https://camo.githubusercontent.com/ccbffd4b7bfc60eb24b17ecb6deb520b1c21f44a3b557d3dfd4af38c94eca760/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f62306530316665333231336463633135333563333132393862613962646662632e706e67" alt="img"></a></p><p>这个时候就引申出一个概念—— <strong>拜占庭将军问题</strong> 。它意指 <strong>在不可靠信道上试图通过消息传递的方式达到一致性是不可能的</strong>， 所以所有的一致性算法的 <strong>必要前提</strong> 就是安全可靠的消息通道。</p><p>而为什么要去解决数据一致性的问题？你想想，如果一个秒杀系统将服务拆分成了下订单和加积分服务，这两个服务部署在不同的机器上了，万一在消息的传播过程中积分系统宕机了，总不能你这边下了订单却没加积分吧？你总得保证两边的数据需要一致吧？</p><h3 id="4-1-2PC（两阶段提交）"><a href="#4-1-2PC（两阶段提交）" class="headerlink" title="4.1. 2PC（两阶段提交）"></a>4.1. 2PC（两阶段提交）</h3><p>两阶段提交是一种保证分布式系统数据一致性的协议，现在很多数据库都是采用的两阶段提交协议来完成 <strong>分布式事务</strong> 的处理。</p><p>在介绍2PC之前，我们先来想想分布式事务到底有什么问题呢？</p><p>还拿秒杀系统的下订单和加积分两个系统来举例吧（我想你们可能都吐了🤮🤮🤮），我们此时下完订单会发个消息给积分系统告诉它下面该增加积分了。如果我们仅仅是发送一个消息也不收回复，那么我们的订单系统怎么能知道积分系统的收到消息的情况呢？如果我们增加一个收回复的过程，那么当积分系统收到消息后返回给订单系统一个 <code>Response</code> ，但在中间出现了网络波动，那个回复消息没有发送成功，订单系统是不是以为积分系统消息接收失败了？它是不是会回滚事务？但此时积分系统是成功收到消息的，它就会去处理消息然后给用户增加积分，这个时候就会出现积分加了但是订单没下成功。</p><p>所以我们所需要解决的是在分布式系统中，整个调用链中，我们所有服务的数据处理要么都成功要么都失败，即所有服务的 <strong>原子性问题</strong> 。</p><p>在两阶段提交中，主要涉及到两个角色，分别是协调者和参与者。</p><p>第一阶段：当要执行一个分布式事务的时候，事务发起者首先向协调者发起事务请求，然后协调者会给所有参与者发送 <code>prepare</code> 请求（其中包括事务内容）告诉参与者你们需要执行事务了，如果能执行我发的事务内容那么就先执行但不提交，执行后请给我回复。然后参与者收到 <code>prepare</code> 消息后，他们会开始执行事务（但不提交），并将 <code>Undo</code> 和 <code>Redo</code> 信息记入事务日志中，之后参与者就向协调者反馈是否准备好了。</p><p>第二阶段：第二阶段主要是协调者根据参与者反馈的情况来决定接下来是否可以进行事务的提交操作，即提交事务或者回滚事务。</p><p>比如这个时候 <strong>所有的参与者</strong> 都返回了准备好了的消息，这个时候就进行事务的提交，协调者此时会给所有的参与者发送 <strong><code>Commit</code> 请求</strong> ，当参与者收到 <code>Commit</code> 请求的时候会执行前面执行的事务的 <strong>提交操作</strong> ，提交完毕之后将给协调者发送提交成功的响应。</p><p>而如果在第一阶段并不是所有参与者都返回了准备好了的消息，那么此时协调者将会给所有参与者发送 <strong>回滚事务的 <code>rollback</code> 请求</strong>，参与者收到之后将会 <strong>回滚它在第一阶段所做的事务处理</strong> ，然后再将处理情况返回给协调者，最终协调者收到响应后便给事务发起者返回处理失败的结果。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/36a877fb15e7da608af2f12c5279424b796782ff1075e2fffd1351e8180af7db/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f37636534653430623638643632353637366262343263323965666365303436612e706e67"><img src="https://camo.githubusercontent.com/36a877fb15e7da608af2f12c5279424b796782ff1075e2fffd1351e8180af7db/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f37636534653430623638643632353637366262343263323965666365303436612e706e67" alt="2PC流程"></a></p><p>个人觉得 2PC 实现得还是比较鸡肋的，因为事实上它只解决了各个事务的原子性问题，随之也带来了很多的问题。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/8a9f827367cc9e4971b6fd22580e28734d3913244bcb85e829598f0be44d7035/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f39616631616436383531373536316138653966356433343535613332313332642e706e67"><img src="https://camo.githubusercontent.com/8a9f827367cc9e4971b6fd22580e28734d3913244bcb85e829598f0be44d7035/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f39616631616436383531373536316138653966356433343535613332313332642e706e67" alt="img"></a></p><ul><li><strong>单点故障问题</strong>，如果协调者挂了那么整个系统都处于不可用的状态了。</li><li><strong>阻塞问题</strong>，即当协调者发送 <code>prepare</code> 请求，参与者收到之后如果能处理那么它将会进行事务的处理但并不提交，这个时候会一直占用着资源不释放，如果此时协调者挂了，那么这些资源都不会再释放了，这会极大影响性能。</li><li><strong>数据不一致问题</strong>，比如当第二阶段，协调者只发送了一部分的 <code>commit</code> 请求就挂了，那么也就意味着，收到消息的参与者会进行事务的提交，而后面没收到的则不会进行事务提交，那么这时候就会产生数据不一致性问题。</li></ul><h3 id="4-2-3PC（三阶段提交）"><a href="#4-2-3PC（三阶段提交）" class="headerlink" title="4.2. 3PC（三阶段提交）"></a>4.2. 3PC（三阶段提交）</h3><p>因为2PC存在的一系列问题，比如单点，容错机制缺陷等等，从而产生了 <strong>3PC（三阶段提交）</strong> 。那么这三阶段又分别是什么呢？</p><blockquote><p>千万不要吧PC理解成个人电脑了，其实他们是 phase-commit 的缩写，即阶段提交。</p></blockquote><ol><li><strong>CanCommit阶段</strong>：协调者向所有参与者发送 <code>CanCommit</code> 请求，参与者收到请求后会根据自身情况查看是否能执行事务，如果可以则返回 YES 响应并进入预备状态，否则返回 NO 。</li><li><strong>PreCommit阶段</strong>：协调者根据参与者返回的响应来决定是否可以进行下面的 <code>PreCommit</code> 操作。如果上面参与者返回的都是 YES，那么协调者将向所有参与者发送 <code>PreCommit</code> 预提交请求，<strong>参与者收到预提交请求后，会进行事务的执行操作，并将 <code>Undo</code> 和 <code>Redo</code> 信息写入事务日志中</strong> ，最后如果参与者顺利执行了事务则给协调者返回成功的响应。如果在第一阶段协调者收到了 <strong>任何一个 NO</strong> 的信息，或者 <strong>在一定时间内</strong> 并没有收到全部的参与者的响应，那么就会中断事务，它会向所有参与者发送中断请求（abort），参与者收到中断请求之后会立即中断事务，或者在一定时间内没有收到协调者的请求，它也会中断事务。</li><li><strong>DoCommit阶段</strong>：这个阶段其实和 <code>2PC</code> 的第二阶段差不多，如果协调者收到了所有参与者在 <code>PreCommit</code> 阶段的 YES 响应，那么协调者将会给所有参与者发送 <code>DoCommit</code> 请求，<strong>参与者收到 <code>DoCommit</code> 请求后则会进行事务的提交工作</strong>，完成后则会给协调者返回响应，协调者收到所有参与者返回的事务提交成功的响应之后则完成事务。若协调者在 <code>PreCommit</code> 阶段 <strong>收到了任何一个 NO 或者在一定时间内没有收到所有参与者的响应</strong> ，那么就会进行中断请求的发送，参与者收到中断请求后则会 <strong>通过上面记录的回滚日志</strong> 来进行事务的回滚操作，并向协调者反馈回滚状况，协调者收到参与者返回的消息后，中断事务。</li></ol><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/1f5526f43f64af9da097a3e968567288354283c2abb7f997c90ed24ca969350f/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f64306234343336316337343635393366373061366534326332393862343133612e706e67"><img src="https://camo.githubusercontent.com/1f5526f43f64af9da097a3e968567288354283c2abb7f997c90ed24ca969350f/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f64306234343336316337343635393366373061366534326332393862343133612e706e67" alt="3PC流程"></a></p><blockquote><p>这里是 <code>3PC</code> 在成功的环境下的流程图，你可以看到 <code>3PC</code> 在很多地方进行了超时中断的处理，比如协调者在指定时间内为收到全部的确认消息则进行事务中断的处理，这样能 <strong>减少同步阻塞的时间</strong> 。还有需要注意的是，**<code>3PC</code> 在 <code>DoCommit</code> 阶段参与者如未收到协调者发送的提交事务的请求，它会在一定时间内进行事务的提交<strong>。为什么这么做呢？是因为这个时候我们肯定</strong>保证了在第一阶段所有的协调者全部返回了可以执行事务的响应<strong>，这个时候我们有理由</strong>相信其他系统都能进行事务的执行和提交<strong>，所以</strong>不管**协调者有没有发消息给参与者，进入第三阶段参与者都会进行事务的提交操作。</p></blockquote><p>总之，<code>3PC</code> 通过一系列的超时机制很好的缓解了阻塞问题，但是最重要的一致性并没有得到根本的解决，比如在 <code>PreCommit</code> 阶段，当一个参与者收到了请求之后其他参与者和协调者挂了或者出现了网络分区，这个时候收到消息的参与者都会进行事务提交，这就会出现数据不一致性问题。</p><p>所以，要解决一致性问题还需要靠 <code>Paxos</code> 算法⭐️ ⭐️ ⭐️ 。</p><h3 id="4-3-Paxos-算法"><a href="#4-3-Paxos-算法" class="headerlink" title="4.3. Paxos 算法"></a>4.3. <code>Paxos</code> 算法</h3><p><code>Paxos</code> 算法是基于<strong>消息传递且具有高度容错特性的一致性算法</strong>，是目前公认的解决分布式一致性问题最有效的算法之一，<strong>其解决的问题就是在分布式系统中如何就某个值（决议）达成一致</strong> 。</p><p>在 <code>Paxos</code> 中主要有三个角色，分别为 <code>Proposer提案者</code>、<code>Acceptor表决者</code>、<code>Learner学习者</code>。<code>Paxos</code> 算法和 <code>2PC</code> 一样，也有两个阶段，分别为 <code>Prepare</code> 和 <code>accept</code> 阶段。</p><h4 id="4-3-1-prepare-阶段"><a href="#4-3-1-prepare-阶段" class="headerlink" title="4.3.1. prepare 阶段"></a>4.3.1. prepare 阶段</h4><ul><li><code>Proposer提案者</code>：负责提出 <code>proposal</code>，每个提案者在提出提案时都会首先获取到一个 <strong>具有全局唯一性的、递增的提案编号N</strong>，即在整个集群中是唯一的编号 N，然后将该编号赋予其要提出的提案，在<strong>第一阶段是只将提案编号发送给所有的表决者</strong>。</li><li><code>Acceptor表决者</code>：每个表决者在 <code>accept</code> 某提案后，会将该提案编号N记录在本地，这样每个表决者中保存的已经被 accept 的提案中会存在一个<strong>编号最大的提案</strong>，其编号假设为 <code>maxN</code>。每个表决者仅会 <code>accept</code> 编号大于自己本地 <code>maxN</code> 的提案，在批准提案时表决者会将以前接受过的最大编号的提案作为响应反馈给 <code>Proposer</code> 。</li></ul><blockquote><p>下面是 <code>prepare</code> 阶段的流程图，你可以对照着参考一下。</p></blockquote><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/958acf97caf9cb2405b7004dceb966a4c587f43bcc78af3bf150c932dbdf53e8/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f32326538643531326439353436373662646630636339326432303061663865662e706e67"><img src="https://camo.githubusercontent.com/958acf97caf9cb2405b7004dceb966a4c587f43bcc78af3bf150c932dbdf53e8/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f32326538643531326439353436373662646630636339326432303061663865662e706e67" alt="paxos第一阶段"></a></p><h4 id="4-3-2-accept-阶段"><a href="#4-3-2-accept-阶段" class="headerlink" title="4.3.2. accept 阶段"></a>4.3.2. accept 阶段</h4><p>当一个提案被 <code>Proposer</code> 提出后，如果 <code>Proposer</code> 收到了超过半数的 <code>Acceptor</code> 的批准（<code>Proposer</code> 本身同意），那么此时 <code>Proposer</code> 会给所有的 <code>Acceptor</code> 发送真正的提案（你可以理解为第一阶段为试探），这个时候 <code>Proposer</code> 就会发送提案的内容和提案编号。</p><p>表决者收到提案请求后会再次比较本身已经批准过的最大提案编号和该提案编号，如果该提案编号 <strong>大于等于</strong> 已经批准过的最大提案编号，那么就 <code>accept</code> 该提案（此时执行提案内容但不提交），随后将情况返回给 <code>Proposer</code> 。如果不满足则不回应或者返回 NO 。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/b14a7f3dbb14ca52f7c2c16ec32f607c16319a11da5e512b2270fdf7691c42de/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f62383235333666393536663730613538346336613230633130313133663232352e706e67"><img src="https://camo.githubusercontent.com/b14a7f3dbb14ca52f7c2c16ec32f607c16319a11da5e512b2270fdf7691c42de/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f62383235333666393536663730613538346336613230633130313133663232352e706e67" alt="paxos第二阶段1"></a></p><p>当 <code>Proposer</code> 收到超过半数的 <code>accept</code> ，那么它这个时候会向所有的 <code>acceptor</code> 发送提案的提交请求。需要注意的是，因为上述仅仅是超过半数的 <code>acceptor</code> 批准执行了该提案内容，其他没有批准的并没有执行该提案内容，所以这个时候需要<strong>向未批准的 <code>acceptor</code> 发送提案内容和提案编号并让它无条件执行和提交</strong>，而对于前面已经批准过该提案的 <code>acceptor</code> 来说 <strong>仅仅需要发送该提案的编号</strong> ，让 <code>acceptor</code> 执行提交就行了。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/b52244dcbd9969bdf2938521312ab29333a6a9b2d6264335e0e430f2dcf3f4f0/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f37343338383962393734383566646665323039346535656630616636623134312e706e67"><img src="https://camo.githubusercontent.com/b52244dcbd9969bdf2938521312ab29333a6a9b2d6264335e0e430f2dcf3f4f0/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f37343338383962393734383566646665323039346535656630616636623134312e706e67" alt="paxos第二阶段2"></a></p><p>而如果 <code>Proposer</code> 如果没有收到超过半数的 <code>accept</code> 那么它将会将 <strong>递增</strong> 该 <code>Proposal</code> 的编号，然后 <strong>重新进入 <code>Prepare</code> 阶段</strong> 。</p><blockquote><p>对于 <code>Learner</code> 来说如何去学习 <code>Acceptor</code> 批准的提案内容，这有很多方式，读者可以自己去了解一下，这里不做过多解释。</p></blockquote><h4 id="4-3-3-paxos-算法的死循环问题"><a href="#4-3-3-paxos-算法的死循环问题" class="headerlink" title="4.3.3. paxos 算法的死循环问题"></a>4.3.3. <code>paxos</code> 算法的死循环问题</h4><p>其实就有点类似于两个人吵架，小明说我是对的，小红说我才是对的，两个人据理力争的谁也不让谁🤬🤬。</p><p>比如说，此时提案者 P1 提出一个方案 M1，完成了 <code>Prepare</code> 阶段的工作，这个时候 <code>acceptor</code> 则批准了 M1，但是此时提案者 P2 同时也提出了一个方案 M2，它也完成了 <code>Prepare</code> 阶段的工作。然后 P1 的方案已经不能在第二阶段被批准了（因为 <code>acceptor</code> 已经批准了比 M1 更大的 M2），所以 P1 自增方案变为 M3 重新进入 <code>Prepare</code> 阶段，然后 <code>acceptor</code> ，又批准了新的 M3 方案，它又不能批准 M2 了，这个时候 M2 又自增进入 <code>Prepare</code> 阶段。。。</p><p>就这样无休无止的永远提案下去，这就是 <code>paxos</code> 算法的死循环问题。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/05d4b58c74150218cc596a50dd74075dcfebff91d900147ac4dc33de24a44627/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f37326363663635636463313037333436666632613161383831643239366132622e706e67"><img src="https://camo.githubusercontent.com/05d4b58c74150218cc596a50dd74075dcfebff91d900147ac4dc33de24a44627/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f37326363663635636463313037333436666632613161383831643239366132622e706e67" alt="img"></a></p><p>那么如何解决呢？很简单，人多了容易吵架，我现在 <strong>就允许一个能提案</strong> 就行了。</p><h2 id="5-引出-ZAB"><a href="#5-引出-ZAB" class="headerlink" title="5. 引出 ZAB"></a>5. 引出 <code>ZAB</code></h2><h3 id="5-1-Zookeeper-架构"><a href="#5-1-Zookeeper-架构" class="headerlink" title="5.1. Zookeeper 架构"></a>5.1. <code>Zookeeper</code> 架构</h3><p>作为一个优秀高效且可靠的分布式协调框架，<code>ZooKeeper</code> 在解决分布式数据一致性问题时并没有直接使用 <code>Paxos</code> ，而是专门定制了一致性协议叫做 <code>ZAB(ZooKeeper Atomic Broadcast)</code> 原子广播协议，该协议能够很好地支持 <strong>崩溃恢复</strong> 。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/077fd611729e99b56c3b2c6af07c4d145b89d620189bfe4d4460536e9d5f8c1e/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f30633338643038656130323665323562663338343963633736353461346537392e706e67"><img src="https://camo.githubusercontent.com/077fd611729e99b56c3b2c6af07c4d145b89d620189bfe4d4460536e9d5f8c1e/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f30633338643038656130323665323562663338343963633736353461346537392e706e67" alt="Zookeeper架构"></a></p><h3 id="5-2-ZAB-中的三个角色"><a href="#5-2-ZAB-中的三个角色" class="headerlink" title="5.2. ZAB 中的三个角色"></a>5.2. <code>ZAB</code> 中的三个角色</h3><p>和介绍 <code>Paxos</code> 一样，在介绍 <code>ZAB</code> 协议之前，我们首先来了解一下在 <code>ZAB</code> 中三个主要的角色，<code>Leader 领导者</code>、<code>Follower跟随者</code>、<code>Observer观察者</code> 。</p><ul><li><code>Leader</code> ：集群中 <strong>唯一的写请求处理者</strong> ，能够发起投票（投票也是为了进行写请求）。</li><li><code>Follower</code>：能够接收客户端的请求，如果是读请求则可以自己处理，<strong>如果是写请求则要转发给 <code>Leader</code></strong> 。在选举过程中会参与投票，<strong>有选举权和被选举权</strong> 。</li><li><code>Observer</code> ：就是没有选举权和被选举权的 <code>Follower</code> 。</li></ul><p>在 <code>ZAB</code> 协议中对 <code>zkServer</code>(即上面我们说的三个角色的总称) 还有两种模式的定义，分别是 <strong>消息广播</strong> 和 <strong>崩溃恢复</strong> 。</p><h3 id="5-3-消息广播模式"><a href="#5-3-消息广播模式" class="headerlink" title="5.3. 消息广播模式"></a>5.3. 消息广播模式</h3><p>说白了就是 <code>ZAB</code> 协议是如何处理写请求的，上面我们不是说只有 <code>Leader</code> 能处理写请求嘛？那么我们的 <code>Follower</code> 和 <code>Observer</code> 是不是也需要 <strong>同步更新数据</strong> 呢？总不能数据只在 <code>Leader</code> 中更新了，其他角色都没有得到更新吧？</p><p>不就是 <strong>在整个集群中保持数据的一致性</strong> 嘛？如果是你，你会怎么做呢？</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/0bc03fe182c291f01eb9c7869c69408887d4a32d3ffe7d4319ab55a8f1140ca5/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f65363036346165613732396463633264393237643564383163343739376537342e706e67"><img src="https://camo.githubusercontent.com/0bc03fe182c291f01eb9c7869c69408887d4a32d3ffe7d4319ab55a8f1140ca5/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f65363036346165613732396463633264393237643564383163343739376537342e706e67" alt="img"></a></p><p>废话，第一步肯定需要 <code>Leader</code> 将写请求 <strong>广播</strong> 出去呀，让 <code>Leader</code> 问问 <code>Followers</code> 是否同意更新，如果超过半数以上的同意那么就进行 <code>Follower</code> 和 <code>Observer</code> 的更新（和 <code>Paxos</code> 一样）。当然这么说有点虚，画张图理解一下。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/597f31bca7f3b432b292df30c09b8fef9f703a5b0439c5c2cf23c173af262da0/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f30386363636534383139306665346564636263626232323364363233313837362e706e67"><img src="https://camo.githubusercontent.com/597f31bca7f3b432b292df30c09b8fef9f703a5b0439c5c2cf23c173af262da0/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f30386363636534383139306665346564636263626232323364363233313837362e706e67" alt="消息广播"></a></p><p>嗯。。。看起来很简单，貌似懂了🤥🤥🤥。这两个 <code>Queue</code> 哪冒出来的？答案是 <strong><code>ZAB</code> 需要让 <code>Follower</code> 和 <code>Observer</code> 保证顺序性</strong> 。何为顺序性，比如我现在有一个写请求A，此时 <code>Leader</code> 将请求A广播出去，因为只需要半数同意就行，所以可能这个时候有一个 <code>Follower</code> F1因为网络原因没有收到，而 <code>Leader</code> 又广播了一个请求B，因为网络原因，F1竟然先收到了请求B然后才收到了请求A，这个时候请求处理的顺序不同就会导致数据的不同，从而 <strong>产生数据不一致问题</strong> 。</p><p>所以在 <code>Leader</code> 这端，它为每个其他的 <code>zkServer</code> 准备了一个 <strong>队列</strong> ，采用先进先出的方式发送消息。由于协议是 **通过 <code>TCP</code> **来进行网络通信的，保证了消息的发送顺序性，接受顺序性也得到了保证。</p><p>除此之外，在 <code>ZAB</code> 中还定义了一个 <strong>全局单调递增的事务ID <code>ZXID</code></strong> ，它是一个64位long型，其中高32位表示 <code>epoch</code> 年代，低32位表示事务id。<code>epoch</code> 是会根据 <code>Leader</code> 的变化而变化的，当一个 <code>Leader</code> 挂了，新的 <code>Leader</code> 上位的时候，年代（<code>epoch</code>）就变了。而低32位可以简单理解为递增的事务id。</p><p>定义这个的原因也是为了顺序性，每个 <code>proposal</code> 在 <code>Leader</code> 中生成后需要 <strong>通过其 <code>ZXID</code> 来进行排序</strong> ，才能得到处理。</p><h3 id="5-4-崩溃恢复模式"><a href="#5-4-崩溃恢复模式" class="headerlink" title="5.4. 崩溃恢复模式"></a>5.4. 崩溃恢复模式</h3><p>说到崩溃恢复我们首先要提到 <code>ZAB</code> 中的 <code>Leader</code> 选举算法，当系统出现崩溃影响最大应该是 <code>Leader</code> 的崩溃，因为我们只有一个 <code>Leader</code> ，所以当 <code>Leader</code> 出现问题的时候我们势必需要重新选举 <code>Leader</code> 。</p><p><code>Leader</code> 选举可以分为两个不同的阶段，第一个是我们提到的 <code>Leader</code> 宕机需要重新选举，第二则是当 <code>Zookeeper</code> 启动时需要进行系统的 <code>Leader</code> 初始化选举。下面我先来介绍一下 <code>ZAB</code> 是如何进行初始化选举的。</p><p>假设我们集群中有3台机器，那也就意味着我们需要两台以上同意（超过半数）。比如这个时候我们启动了 <code>server1</code> ，它会首先 <strong>投票给自己</strong> ，投票内容为服务器的 <code>myid</code> 和 <code>ZXID</code> ，因为初始化所以 <code>ZXID</code> 都为0，此时 <code>server1</code> 发出的投票为 (1,0)。但此时 <code>server1</code> 的投票仅为1，所以不能作为 <code>Leader</code> ，此时还在选举阶段所以整个集群处于 <strong><code>Looking</code> 状态</strong>。</p><p>接着 <code>server2</code> 启动了，它首先也会将投票选给自己(2,0)，并将投票信息广播出去（<code>server1</code>也会，只是它那时没有其他的服务器了），<code>server1</code> 在收到 <code>server2</code> 的投票信息后会将投票信息与自己的作比较。**首先它会比较 <code>ZXID</code> ，<code>ZXID</code> 大的优先为 <code>Leader</code>，如果相同则比较 <code>myid</code>，<code>myid</code> 大的优先作为 <code>Leader</code>**。所以此时<code>server1</code> 发现 <code>server2</code> 更适合做 <code>Leader</code>，它就会将自己的投票信息更改为(2,0)然后再广播出去，之后<code>server2</code> 收到之后发现和自己的一样无需做更改，并且自己的 <strong>投票已经超过半数</strong> ，则 **确定 <code>server2</code> 为 <code>Leader</code>**，<code>server1</code> 也会将自己服务器设置为 <code>Following</code> 变为 <code>Follower</code>。整个服务器就从 <code>Looking</code> 变为了正常状态。</p><p>当 <code>server3</code> 启动发现集群没有处于 <code>Looking</code> 状态时，它会直接以 <code>Follower</code> 的身份加入集群。</p><p>还是前面三个 <code>server</code> 的例子，如果在整个集群运行的过程中 <code>server2</code> 挂了，那么整个集群会如何重新选举 <code>Leader</code> 呢？其实和初始化选举差不多。</p><p>首先毫无疑问的是剩下的两个 <code>Follower</code> 会将自己的状态 <strong>从 <code>Following</code> 变为 <code>Looking</code> 状态</strong> ，然后每个 <code>server</code> 会向初始化投票一样首先给自己投票（这不过这里的 <code>zxid</code> 可能不是0了，这里为了方便随便取个数字）。</p><p>假设 <code>server1</code> 给自己投票为(1,99)，然后广播给其他 <code>server</code>，<code>server3</code> 首先也会给自己投票(3,95)，然后也广播给其他 <code>server</code>。<code>server1</code> 和 <code>server3</code> 此时会收到彼此的投票信息，和一开始选举一样，他们也会比较自己的投票和收到的投票（<code>zxid</code> 大的优先，如果相同那么就 <code>myid</code> 大的优先）。这个时候 <code>server1</code> 收到了 <code>server3</code> 的投票发现没自己的合适故不变，<code>server3</code> 收到 <code>server1</code> 的投票结果后发现比自己的合适于是更改投票为(1,99)然后广播出去，最后 <code>server1</code> 收到了发现自己的投票已经超过半数就把自己设为 <code>Leader</code>，<code>server3</code> 也随之变为 <code>Follower</code>。</p><blockquote><p>请注意 <code>ZooKeeper</code> 为什么要设置奇数个结点？比如这里我们是三个，挂了一个我们还能正常工作，挂了两个我们就不能正常工作了（已经没有超过半数的节点数了，所以无法进行投票等操作了）。而假设我们现在有四个，挂了一个也能工作，<strong>但是挂了两个也不能正常工作了</strong>，这是和三个一样的，而三个比四个还少一个，带来的效益是一样的，所以 <code>Zookeeper</code> 推荐奇数个 <code>server</code> 。</p></blockquote><p>那么说完了 <code>ZAB</code> 中的 <code>Leader</code> 选举方式之后我们再来了解一下 <strong>崩溃恢复</strong> 是什么玩意？</p><p>其实主要就是 <strong>当集群中有机器挂了，我们整个集群如何保证数据一致性？</strong></p><p>如果只是 <code>Follower</code> 挂了，而且挂的没超过半数的时候，因为我们一开始讲了在 <code>Leader</code> 中会维护队列，所以不用担心后面的数据没接收到导致数据不一致性。</p><p>如果 <code>Leader</code> 挂了那就麻烦了，我们肯定需要先暂停服务变为 <code>Looking</code> 状态然后进行 <code>Leader</code> 的重新选举（上面我讲过了），但这个就要分为两种情况了，分别是 <strong>确保已经被Leader提交的提案最终能够被所有的Follower提交</strong> 和 <strong>跳过那些已经被丢弃的提案</strong> 。</p><p>确保已经被Leader提交的提案最终能够被所有的Follower提交是什么意思呢？</p><p>假设 <code>Leader (server2)</code> 发送 <code>commit</code> 请求（忘了请看上面的消息广播模式），他发送给了 <code>server3</code>，然后要发给 <code>server1</code> 的时候突然挂了。这个时候重新选举的时候我们如果把 <code>server1</code> 作为 <code>Leader</code> 的话，那么肯定会产生数据不一致性，因为 <code>server3</code> 肯定会提交刚刚 <code>server2</code> 发送的 <code>commit</code> 请求的提案，而 <code>server1</code> 根本没收到所以会丢弃。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/77dce0fe55188c51fd83aa9e9475c662a5a4d8156c8c1899b4d848daeaeb32a1/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f66666362313263366662326261643736616337313035363936363535653835632e706e67"><img src="https://camo.githubusercontent.com/77dce0fe55188c51fd83aa9e9475c662a5a4d8156c8c1899b4d848daeaeb32a1/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f66666362313263366662326261643736616337313035363936363535653835632e706e67" alt="崩溃恢复"></a></p><p>那怎么解决呢？</p><p>聪明的同学肯定会质疑，<strong>这个时候 <code>server1</code> 已经不可能成为 <code>Leader</code> 了，因为 <code>server1</code> 和 <code>server3</code> 进行投票选举的时候会比较 <code>ZXID</code> ，而此时 <code>server3</code> 的 <code>ZXID</code> 肯定比 <code>server1</code> 的大了</strong>。(不理解可以看前面的选举算法)</p><p>那么跳过那些已经被丢弃的提案又是什么意思呢？</p><p>假设 <code>Leader (server2)</code> 此时同意了提案N1，自身提交了这个事务并且要发送给所有 <code>Follower</code> 要 <code>commit</code> 的请求，却在这个时候挂了，此时肯定要重新进行 <code>Leader</code> 的选举，比如说此时选 <code>server1</code> 为 <code>Leader</code> （这无所谓）。但是过了一会，这个 <strong>挂掉的 <code>Leader</code> 又重新恢复了</strong> ，此时它肯定会作为 <code>Follower</code> 的身份进入集群中，需要注意的是刚刚 <code>server2</code> 已经同意提交了提案N1，但其他 <code>server</code> 并没有收到它的 <code>commit</code> 信息，所以其他 <code>server</code> 不可能再提交这个提案N1了，这样就会出现数据不一致性问题了，所以 <strong>该提案N1最终需要被抛弃掉</strong> 。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/bc42dd3ff23dea6e243e5bda08d91368ef5809cfe3fca57d4f005c7eb8e3db2e/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f61626236656663376434646639633832623136326362656362313239613665332e706e67"><img src="https://camo.githubusercontent.com/bc42dd3ff23dea6e243e5bda08d91368ef5809cfe3fca57d4f005c7eb8e3db2e/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f61626236656663376434646639633832623136326362656362313239613665332e706e67" alt="崩溃恢复"></a></p><h2 id="6-Zookeeper的几个理论知识"><a href="#6-Zookeeper的几个理论知识" class="headerlink" title="6. Zookeeper的几个理论知识"></a>6. Zookeeper的几个理论知识</h2><p>了解了 <code>ZAB</code> 协议还不够，它仅仅是 <code>Zookeeper</code> 内部实现的一种方式，而我们如何通过 <code>Zookeeper</code> 去做一些典型的应用场景呢？比如说集群管理，分布式锁，<code>Master</code> 选举等等。</p><p>这就涉及到如何使用 <code>Zookeeper</code> 了，但在使用之前我们还需要掌握几个概念。比如 <code>Zookeeper</code> 的 <strong>数据模型</strong> 、<strong>会话机制</strong>、<strong>ACL</strong>、<strong>Watcher机制</strong> 等等。</p><h3 id="6-1-数据模型"><a href="#6-1-数据模型" class="headerlink" title="6.1. 数据模型"></a>6.1. 数据模型</h3><p><code>zookeeper</code> 数据存储结构与标准的 <code>Unix</code> 文件系统非常相似，都是在根节点下挂很多子节点(树型)。但是 <code>zookeeper</code> 中没有文件系统中目录与文件的概念，而是 <strong>使用了 <code>znode</code> 作为数据节点</strong> 。<code>znode</code> 是 <code>zookeeper</code> 中的最小数据单元，每个 <code>znode</code> 上都可以保存数据，同时还可以挂载子节点，形成一个树形化命名空间。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/1e6f78a5b23fde663c00b29f76710c83620711ee095e6b16eaf4158dcb839bdb/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f38663335646261386334346334613130643831653333393564663937316365372e706e67"><img src="https://camo.githubusercontent.com/1e6f78a5b23fde663c00b29f76710c83620711ee095e6b16eaf4158dcb839bdb/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f38663335646261386334346334613130643831653333393564663937316365372e706e67" alt="zk数据模型"></a></p><p>每个 <code>znode</code> 都有自己所属的 <strong>节点类型</strong> 和 <strong>节点状态</strong>。</p><p>其中节点类型可以分为 <strong>持久节点</strong>、<strong>持久顺序节点</strong>、<strong>临时节点</strong> 和 <strong>临时顺序节点</strong>。</p><ul><li>持久节点：一旦创建就一直存在，直到将其删除。</li><li>持久顺序节点：一个父节点可以为其子节点 <strong>维护一个创建的先后顺序</strong> ，这个顺序体现在 <strong>节点名称</strong> 上，是节点名称后自动添加一个由 10 位数字组成的数字串，从 0 开始计数。</li><li>临时节点：临时节点的生命周期是与 <strong>客户端会话</strong> 绑定的，<strong>会话消失则节点消失</strong> 。临时节点 <strong>只能做叶子节点</strong> ，不能创建子节点。</li><li>临时顺序节点：父节点可以创建一个维持了顺序的临时节点(和前面的持久顺序性节点一样)。</li></ul><p>节点状态中包含了很多节点的属性比如 <code>czxid</code> 、<code>mzxid</code> 等等，在 <code>zookeeper</code> 中是使用 <code>Stat</code> 这个类来维护的。下面我列举一些属性解释。</p><ul><li><code>czxid</code>：<code>Created ZXID</code>，该数据节点被 <strong>创建</strong> 时的事务ID。</li><li><code>mzxid</code>：<code>Modified ZXID</code>，节点 <strong>最后一次被更新时</strong> 的事务ID。</li><li><code>ctime</code>：<code>Created Time</code>，该节点被创建的时间。</li><li><code>mtime</code>： <code>Modified Time</code>，该节点最后一次被修改的时间。</li><li><code>version</code>：节点的版本号。</li><li><code>cversion</code>：<strong>子节点</strong> 的版本号。</li><li><code>aversion</code>：节点的 <code>ACL</code> 版本号。</li><li><code>ephemeralOwner</code>：创建该节点的会话的 <code>sessionID</code> ，如果该节点为持久节点，该值为0。</li><li><code>dataLength</code>：节点数据内容的长度。</li><li><code>numChildre</code>：该节点的子节点个数，如果为临时节点为0。</li><li><code>pzxid</code>：该节点子节点列表最后一次被修改时的事务ID，注意是子节点的 <strong>列表</strong> ，不是内容。</li></ul><h3 id="6-2-会话"><a href="#6-2-会话" class="headerlink" title="6.2. 会话"></a>6.2. 会话</h3><p>我想这个对于后端开发的朋友肯定不陌生，不就是 <code>session</code> 吗？只不过 <code>zk</code> 客户端和服务端是通过 <strong><code>TCP</code> 长连接</strong> 维持的会话机制，其实对于会话来说你可以理解为 <strong>保持连接状态</strong> 。</p><p>在 <code>zookeeper</code> 中，会话还有对应的事件，比如 <code>CONNECTION_LOSS 连接丢失事件</code> 、<code>SESSION_MOVED 会话转移事件</code> 、<code>SESSION_EXPIRED 会话超时失效事件</code> 。</p><h3 id="6-3-ACL"><a href="#6-3-ACL" class="headerlink" title="6.3. ACL"></a>6.3. ACL</h3><p><code>ACL</code> 为 <code>Access Control Lists</code> ，它是一种权限控制。在 <code>zookeeper</code> 中定义了5种权限，它们分别为：</p><ul><li><code>CREATE</code> ：创建子节点的权限。</li><li><code>READ</code>：获取节点数据和子节点列表的权限。</li><li><code>WRITE</code>：更新节点数据的权限。</li><li><code>DELETE</code>：删除子节点的权限。</li><li><code>ADMIN</code>：设置节点 ACL 的权限。</li></ul><h3 id="6-4-Watcher机制"><a href="#6-4-Watcher机制" class="headerlink" title="6.4. Watcher机制"></a>6.4. Watcher机制</h3><p><code>Watcher</code> 为事件监听器，是 <code>zk</code> 非常重要的一个特性，很多功能都依赖于它，它有点类似于订阅的方式，即客户端向服务端 <strong>注册</strong> 指定的 <code>watcher</code> ，当服务端符合了 <code>watcher</code> 的某些事件或要求则会 <strong>向客户端发送事件通知</strong> ，客户端收到通知后找到自己定义的 <code>Watcher</code> 然后 <strong>执行相应的回调方法</strong> 。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/6a11f5f5204500938b548bbabeffbe59c450c785aa5e66c10c84e22e66be88df/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f32343161623863633337353731303334666139383433323262373533633762612e706e67"><img src="https://camo.githubusercontent.com/6a11f5f5204500938b548bbabeffbe59c450c785aa5e66c10c84e22e66be88df/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f32343161623863633337353731303334666139383433323262373533633762612e706e67" alt="watcher机制"></a></p><h2 id="7-Zookeeper的几个典型应用场景"><a href="#7-Zookeeper的几个典型应用场景" class="headerlink" title="7. Zookeeper的几个典型应用场景"></a>7. Zookeeper的几个典型应用场景</h2><p>前面说了这么多的理论知识，你可能听得一头雾水，这些玩意有啥用？能干啥事？别急，听我慢慢道来。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/a838fc021a499e66e0fc6ea594a4978bf6a73fc64ef6eeba86c73dff4a5fc870/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f39633962643261383932653233653062373538323337306335303131376438632e706e67"><img src="https://camo.githubusercontent.com/a838fc021a499e66e0fc6ea594a4978bf6a73fc64ef6eeba86c73dff4a5fc870/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f39633962643261383932653233653062373538323337306335303131376438632e706e67" alt="img"></a></p><h3 id="7-1-选主"><a href="#7-1-选主" class="headerlink" title="7.1. 选主"></a>7.1. 选主</h3><p>还记得上面我们的所说的临时节点吗？因为 <code>Zookeeper</code> 的强一致性，能够很好地在保证 <strong>在高并发的情况下保证节点创建的全局唯一性</strong> (即无法重复创建同样的节点)。</p><p>利用这个特性，我们可以 <strong>让多个客户端创建一个指定的节点</strong> ，创建成功的就是 <code>master</code>。</p><p>但是，如果这个 <code>master</code> 挂了怎么办？？？</p><p>你想想为什么我们要创建临时节点？还记得临时节点的生命周期吗？<code>master</code> 挂了是不是代表会话断了？会话断了是不是意味着这个节点没了？还记得 <code>watcher</code> 吗？我们是不是可以 <strong>让其他不是 <code>master</code> 的节点监听节点的状态</strong> ，比如说我们监听这个临时节点的父节点，如果子节点个数变了就代表 <code>master</code> 挂了，这个时候我们 <strong>触发回调函数进行重新选举</strong> ，或者我们直接监听节点的状态，我们可以通过节点是否已经失去连接来判断 <code>master</code> 是否挂了等等。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/d699db5d4e7733aadf1908e223eb0090a5b96d868a6c9c74d789d466ef801ac4/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f61393437303730323863353538316338313566373266626130663530663433612e706e67"><img src="https://camo.githubusercontent.com/d699db5d4e7733aadf1908e223eb0090a5b96d868a6c9c74d789d466ef801ac4/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f61393437303730323863353538316338313566373266626130663530663433612e706e67" alt="选主"></a></p><p>总的来说，我们可以完全 <strong>利用 临时节点、节点状态 和 <code>watcher</code> 来实现选主的功能</strong>，临时节点主要用来选举，节点状态和<code>watcher</code> 可以用来判断 <code>master</code> 的活性和进行重新选举。</p><h3 id="7-2-分布式锁"><a href="#7-2-分布式锁" class="headerlink" title="7.2. 分布式锁"></a>7.2. 分布式锁</h3><p>分布式锁的实现方式有很多种，比如 <code>Redis</code> 、数据库 、<code>zookeeper</code> 等。个人认为 <code>zookeeper</code> 在实现分布式锁这方面是非常非常简单的。</p><p>上面我们已经提到过了 <strong>zk在高并发的情况下保证节点创建的全局唯一性</strong>，这玩意一看就知道能干啥了。实现互斥锁呗，又因为能在分布式的情况下，所以能实现分布式锁呗。</p><p>如何实现呢？这玩意其实跟选主基本一样，我们也可以利用临时节点的创建来实现。</p><p>首先肯定是如何获取锁，因为创建节点的唯一性，我们可以让多个客户端同时创建一个临时节点，<strong>创建成功的就说明获取到了锁</strong> 。然后没有获取到锁的客户端也像上面选主的非主节点创建一个 <code>watcher</code> 进行节点状态的监听，如果这个互斥锁被释放了（可能获取锁的客户端宕机了，或者那个客户端主动释放了锁）可以调用回调函数重新获得锁。</p><blockquote><p><code>zk</code> 中不需要向 <code>redis</code> 那样考虑锁得不到释放的问题了，因为当客户端挂了，节点也挂了，锁也释放了。是不是很简单？</p></blockquote><p>那能不能使用 <code>zookeeper</code> 同时实现 <strong>共享锁和独占锁</strong> 呢？答案是可以的，不过稍微有点复杂而已。</p><p>还记得 <strong>有序的节点</strong> 吗？</p><p>这个时候我规定所有创建节点必须有序，当你是读请求（要获取共享锁）的话，如果 <strong>没有比自己更小的节点，或比自己小的节点都是读请求</strong> ，则可以获取到读锁，然后就可以开始读了。<strong>若比自己小的节点中有写请求</strong> ，则当前客户端无法获取到读锁，只能等待前面的写请求完成。</p><p>如果你是写请求（获取独占锁），若 <strong>没有比自己更小的节点</strong> ，则表示当前客户端可以直接获取到写锁，对数据进行修改。若发现 <strong>有比自己更小的节点，无论是读操作还是写操作，当前客户端都无法获取到写锁</strong> ，等待所有前面的操作完成。</p><p>这就很好地同时实现了共享锁和独占锁，当然还有优化的地方，比如当一个锁得到释放它会通知所有等待的客户端从而造成 <strong>羊群效应</strong> 。此时你可以通过让等待的节点只监听他们前面的节点。</p><p>具体怎么做呢？其实也很简单，你可以让 <strong>读请求监听比自己小的最后一个写请求节点，写请求只监听比自己小的最后一个节点</strong> ，感兴趣的小伙伴可以自己去研究一下。</p><h3 id="7-3-命名服务"><a href="#7-3-命名服务" class="headerlink" title="7.3. 命名服务"></a>7.3. 命名服务</h3><p>如何给一个对象设置ID，大家可能都会想到 <code>UUID</code>，但是 <code>UUID</code> 最大的问题就在于它太长了。。。(太长不一定是好事，嘿嘿嘿)。那么在条件允许的情况下，我们能不能使用 <code>zookeeper</code> 来实现呢？</p><p>我们之前提到过 <code>zookeeper</code> 是通过 <strong>树形结构</strong> 来存储数据节点的，那也就是说，对于每个节点的 <strong>全路径</strong>，它必定是唯一的，我们可以使用节点的全路径作为命名方式了。而且更重要的是，路径是我们可以自己定义的，这对于我们对有些有语意的对象的ID设置可以更加便于理解。</p><h3 id="7-4-集群管理和注册中心"><a href="#7-4-集群管理和注册中心" class="headerlink" title="7.4. 集群管理和注册中心"></a>7.4. 集群管理和注册中心</h3><p>看到这里是不是觉得 <code>zookeeper</code> 实在是太强大了，它怎么能这么能干！</p><p>别急，它能干的事情还很多呢。可能我们会有这样的需求，我们需要了解整个集群中有多少机器在工作，我们想对集群中的每台机器的运行时状态进行数据采集，对集群中机器进行上下线操作等等。</p><p>而 <code>zookeeper</code> 天然支持的 <code>watcher</code> 和 临时节点能很好的实现这些需求。我们可以为每条机器创建临时节点，并监控其父节点，如果子节点列表有变动（我们可能创建删除了临时节点），那么我们可以使用在其父节点绑定的 <code>watcher</code> 进行状态监控和回调。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/74084b1a9ca9db8ae26f336e8338e014e2b7c7a3f5ae38bd6b6b6e23592728c6/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f36313135383230323139633335633638626362326339613835356562616365332e706e67"><img src="https://camo.githubusercontent.com/74084b1a9ca9db8ae26f336e8338e014e2b7c7a3f5ae38bd6b6b6e23592728c6/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f36313135383230323139633335633638626362326339613835356562616365332e706e67" alt="集群管理"></a></p><p>至于注册中心也很简单，我们同样也是让 <strong>服务提供者</strong> 在 <code>zookeeper</code> 中创建一个临时节点并且将自己的 <code>ip、port、调用方式</code> 写入节点，当 <strong>服务消费者</strong> 需要进行调用的时候会 <strong>通过注册中心找到相应的服务的地址列表(IP端口什么的)</strong> ，并缓存到本地(方便以后调用)，当消费者调用服务时，不会再去请求注册中心，而是直接通过负载均衡算法从地址列表中取一个服务提供者的服务器调用服务。</p><p>当服务提供者的某台服务器宕机或下线时，相应的地址会从服务提供者地址列表中移除。同时，注册中心会将新的服务地址列表发送给服务消费者的机器并缓存在消费者本机（当然你可以让消费者进行节点监听，我记得 <code>Eureka</code> 会先试错，然后再更新）。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/595441ee538e1ac20ea08cd64fe0e83f91a8b92011b4a8b5a7f02299956ff914/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f30623562333931316137633264616532333339316431376339313431366232392e706e67"><img src="https://camo.githubusercontent.com/595441ee538e1ac20ea08cd64fe0e83f91a8b92011b4a8b5a7f02299956ff914/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f30623562333931316137633264616532333339316431376339313431366232392e706e67" alt="注册中心"></a></p><h2 id="8-总结"><a href="#8-总结" class="headerlink" title="8. 总结"></a>8. 总结</h2><p>看到这里的同学实在是太有耐心了👍👍👍，如果觉得我写得不错的话点个赞哈。</p><p>不知道大家是否还记得我讲了什么😒。</p><p><a target="_blank" rel="noopener" href="https://camo.githubusercontent.com/7f6c8deb2178bf3b85184b9ec0a9b714a02e00a28f5c5b125e48a9289f2ecc0d/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f62306264653966333937396238663139313837653663333734616439383939332e706e67"><img src="https://camo.githubusercontent.com/7f6c8deb2178bf3b85184b9ec0a9b714a02e00a28f5c5b125e48a9289f2ecc0d/68747470733a2f2f696d672d626c6f672e6373646e696d672e636e2f696d675f636f6e766572742f62306264653966333937396238663139313837653663333734616439383939332e706e67" alt="img"></a></p><p>这篇文章中我带大家入门了 <code>zookeeper</code> 这个强大的分布式协调框架。现在我们来简单梳理一下整篇文章的内容。</p><ul><li><p>分布式与集群的区别</p></li><li><p><code>2PC</code> 、<code>3PC</code> 以及 <code>paxos</code> 算法这些一致性框架的原理和实现。</p></li><li><p><code>zookeeper</code> 专门的一致性算法 <code>ZAB</code> 原子广播协议的内容（<code>Leader</code> 选举、崩溃恢复、消息广播）。</p></li><li><p><code>zookeeper</code> 中的一些基本概念，比如 <code>ACL</code>，数据节点，会话，<code>watcher</code>机制等等。</p></li><li><p><code>zookeeper</code> 的典型应用场景，比如选主，注册中心等等。</p><p>如果忘了可以回去看看再次理解一下，如果有疑问和建议欢迎提出🤝🤝🤝。</p></li></ul></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://enpsl.github.io/2021/02/16/2021-02-16-MQ%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="enpsl"><meta itemprop="description" content="my blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="彭诗亮的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"> <a href="/2021/02/16/2021-02-16-MQ%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">MQ面试问题总结</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-02-16 19:30:00" itemprop="dateCreated datePublished" datetime="2021-02-16T19:30:00+08:00">2021-02-16</time></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="为什么使用MQ？"><a href="#为什么使用MQ？" class="headerlink" title="为什么使用MQ？"></a>为什么使用MQ？</h2><p>使用MQ的场景很多，主要有三个：解耦、异步、削峰。</p><ul><li>解耦：假设现在，日志不光要插入到数据库里，还要在硬盘中增加文件类型的日志，同时，一些关键日志还要通过邮件的方式发送给指定的人。那么，如果按照原来的逻辑，A可能就需要在原来的代码上做扩展，除了B服务，还要加上日志文件的存储和日志邮件的发送。但是，如果你使用了MQ，那么，A服务是不需要做更改的，它还是将消息放到MQ中即可，其它的服务，无论是原来的B服务还是新增的日志文件存储服务或日志邮件发送服务，都直接从MQ中获取消息并处理即可。这就是解耦，它的好处是提高系统灵活性，扩展性。</li><li>异步：可以将一些非核心流程，如日志，短信，邮件等，通过MQ的方式异步去处理。这样做的好处是缩短主流程的响应时间，提升用户体验。</li><li>削峰：MQ的本质就是业务的排队。所以，面对突然到来的高并发，MQ也可以不用慌忙，先排好队，不要着急，一个一个来。削峰的好处就是避免高并发压垮系统的关键组件，如某个核心服务或数据库等。</li></ul><p>下面附场景解释：</p><h3 id="解耦"><a href="#解耦" class="headerlink" title="解耦"></a>解耦</h3><p>场景：A 系统发送数据到 BCD 三个系统，通过接口调用发送。如果 E 系统也要这个数据呢？那如果 C 系统现在不需要了呢？A 系统负责人几乎崩溃……</p><p><img src="http://blog-img.coolsen.cn/img/727602-20200108091205317-949408193.png" alt="img"></p><p>在这个场景中，A 系统跟其它各种乱七八糟的系统严重耦合，A 系统产生一条比较关键的数据，很多系统都需要 A 系统将这个数据发送过来。A 系统要时时刻刻考虑 BCDE 四个系统如果挂了该咋办？要不要重发，要不要把消息存起来？头发都白了啊！</p><p>如果使用 MQ，A 系统产生一条数据，发送到 MQ 里面去，哪个系统需要数据自己去 MQ 里面消费。如果新系统需要数据，直接从 MQ 里消费即可；如果某个系统不需要这条数据了，就取消对 MQ 消息的消费即可。这样下来，A 系统压根儿不需要去考虑要给谁发送数据，不需要维护这个代码，也不需要考虑人家是否调用成功、失败超时等情况。</p><p><img src="http://blog-img.coolsen.cn/img/727602-20200108091329888-1880681145.png" alt="img"></p><p>总结：通过一个 MQ，Pub&#x2F;Sub 发布订阅消息这么一个模型，A 系统就跟其它系统彻底解耦了。</p><h3 id="异步"><a href="#异步" class="headerlink" title="异步"></a>异步</h3><p>场景：A 系统接收一个请求，需要在自己本地写库，还需要在 BCD 三个系统写库，自己本地写库要 3ms，BCD 三个系统分别写库要 300ms、450ms、200ms。最终请求总延时是 3 + 300 + 450 + 200 &#x3D; 953ms，接近 1s，用户感觉搞个什么东西，慢死了慢死了。用户通过浏览器发起请求，等待个 1s，这几乎是不可接受的。</p><p><img src="http://blog-img.coolsen.cn/img/727602-20200108091632167-740723329.png" alt="img"></p><p>一般互联网类的企业，对于用户直接的操作，一般要求是每个请求都必须在 200 ms 以内完成，对用户几乎是无感知的。</p><p>如果使用 MQ，那么 A 系统连续发送 3 条消息到 MQ 队列中，假如耗时 5ms，A 系统从接受一个请求到返回响应给用户，总时长是 3 + 5 &#x3D; 8ms，对于用户而言，其实感觉上就是点个按钮，8ms 以后就直接返回了。</p><p><img src="http://blog-img.coolsen.cn/img/727602-20200108091722601-747710174.png" alt="img"></p><h3 id="削峰"><a href="#削峰" class="headerlink" title="削峰"></a>削峰</h3><p>场景：每天 0:00 到 12:00，A 系统风平浪静，每秒并发请求数量就 50 个。结果每次一到 12:00 ~ 13:00 ，每秒并发请求数量突然会暴增到 5k+ 条。但是系统是直接基于 MySQL 的，大量的请求涌入 MySQL，每秒钟对 MySQL 执行约 5k 条 SQL。</p><p>使用 MQ，每秒 5k 个请求写入 MQ，A 系统每秒钟最多处理 2k 个请求，因为 MySQL 每秒钟最多处理 2k 个。A 系统从 MQ 中慢慢拉取请求，每秒钟就拉取 2k 个请求，不要超过自己每秒能处理的最大请求数量就 ok，这样下来，哪怕是高峰期的时候，A 系统也绝对不会挂掉。而 MQ 每秒钟 5k 个请求进来，就 2k 个请求出去，结果就导致在中午高峰期（1 个小时），可能有几十万甚至几百万的请求积压在 MQ 中。</p><p><img src="http://blog-img.coolsen.cn/img/727602-20200108091915241-1598228624.png" alt="img"></p><p>这个短暂的高峰期积压是 ok 的，因为高峰期过了之后，每秒钟就 50 个请求进 MQ，但是 A 系统依然会按照每秒 2k 个请求的速度在处理。所以说，只要高峰期一过，A 系统就会快速将积压的消息给解决掉。</p><h2 id="消息队列的缺点"><a href="#消息队列的缺点" class="headerlink" title="消息队列的缺点"></a>消息队列的缺点</h2><p>1、 系统可用性降低</p><p>系统引入的外部依赖越多，越容易挂掉。</p><p>2、 系统复杂度提高</p><p>加入了消息队列，要多考虑很多方面的问题，比如：一致性问题、如何保证消息不被重复消费、如何保证消息可靠性传输等。因此，需要考虑的东西更多，复杂性增大。</p><p>3、 一致性问题</p><p>A 系统处理完了直接返回成功了，人都以为你这个请求就成功了；但是问题是，要是 BCD 三个系统那里，BD 两个系统写库成功了，结果 C 系统写库失败了，这就数据不一致了。</p><h2 id="Kafka、ActiveMQ、RabbitMQ、RocketMQ-有什么优缺点？"><a href="#Kafka、ActiveMQ、RabbitMQ、RocketMQ-有什么优缺点？" class="headerlink" title="Kafka、ActiveMQ、RabbitMQ、RocketMQ 有什么优缺点？"></a>Kafka、ActiveMQ、RabbitMQ、RocketMQ 有什么优缺点？</h2><table><thead><tr><th>特性</th><th>ActiveMQ</th><th>RabbitMQ</th><th>RocketMQ</th><th>Kafka</th></tr></thead><tbody><tr><td>开发语言</td><td>java</td><td>erlang</td><td>java</td><td>scala</td></tr><tr><td>单机吞吐量</td><td>万级，比 RocketMQ、Kafka 低一个数量级</td><td>同 ActiveMQ</td><td>10 万级，支撑高吞吐</td><td>10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景</td></tr><tr><td>topic 数量对吞吐量的影响</td><td></td><td></td><td>topic 可以达到几百&#x2F;几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic</td><td>topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源</td></tr><tr><td>时效性</td><td>ms 级</td><td>微秒级，这是 RabbitMQ 的一大特点，延迟最低</td><td>ms 级</td><td>延迟在 ms 级以内</td></tr><tr><td>可用性</td><td>高，基于主从架构实现高可用</td><td>同 ActiveMQ</td><td>非常高，分布式架构</td><td>非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用</td></tr><tr><td>消息可靠性</td><td>有较低的概率丢失数据</td><td>基本不丢</td><td>经过参数优化配置，可以做到 0 丢失</td><td>同 RocketMQ</td></tr><tr><td>功能支持</td><td>MQ 领域的功能极其完备</td><td>基于 erlang 开发，并发能力很强，性能极好，延时很低</td><td>MQ 功能较为完善，还是分布式的，扩展性好</td><td>功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用</td></tr><tr><td>社区活跃度</td><td>低</td><td>很高</td><td>一般</td><td>很高</td></tr></tbody></table><ul><li>中小型公司，技术实力较为一般，技术挑战不是特别高，用 RabbitMQ 是不错的选择；</li><li>大型公司，基础架构研发实力较强，用 RocketMQ 是很好的选择。</li><li>大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，几乎是全世界这个领域的事实性规范。</li></ul><h1 id="RabbitMQ"><a href="#RabbitMQ" class="headerlink" title="RabbitMQ"></a>RabbitMQ</h1><h2 id="1-RabbitMQ是什么？"><a href="#1-RabbitMQ是什么？" class="headerlink" title="1. RabbitMQ是什么？"></a>1. RabbitMQ是什么？</h2><p>RabbitMQ是实现了高级消息队列协议（AMQP）的开源消息代理软件（亦称面向消息的中间件）。RabbitMQ服务器是用Erlang语言编写的，而群集和故障转移是构建在开放电信平台框架上的。所有主要的编程语言均有与代理接口通讯的客户端库。</p><h2 id="2-RabbitMQ特点"><a href="#2-RabbitMQ特点" class="headerlink" title="2. RabbitMQ特点?"></a>2. RabbitMQ特点?</h2><p>可靠性: RabbitMQ使用一些机制来保证可靠性， 如持久化、传输确认及发布确认等。</p><p>灵活的路由 : 在消息进入队列之前，通过交换器来路由消息。对于典型的路由功能， RabbitMQ 己经提供了一些内置的交换器来实现。针对更复杂的路由功能，可以将多个 交换器绑定在一起， 也可以通过插件机制来实现自己的交换器。</p><p>扩展性: 多个RabbitMQ节点可以组成一个集群，也可以根据实际业务情况动态地扩展 集群中节点。</p><p>高可用性 : 队列可以在集群中的机器上设置镜像，使得在部分节点出现问题的情况下队 列仍然可用。</p><p>多种协议: RabbitMQ除了原生支持AMQP协议，还支持STOMP， MQTT等多种消息 中间件协议。</p><p>多语言客户端 :RabbitMQ 几乎支持所有常用语言，比如 Java、 Python、 Ruby、 PHP、 C#、 JavaScript 等。</p><p>管理界面 : RabbitMQ 提供了一个易用的用户界面，使得用户可以监控和管理消息、集 群中的节点等。</p><p>令插件机制: RabbitMQ 提供了许多插件 ， 以实现从多方面进行扩展，当然也可以编写自 己的插件。</p><h2 id="3-AMQP是什么"><a href="#3-AMQP是什么" class="headerlink" title="3. AMQP是什么?"></a>3. AMQP是什么?</h2><p>RabbitMQ就是 AMQP 协议的 <code>Erlang</code> 的实现(当然 RabbitMQ 还支持 <code>STOMP2</code>、 <code>MQTT3</code> 等协议 ) AMQP 的模型架构 和 RabbitMQ 的模型架构是一样的，生产者将消息发送给交换器，交换器和队列绑定 。</p><p>RabbitMQ 中的交换器、交换器类型、队列、绑定、路由键等都是遵循的 AMQP 协议中相 应的概念。目前 RabbitMQ 最新版本默认支持的是 AMQP 0-9-1。</p><h2 id="4-AMQP的3层协议？"><a href="#4-AMQP的3层协议？" class="headerlink" title="4. AMQP的3层协议？"></a>4. AMQP的3层协议？</h2><p>Module Layer:协议最高层，主要定义了一些客户端调用的命令，客户端可以用这些命令实现自己的业务逻辑。</p><p>Session Layer:中间层，主要负责客户端命令发送给服务器，再将服务端应答返回客户端，提供可靠性同步机制和错误处理。</p><p>TransportLayer:最底层，主要传输二进制数据流，提供帧的处理、信道服用、错误检测和数据表示等。</p><h2 id="5-说说Broker服务节点、Queue队列、Exchange交换器？"><a href="#5-说说Broker服务节点、Queue队列、Exchange交换器？" class="headerlink" title="5. 说说Broker服务节点、Queue队列、Exchange交换器？"></a>5. 说说Broker服务节点、Queue队列、Exchange交换器？</h2><ul><li>Broker可以看做RabbitMQ的服务节点。一般请下一个Broker可以看做一个RabbitMQ服务器。</li><li>Queue:RabbitMQ的内部对象，用于存储消息。多个消费者可以订阅同一队列，这时队列中的消息会被平摊（轮询）给多个消费者进行处理。</li><li>Exchange:生产者将消息发送到交换器，由交换器将消息路由到一个或者多个队列中。当路由不到时，或返回给生产者或直接丢弃。</li></ul><h2 id="6-如何保证消息的可靠性？"><a href="#6-如何保证消息的可靠性？" class="headerlink" title="6. 如何保证消息的可靠性？"></a>6. 如何保证消息的可靠性？</h2><p>分三点：</p><ul><li>生产者到RabbitMQ：事务机制和Confirm机制，注意：事务机制和 Confirm 机制是互斥的，两者不能共存，会导致 RabbitMQ 报错。</li><li>RabbitMQ自身：持久化、集群、普通模式、镜像模式。</li><li>RabbitMQ到消费者：basicAck机制、死信队列、消息补偿机制。</li></ul><h2 id="7-生产者消息运转的流程？"><a href="#7-生产者消息运转的流程？" class="headerlink" title="7. 生产者消息运转的流程？"></a>7. 生产者消息运转的流程？</h2><ol><li><p><code>Producer</code>先连接到Broker,建立连接Connection,开启一个信道(Channel)。</p></li><li><p><code>Producer</code>声明一个交换器并设置好相关属性。</p></li><li><p><code>Producer</code>声明一个队列并设置好相关属性。</p></li><li><p><code>Producer</code>通过路由键将交换器和队列绑定起来。</p></li><li><p><code>Producer</code>发送消息到<code>Broker</code>,其中包含路由键、交换器等信息。</p></li><li><p>相应的交换器根据接收到的路由键查找匹配的队列。</p></li><li><p>如果找到，将消息存入对应的队列，如果没有找到，会根据生产者的配置丢弃或者退回给生产者。</p></li><li><p>关闭信道。</p></li><li><p>管理连接。</p></li></ol><h2 id="8-消费者接收消息过程？"><a href="#8-消费者接收消息过程？" class="headerlink" title="8.消费者接收消息过程？"></a>8.消费者接收消息过程？</h2><ol><li><p><code>Producer</code>先连接到<code>Broker</code>,建立连接<code>Connection</code>,开启一个信道(<code>Channel</code>)。</p></li><li><p>向<code>Broker</code>请求消费响应的队列中消息，可能会设置响应的回调函数。</p></li><li><p>等待<code>Broker</code>回应并投递相应队列中的消息，接收消息。</p></li><li><p>消费者确认收到的消息,<code>ack</code>。</p></li><li><p><code>RabbitMq</code>从队列中删除已经确定的消息。</p></li><li><p>关闭信道。</p></li><li><p>关闭连接。</p></li></ol><h2 id="9-生产者如何将消息可靠投递到RabbitMQ？"><a href="#9-生产者如何将消息可靠投递到RabbitMQ？" class="headerlink" title="9. 生产者如何将消息可靠投递到RabbitMQ？"></a>9. 生产者如何将消息可靠投递到RabbitMQ？</h2><ol><li><p>Client发送消息给MQ</p></li><li><p>MQ将消息持久化后，发送Ack消息给Client，此处有可能因为网络问题导致Ack消息无法发送到Client，那么Client在等待超时后，会重传消息；</p></li><li><p>Client收到Ack消息后，认为消息已经投递成功。</p></li></ol><h2 id="10-RabbitMQ如何将消息可靠投递到消费者？"><a href="#10-RabbitMQ如何将消息可靠投递到消费者？" class="headerlink" title="10. RabbitMQ如何将消息可靠投递到消费者？"></a>10. RabbitMQ如何将消息可靠投递到消费者？</h2><ol><li><p>MQ将消息push给Client（或Client来pull消息）</p></li><li><p>Client得到消息并做完业务逻辑</p></li><li><p>Client发送Ack消息给MQ，通知MQ删除该消息，此处有可能因为网络问题导致Ack失败，那么Client会重复消息，这里就引出消费幂等的问题；</p></li><li><p>MQ将已消费的消息删除。</p></li></ol><h2 id="11-如何保证RabbitMQ消息队列的高可用"><a href="#11-如何保证RabbitMQ消息队列的高可用" class="headerlink" title="11. 如何保证RabbitMQ消息队列的高可用?"></a>11. 如何保证RabbitMQ消息队列的高可用?</h2><p>RabbitMQ 有三种模式：<code>单机模式</code>，<code>普通集群模式</code>，<code>镜像集群模式</code>。</p><p><strong>单机模式</strong>：就是demo级别的，一般就是你本地启动了玩玩儿的，没人生产用单机模式</p><p><strong>普通集群模式</strong>：意思就是在多台机器上启动多个RabbitMQ实例，每个机器启动一个。</p><p><strong>镜像集群模式</strong>：这种模式，才是所谓的RabbitMQ的高可用模式，跟普通集群模式不一样的是，你创建的queue，无论元数据(元数据指RabbitMQ的配置数据)还是queue里的消息都会存在于多个实例上，然后每次你写消息到queue的时候，都会自动把消息到多个实例的queue里进行消息同步。</p><h1 id="RocketMQ"><a href="#RocketMQ" class="headerlink" title="RocketMQ"></a>RocketMQ</h1><h2 id="1-RocketMQ是什么？"><a href="#1-RocketMQ是什么？" class="headerlink" title="1. RocketMQ是什么？"></a>1. RocketMQ是什么？</h2><p>RocketMQ 是阿里巴巴开源的分布式消息中间件。支持事务消息、顺序消息、批量消息、定时消息、消息回溯等。它里面有几个区别于标准消息中件间的概念，如Group、Topic、Queue等。系统组成则由Producer、Consumer、Broker、NameServer等。</p><p><strong>RocketMQ 特点</strong></p><ul><li>是一个队列模型的消息中间件，具有高性能、高可靠、高实时、分布式等特点</li><li>Producer、Consumer、队列都可以分布式</li><li>Producer 向一些队列轮流发送消息，队列集合称为 Topic，Consumer 如果做广播消费，则一个 Consumer 实例消费这个 Topic 对应的所有队列，如果做集群消费，则多个 Consumer 实例平均消费这个 Topic 对应的队列集合</li><li>能够保证严格的消息顺序</li><li>支持拉（pull）和推（push）两种消息模式</li><li>高效的订阅者水平扩展能力</li><li>实时的消息订阅机制</li><li>亿级消息堆积能力</li><li>支持多种消息协议，如 JMS、OpenMessaging 等</li><li>较少的依赖</li></ul><h2 id="2-RocketMQ由哪些角色组成，每个角色作用和特点是什么？"><a href="#2-RocketMQ由哪些角色组成，每个角色作用和特点是什么？" class="headerlink" title="2. RocketMQ由哪些角色组成，每个角色作用和特点是什么？"></a>2. RocketMQ由哪些角色组成，每个角色作用和特点是什么？</h2><table><thead><tr><th>角色</th><th>作用</th></tr></thead><tbody><tr><td>Nameserver</td><td>无状态，动态列表；这也是和zookeeper的重要区别之一。zookeeper是有状态的。</td></tr><tr><td>Producer</td><td>消息生产者，负责发消息到Broker。</td></tr><tr><td>Broker</td><td>就是MQ本身，负责收发消息、持久化消息等。</td></tr><tr><td>Consumer</td><td>消息消费者，负责从Broker上拉取消息进行消费，消费完进行ack。</td></tr></tbody></table><h2 id="3-RocketMQ消费模式有几种？"><a href="#3-RocketMQ消费模式有几种？" class="headerlink" title="3. RocketMQ消费模式有几种？"></a>3. RocketMQ消费模式有几种？</h2><p>消费模型由Consumer决定，消费维度为Topic。</p><p>1、集群消费</p><ul><li><p>一条消息只会被同Group中的一个Consumer消费</p></li><li><p>多个Group同时消费一个Topic时，每个Group都会有一个Consumer消费到数据</p></li></ul><p>2、广播消费</p><p>消息将对一 个Consumer Group 下的各个 Consumer 实例都消费一遍。即即使这些 Consumer 属于同一个Consumer Group ，消息也会被 Consumer Group 中的每个 Consumer 都消费一次。</p><h2 id="4-RocketMQ消费消息是push还是pull？"><a href="#4-RocketMQ消费消息是push还是pull？" class="headerlink" title="4. RocketMQ消费消息是push还是pull？"></a>4. RocketMQ消费消息是push还是pull？</h2><p>RocketMQ没有真正意义的push，都是pull，虽然有push类，但实际底层实现采用的是<strong>长轮询机制</strong>，即拉取方式</p><blockquote><p>broker端属性 longPollingEnable 标记是否开启长轮询。默认开启</p></blockquote><h3 id="追问：为什么要主动拉取消息而不使用事件监听方式？"><a href="#追问：为什么要主动拉取消息而不使用事件监听方式？" class="headerlink" title="追问：为什么要主动拉取消息而不使用事件监听方式？"></a>追问：为什么要主动拉取消息而不使用事件监听方式？</h3><p>事件驱动方式是建立好长连接，由事件（发送数据）的方式来实时推送。</p><p>如果broker主动推送消息的话有可能push速度快，消费速度慢的情况，那么就会造成消息在consumer端堆积过多，同时又不能被其他consumer消费的情况。而pull的方式可以根据当前自身情况来pull，不会造成过多的压力而造成瓶颈。所以采取了pull的方式。</p><h2 id="5-broker如何处理拉取请求的？"><a href="#5-broker如何处理拉取请求的？" class="headerlink" title="5. broker如何处理拉取请求的？"></a>5. broker如何处理拉取请求的？</h2><p>Consumer首次请求Broker</p><ul><li><p>Broker中是否有符合条件的消息</p></li><li><p>有</p></li><li><ul><li>响应Consumer</li></ul></li><li><p>等待下次Consumer的请求</p></li><li><p>没有</p></li><li><ul><li>DefaultMessageStore#ReputMessageService#run方法</li></ul></li><li><p>PullRequestHoldService 来Hold连接，每个5s执行一次检查pullRequestTable有没有消息，有的话立即推送</p></li><li><p>每隔1ms检查commitLog中是否有新消息，有的话写入到pullRequestTable</p></li><li><p>当有新消息的时候返回请求</p></li><li><p>挂起consumer的请求，即不断开连接，也不返回数据</p></li><li><p>使用consumer的offset，</p></li></ul><h2 id="6-如何让RocketMQ保证消息的顺序消费？"><a href="#6-如何让RocketMQ保证消息的顺序消费？" class="headerlink" title="6. 如何让RocketMQ保证消息的顺序消费？"></a>6. 如何让RocketMQ保证消息的顺序消费？</h2><p>首先多个queue只能保证单个queue里的顺序，queue是典型的FIFO，天然顺序。多个queue同时消费是无法绝对保证消息的有序性的。所以总结如下：</p><p>同一topic，同一个QUEUE，发消息的时候一个线程去发送消息，消费的时候 一个线程去消费一个queue里的消息。</p><h2 id="7-RocketMQ如何保证消息不丢失？"><a href="#7-RocketMQ如何保证消息不丢失？" class="headerlink" title="7. RocketMQ如何保证消息不丢失？"></a>7. RocketMQ如何保证消息不丢失？</h2><p>首先在如下三个部分都可能会出现丢失消息的情况：</p><ul><li>Producer端</li><li>Broker端</li><li>Consumer端</li></ul><p>1 、Producer端如何保证消息不丢失</p><ul><li><p>采取send()同步发消息，发送结果是同步感知的。</p></li><li><p>发送失败后可以重试，设置重试次数。默认3次。</p></li><li><p>集群部署，比如发送失败了的原因可能是当前Broker宕机了，重试的时候会发送到其他Broker上。</p></li></ul><p>2、Broker端如何保证消息不丢失</p><ul><li><p>修改刷盘策略为同步刷盘。默认情况下是异步刷盘的。</p></li><li><p>集群部署，主从模式，高可用。</p></li></ul><p>3、Consumer端如何保证消息不丢失</p><ul><li>完全消费正常后在进行手动ack确认。</li></ul><h2 id="7-rocketMQ的消息堆积如何处理？"><a href="#7-rocketMQ的消息堆积如何处理？" class="headerlink" title="7. rocketMQ的消息堆积如何处理？"></a>7. rocketMQ的消息堆积如何处理？</h2><p>首先要找到是什么原因导致的消息堆积，是Producer太多了，Consumer太少了导致的还是说其他情况，总之先定位问题。</p><p>然后看下消息消费速度是否正常，正常的话，可以通过上线更多consumer临时解决消息堆积问题</p><h3 id="追问：如果Consumer和Queue不对等，上线了多台也在短时间内无法消费完堆积的消息怎么办？"><a href="#追问：如果Consumer和Queue不对等，上线了多台也在短时间内无法消费完堆积的消息怎么办？" class="headerlink" title="追问：如果Consumer和Queue不对等，上线了多台也在短时间内无法消费完堆积的消息怎么办？"></a>追问：如果Consumer和Queue不对等，上线了多台也在短时间内无法消费完堆积的消息怎么办？</h3><ul><li>准备一个临时的topic</li><li>queue的数量是堆积的几倍</li><li>queue分布到多Broker中</li><li>上线一台Consumer做消息的搬运工，把原来Topic中的消息挪到新的Topic里，不做业务逻辑处理，只是挪过去</li><li>上线N台Consumer同时消费临时Topic中的数据</li><li>改bug</li><li>恢复原来的Consumer，继续消费之前的Topic</li></ul><h3 id="追问：堆积时间过长消息超时了？"><a href="#追问：堆积时间过长消息超时了？" class="headerlink" title="追问：堆积时间过长消息超时了？"></a>追问：堆积时间过长消息超时了？</h3><p>RocketMQ中的消息只会在commitLog被删除的时候才会消失，不会超时。也就是说未被消费的消息不会存在超时删除这情况。</p><h3 id="追问：堆积的消息会不会进死信队列？"><a href="#追问：堆积的消息会不会进死信队列？" class="headerlink" title="追问：堆积的消息会不会进死信队列？"></a>追问：堆积的消息会不会进死信队列？</h3><p>不会，消息在消费失败后会进入重试队列（%RETRY%+ConsumerGroup），18次（默认18次，网上所有文章都说是16次，无一例外。但是我没搞懂为啥是16次，这不是18个时间吗 ？）才会进入死信队列（%DLQ%+ConsumerGroup）。</p><h2 id="8-RocketMQ为什么自研nameserver而不用zk？"><a href="#8-RocketMQ为什么自研nameserver而不用zk？" class="headerlink" title="8. RocketMQ为什么自研nameserver而不用zk？"></a>8. RocketMQ为什么自研nameserver而不用zk？</h2><ol><li>RocketMQ只需要一个轻量级的维护元数据信息的组件，为此引入zk增加维护成本还强依赖另一个中间件了。</li><li>RocketMQ追求的是AP，而不是CP，也就是需要高可用。<ul><li>zk是CP，因为zk节点间通过zap协议有数据共享，每个节点数据会一致，但是zk集群当挂了一半以上的节点就没法使用了。</li><li>nameserver是AP，节点间不通信，这样会导致节点间数据信息会发生短暂的不一致，但每个broker都会定时向所有nameserver上报路由信息和心跳。当某个broker下线了，nameserver也会延时30s才知道，而且不会通知客户端（生产和消费者），只能靠客户端自己来拉，rocketMQ是靠消息重试机制解决这个问题的，所以是最终一致性。但nameserver集群只要有一个节点就可用。<a target="_blank" rel="noopener" href="https://juejin.cn/post/6844904068771479559">https://juejin.cn/post/6844904068771479559</a></li></ul></li></ol></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://enpsl.github.io/2021/02/15/2021-02-15-kafka%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="enpsl"><meta itemprop="description" content="my blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="彭诗亮的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"> <a href="/2021/02/15/2021-02-15-kafka%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">Kafka面试问题总结</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-02-15 19:30:00" itemprop="dateCreated datePublished" datetime="2021-02-15T19:30:00+08:00">2021-02-15</time></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="1-Apache-Kafka是什么？"><a href="#1-Apache-Kafka是什么？" class="headerlink" title="1. Apache Kafka是什么？"></a>1. Apache Kafka是什么？</h2><p>Apach Kafka是一款分布式流处理平台，用于实时构建流处理应用。它有一个核心的功能广为人知，即作为企业级的消息引擎被广泛使用（通常也会称之为消息总线message bus）。</p><h2 id="2-Kafka-的设计是什么样的？"><a href="#2-Kafka-的设计是什么样的？" class="headerlink" title="2. Kafka 的设计是什么样的？"></a>2. Kafka 的设计是什么样的？</h2><p>Kafka 将消息以 topic 为单位进行归纳</p><p>将向 Kafka topic 发布消息的程序成为 producers.</p><p>将预订 topics 并消费消息的程序成为 consumer.</p><p>Kafka 以集群的方式运行，可以由一个或多个服务组成，每个服务叫做一个 broker.</p><p>producers 通过网络将消息发送到 Kafka 集群，集群向消费者提供消息</p><h2 id="3-Kafka-如何保证高可用？"><a href="#3-Kafka-如何保证高可用？" class="headerlink" title="3. Kafka 如何保证高可用？"></a>3. Kafka 如何保证高可用？</h2><p><code>Kafka</code> 的基本架构组成是：由多个 <code>broker</code> 组成一个集群，每个 <code>broker</code> 是一个节点；当创建一个 <code>topic</code> 时，这个 <code>topic</code> 会被划分为多个 <code>partition</code>，每个 <code>partition</code> 可以存在于不同的 <code>broker</code> 上，每个 <code>partition</code> 只存放一部分数据。</p><p>这就是<strong>天然的分布式消息队列</strong>，就是说一个 <code>topic</code> 的数据，是<strong>分散放在多个机器上的，每个机器就放一部分数据</strong>。</p><p>在 <code>Kafka 0.8</code> 版本之前，是没有 <code>HA</code> 机制的，当任何一个 <code>broker</code> 所在节点宕机了，这个 <code>broker</code> 上的 <code>partition</code> 就无法提供读写服务，所以这个版本之前，<code>Kafka</code> 没有什么高可用性可言。</p><p>在 <code>Kafka 0.8</code> 以后，提供了 <code>HA</code> 机制，就是 <code>replica</code> 副本机制。每个 <code>partition</code> 上的数据都会同步到其它机器，形成自己的多个 <code>replica</code> 副本。所有 <code>replica</code> 会选举一个 <code>leader</code> 出来，消息的生产者和消费者都跟这个 <code>leader</code> 打交道，其他 <code>replica</code> 作为 <code>follower</code>。写的时候，<code>leader</code> 会负责把数据同步到所有 <code>follower</code> 上去，读的时候就直接读 <code>leader</code> 上的数据即可。<code>Kafka</code> 负责均匀的将一个 <code>partition</code> 的所有 <code>replica</code> 分布在不同的机器上，这样才可以提高容错性。</p><p><img src="http://blog-img.coolsen.cn/img/Solve-MQ-Problem-With-Kafka-01.png" alt="img"></p><p>拥有了 <code>replica</code> 副本机制，如果某个 <code>broker</code> 宕机了，这个 <code>broker</code> 上的 <code>partition</code> 在其他机器上还存在副本。如果这个宕机的 <code>broker</code> 上面有某个 <code>partition</code> 的 <code>leader</code>，那么此时会从其 <code>follower</code> 中重新选举一个新的 <code>leader</code> 出来，这个新的 <code>leader</code> 会继续提供读写服务，这就有达到了所谓的高可用性。</p><p>写数据的时候，生产者只将数据写入 <code>leader</code> 节点，<code>leader</code> 会将数据写入本地磁盘，接着其他 <code>follower</code> 会主动从 <code>leader</code> 来拉取数据，<code>follower</code> 同步好数据了，就会发送 <code>ack</code> 给 <code>leader</code>，<code>leader</code> 收到所有 <code>follower</code> 的 <code>ack</code> 之后，就会返回写成功的消息给生产者。</p><p>消费数据的时候，消费者只会从 <code>leader</code> 节点去读取消息，但是只有当一个消息已经被所有 <code>follower</code> 都同步成功返回 <code>ack</code> 的时候，这个消息才会被消费者读到。</p><p><img src="https://gitee.com/dongzl/article-images/raw/master/2020/13-Solve-MQ-Problem-With-Kafka/Solve-MQ-Problem-With-Kafka-02.png" alt="img"></p><h2 id="4-Kafka-消息是采用-Pull-模式，还是-Push-模式？"><a href="#4-Kafka-消息是采用-Pull-模式，还是-Push-模式？" class="headerlink" title="4. Kafka 消息是采用 Pull 模式，还是 Push 模式？"></a>4. Kafka 消息是采用 Pull 模式，还是 Push 模式？</h2><p>生产者使用push模式将消息发布到Broker，消费者使用pull模式从Broker订阅消息。</p><p>push模式很难适应消费速率不同的消费者，如果push的速度太快，容易造成消费者拒绝服务或网络拥塞；如果push的速度太慢，容易造成消费者性能浪费。但是采用pull的方式也有一个缺点，就是当Broker没有消息时，消费者会陷入不断地轮询中，为了避免这点，kafka有个参数可以让消费者阻塞知道是否有新消息到达。</p><h2 id="5-Kafka-与传统消息系统之间的区别"><a href="#5-Kafka-与传统消息系统之间的区别" class="headerlink" title="5. Kafka 与传统消息系统之间的区别"></a>5. Kafka 与传统消息系统之间的区别</h2><ul><li><p>Kafka 持久化日志，这些日志可以被重复读取和无限期保留</p></li><li><p>Kafka 是一个分布式系统：它以集群的方式运行，可以灵活伸缩，在内部通过复制数据提升容错能力和高可用性</p></li><li><p>Kafka 支持实时的流式处理</p></li></ul><h2 id="6-什么是消费者组？"><a href="#6-什么是消费者组？" class="headerlink" title="6. 什么是消费者组？"></a>6. 什么是消费者组？</h2><p>消费者组是Kafka独有的概念，即消费者组是Kafka提供的可扩展且具有容错性的消费者机制。</p><p>但实际上，消费者组（Consumer Group）其实包含两个概念，作为队列，消费者组允许你分割数据处理到一组进程集合上（即一个消费者组中可以包含多个消费者进程，他们共同消费该topic的数据），这有助于你的消费能力的动态调整；作为发布-订阅模型（publish-subscribe），Kafka允许你将同一份消息广播到多个消费者组里，以此来丰富多种数据使用场景。</p><p>需要注意的是：在消费者组中，多个实例共同订阅若干个主题，实现共同消费。同一个组下的每个实例都配置有相同的组ID，被分配不同的订阅分区。当某个实例挂掉的时候，其他实例会自动地承担起它负责消费的分区。 因此，消费者组在一定程度上也保证了消费者程序的高可用性。</p><p><a target="_blank" rel="noopener" href="http://dockone.io/uploads/article/20201024/7b359b7a1381541fbacf3ecf20dfb347.jpg"><img src="http://dockone.io/uploads/article/20201024/7b359b7a1381541fbacf3ecf20dfb347.jpg" alt="1.jpg"></a></p><h2 id="7-在Kafka中，ZooKeeper的作用是什么？"><a href="#7-在Kafka中，ZooKeeper的作用是什么？" class="headerlink" title="7. 在Kafka中，ZooKeeper的作用是什么？"></a>7. 在Kafka中，ZooKeeper的作用是什么？</h2><p>目前，Kafka使用ZooKeeper存放集群元数据、成员管理、Controller选举，以及其他一些管理类任务。之后，等KIP-500提案完成后，Kafka将完全不再依赖于ZooKeeper。</p><ul><li>“存放元数据”是指主题分区的所有数据都保存在 ZooKeeper 中，且以它保存的数据为权威，其他 “人” 都要与它保持对齐。</li><li>“成员管理” 是指 Broker 节点的注册、注销以及属性变更，等等。</li><li>“Controller 选举” 是指选举集群 Controller，而其他管理类任务包括但不限于主题删除、参数配置等。</li></ul><p>KIP-500 思想，是使用社区自研的基于Raft的共识算法，替代ZooKeeper，实现Controller自选举。</p><h2 id="8-解释下Kafka中位移（offset）的作用"><a href="#8-解释下Kafka中位移（offset）的作用" class="headerlink" title="8. 解释下Kafka中位移（offset）的作用"></a>8. 解释下Kafka中位移（offset）的作用</h2><p>在Kafka中，每个主题分区下的每条消息都被赋予了一个唯一的ID数值，用于标识它在分区中的位置。这个ID数值，就被称为位移，或者叫偏移量。一旦消息被写入到分区日志，它的位移值将不能被修改。</p><h2 id="9-kafka-为什么那么快？"><a href="#9-kafka-为什么那么快？" class="headerlink" title="9. kafka 为什么那么快？"></a>9. kafka 为什么那么快？</h2><ul><li>Cache Filesystem Cache PageCache缓存</li><li><code>顺序写</code>：由于现代的操作系统提供了预读和写技术，磁盘的顺序写大多数情况下比随机写内存还要快。</li><li><code>Zero-copy</code>：零拷技术减少拷贝次数</li><li><code>Batching of Messages</code>：批量量处理。合并小的请求，然后以流的方式进行交互，直顶网络上限。</li><li><code>Pull 拉模式</code>：使用拉模式进行消息的获取消费，与消费端处理能力相符。</li></ul><h2 id="10-kafka-producer发送数据，ack为0，1，-1分别是什么意思？"><a href="#10-kafka-producer发送数据，ack为0，1，-1分别是什么意思？" class="headerlink" title="10. kafka producer发送数据，ack为0，1，-1分别是什么意思？"></a>10. kafka producer发送数据，ack为0，1，-1分别是什么意思？</h2><ul><li><code>1</code>（默认） 数据发送到Kafka后，经过leader成功接收消息的的确认，就算是发送成功了。在这种情况下，如果leader宕机了，则会丢失数据。</li><li><code>0</code> 生产者将数据发送出去就不管了，不去等待任何返回。这种情况下数据传输效率最高，但是数据可靠性确是最低的。</li><li><code>-1</code>producer需要等待ISR中的所有follower都确认接收到数据后才算一次发送完成，可靠性最高。当ISR中所有Replica都向Leader发送ACK时，leader才commit，这时候producer才能认为一个请求中的消息都commit了。</li></ul><h2 id="11-Kafka如何保证消息不丢失"><a href="#11-Kafka如何保证消息不丢失" class="headerlink" title="11. Kafka如何保证消息不丢失?"></a>11. Kafka如何保证消息不丢失?</h2><p>首先需要弄明白消息为什么会丢失，对于一个消息队列，会有 <code>生产者</code>、<code>MQ</code>、<code>消费者</code> 这三个角色，在这三个角色数据处理和传输过程中，都有可能会出现消息丢失。</p><p><img src="http://blog-img.coolsen.cn/img/Solve-MQ-Problem-With-Kafka-03.png" alt="img"></p><p>消息丢失的原因以及解决办法：</p><h3 id="消费者异常导致的消息丢失"><a href="#消费者异常导致的消息丢失" class="headerlink" title="消费者异常导致的消息丢失"></a>消费者异常导致的消息丢失</h3><p>消费者可能导致数据丢失的情况是：消费者获取到了这条消息后，还未处理，<code>Kafka</code> 就自动提交了 <code>offset</code>，这时 <code>Kafka</code> 就认为消费者已经处理完这条消息，其实消费者才刚准备处理这条消息，这时如果消费者宕机，那这条消息就丢失了。</p><p>消费者引起消息丢失的主要原因就是消息还未处理完 <code>Kafka</code> 会自动提交了 <code>offset</code>，那么只要关闭自动提交 <code>offset</code>，消费者在处理完之后手动提交 <code>offset</code>，就可以保证消息不会丢失。但是此时需要注意重复消费问题，比如消费者刚处理完，还没提交 <code>offset</code>，这时自己宕机了，此时这条消息肯定会被重复消费一次，这就需要消费者根据实际情况保证幂等性。</p><h3 id="生产者数据传输导致的消息丢失"><a href="#生产者数据传输导致的消息丢失" class="headerlink" title="生产者数据传输导致的消息丢失"></a>生产者数据传输导致的消息丢失</h3><p>对于生产者数据传输导致的数据丢失主常见情况是生产者发送消息给 <code>Kafka</code>，由于网络等原因导致消息丢失，对于这种情况也是通过在 <strong>producer</strong> 端设置 <strong>acks&#x3D;all</strong> 来处理，这个参数是要求 <code>leader</code> 接收到消息后，需要等到所有的 <code>follower</code> 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试。</p><h3 id="Kafka-导致的消息丢失"><a href="#Kafka-导致的消息丢失" class="headerlink" title="Kafka 导致的消息丢失"></a>Kafka 导致的消息丢失</h3><p><code>Kafka</code> 导致的数据丢失一个常见的场景就是 <code>Kafka</code> 某个 <code>broker</code> 宕机，，而这个节点正好是某个 <code>partition</code> 的 <code>leader</code> 节点，这时需要重新重新选举该 <code>partition</code> 的 <code>leader</code>。如果该 <code>partition</code> 的 <code>leader</code> 在宕机时刚好还有些数据没有同步到 <code>follower</code>，此时 <code>leader</code> 挂了，在选举某个 <code>follower</code> 成 <code>leader</code> 之后，就会丢失一部分数据。</p><p>对于这个问题，<code>Kafka</code> 可以设置如下 4 个参数，来尽量避免消息丢失：</p><ul><li>给 <code>topic</code> 设置 <code>replication.factor</code> 参数：这个值必须大于 <code>1</code>，要求每个 <code>partition</code> 必须有至少 <code>2</code> 个副本；</li><li>在 <code>Kafka</code> 服务端设置 <code>min.insync.replicas</code> 参数：这个值必须大于 <code>1</code>，这个参数的含义是一个 <code>leader</code> 至少感知到有至少一个 <code>follower</code> 还跟自己保持联系，没掉队，这样才能确保 <code>leader</code> 挂了还有一个 <code>follower</code> 节点。</li><li>在 <code>producer</code> 端设置 <code>acks=all</code>，这个是要求每条数据，必须是写入所有 <code>replica</code> 之后，才能认为是写成功了；</li><li>在 <code>producer</code> 端设置 <code>retries=MAX</code>（很大很大很大的一个值，无限次重试的意思）：这个参数的含义是一旦写入失败，就无限重试，卡在这里了。</li></ul><h2 id="13-Kafka-如何保证消息的顺序性"><a href="#13-Kafka-如何保证消息的顺序性" class="headerlink" title="13. Kafka 如何保证消息的顺序性"></a>13. Kafka 如何保证消息的顺序性</h2><p>在某些业务场景下，我们需要保证对于有逻辑关联的多条MQ消息被按顺序处理，比如对于某一条数据，正常处理顺序是<code>新增-更新-删除</code>，最终结果是数据被删除；如果消息没有按序消费，处理顺序可能是<code>删除-新增-更新</code>，最终数据没有被删掉，可能会产生一些逻辑错误。对于如何保证消息的顺序性，主要需要考虑如下两点：</p><ul><li>如何保证消息在 <code>Kafka</code> 中顺序性；</li><li>如何保证消费者处理消费的顺序性。</li></ul><h3 id="如何保证消息在-Kafka-中顺序性"><a href="#如何保证消息在-Kafka-中顺序性" class="headerlink" title="如何保证消息在 Kafka 中顺序性"></a>如何保证消息在 Kafka 中顺序性</h3><p>对于 <code>Kafka</code>，如果我们创建了一个 <code>topic</code>，默认有三个 <code>partition</code>。生产者在写数据的时候，可以指定一个 <code>key</code>，比如在订单 <code>topic</code> 中我们可以指定订单 <code>id</code> 作为 <code>key</code>，那么相同订单 <code>id</code> 的数据，一定会被分发到同一个 <code>partition</code> 中去，而且这个 <code>partition</code> 中的数据一定是有顺序的。消费者从 <code>partition</code> 中取出来数据的时候，也一定是有顺序的。通过制定 <code>key</code> 的方式首先可以保证在 <code>kafka</code> 内部消息是有序的。</p><h3 id="如何保证消费者处理消费的顺序性"><a href="#如何保证消费者处理消费的顺序性" class="headerlink" title="如何保证消费者处理消费的顺序性"></a>如何保证消费者处理消费的顺序性</h3><p>对于某个 <code>topic</code> 的一个 <code>partition</code>，只能被同组内部的一个 <code>consumer</code> 消费，如果这个 <code>consumer</code> 内部还是单线程处理，那么其实只要保证消息在 <code>MQ</code> 内部是有顺序的就可以保证消费也是有顺序的。但是单线程吞吐量太低，在处理大量 <code>MQ</code> 消息时，我们一般会开启多线程消费机制，那么如何保证消息在多个线程之间是被顺序处理的呢？对于多线程消费我们可以预先设置 <code>N</code> 个内存 <code>Queue</code>，具有相同 <code>key</code> 的数据都放到同一个内存 <code>Queue</code> 中；然后开启 <code>N</code> 个线程，每个线程分别消费一个内存 <code>Queue</code> 的数据即可，这样就能保证顺序性。当然，消息放到内存 <code>Queue</code> 中，有可能还未被处理，<code>consumer</code> 发生宕机，内存 <code>Queue</code> 中的数据会全部丢失，这就转变为上面提到的<strong>如何保证消息的可靠传输</strong>的问题了。</p><h2 id="14-Kafka中的ISR、AR代表什么？ISR的伸缩指什么？"><a href="#14-Kafka中的ISR、AR代表什么？ISR的伸缩指什么？" class="headerlink" title="14. Kafka中的ISR、AR代表什么？ISR的伸缩指什么？"></a>14. Kafka中的ISR、AR代表什么？ISR的伸缩指什么？</h2><ul><li><code>ISR</code>：In-Sync Replicas 副本同步队列</li><li><code>AR</code>:Assigned Replicas 所有副本</li></ul><p>ISR是由leader维护，follower从leader同步数据有一些延迟（包括<code>延迟时间replica.lag.time.max.ms</code>和<code>延迟条数replica.lag.max.messages</code>两个维度，当前最新的版本0.10.x中只支持<code>replica.lag.time.max.ms</code>这个维度），任意一个超过阈值都会把follower剔除出ISR，存入OSR（Outof-Sync Replicas）列表，新加入的follower也会先存放在OSR中。</p><blockquote><p>AR&#x3D;ISR+OSR。</p></blockquote><h2 id="15-描述下-Kafka-中的领导者副本（Leader-Replica）和追随者副本（Follower-Replica）的区别"><a href="#15-描述下-Kafka-中的领导者副本（Leader-Replica）和追随者副本（Follower-Replica）的区别" class="headerlink" title="15. 描述下 Kafka 中的领导者副本（Leader Replica）和追随者副本（Follower Replica）的区别"></a>15. 描述下 Kafka 中的领导者副本（Leader Replica）和追随者副本（Follower Replica）的区别</h2><p>Kafka副本当前分为领导者副本和追随者副本。只有Leader副本才能对外提供读写服务，响应Clients端的请求。Follower副本只是采用拉（PULL）的方式，被动地同步Leader副本中的数据，并且在Leader副本所在的Broker宕机后，随时准备应聘Leader副本。</p><p>加分点：</p><ul><li>强调Follower副本也能对外提供读服务。自Kafka 2.4版本开始，社区通过引入新的Broker端参数，允许Follower副本有限度地提供读服务。</li><li>强调Leader和Follower的消息序列在实际场景中不一致。通常情况下，很多因素可能造成Leader和Follower之间的不同步，比如程序问题，网络问题，broker问题等，短暂的不同步我们可以关注（秒级别），但长时间的不同步可能就需要深入排查了，因为一旦Leader所在节点异常，可能直接影响可用性。</li></ul><p>注意：之前确保一致性的主要手段是高水位机制（HW），但高水位值无法保证Leader连续变更场景下的数据一致性，因此，社区引入了Leader Epoch机制，来修复高水位值的弊端。</p><h2 id="16-分区Leader选举策略有几种？"><a href="#16-分区Leader选举策略有几种？" class="headerlink" title="16. 分区Leader选举策略有几种？"></a>16. 分区Leader选举策略有几种？</h2><p>分区的Leader副本选举对用户是完全透明的，它是由Controller独立完成的。你需要回答的是，在哪些场景下，需要执行分区Leader选举。每一种场景对应于一种选举策略。</p><ul><li>OfflinePartition Leader选举：每当有分区上线时，就需要执行Leader选举。所谓的分区上线，可能是创建了新分区，也可能是之前的下线分区重新上线。这是最常见的分区Leader选举场景。</li><li>ReassignPartition Leader选举：当你手动运行kafka-reassign-partitions命令，或者是调用Admin的alterPartitionReassignments方法执行分区副本重分配时，可能触发此类选举。假设原来的AR是[1，2，3]，Leader是1，当执行副本重分配后，副本集合AR被设置成[4，5，6]，显然，Leader必须要变更，此时会发生Reassign Partition Leader选举。</li><li>PreferredReplicaPartition Leader选举：当你手动运行kafka-preferred-replica-election命令，或自动触发了Preferred Leader选举时，该类策略被激活。所谓的Preferred Leader，指的是AR中的第一个副本。比如AR是[3，2，1]，那么，Preferred Leader就是3。</li><li>ControlledShutdownPartition Leader选举：当Broker正常关闭时，该Broker上的所有Leader副本都会下线，因此，需要为受影响的分区执行相应的Leader选举。</li></ul><p>这4类选举策略的大致思想是类似的，即从AR中挑选首个在ISR中的副本，作为新Leader。</p><h2 id="17-Kafka的哪些场景中使用了零拷贝（Zero-Copy）？"><a href="#17-Kafka的哪些场景中使用了零拷贝（Zero-Copy）？" class="headerlink" title="17. Kafka的哪些场景中使用了零拷贝（Zero Copy）？"></a>17. Kafka的哪些场景中使用了零拷贝（Zero Copy）？</h2><p>在Kafka中，体现Zero Copy使用场景的地方有两处：基于mmap的索引和日志文件读写所用的TransportLayer。</p><p>先说第一个。索引都是基于MappedByteBuffer的，也就是让用户态和内核态共享内核态的数据缓冲区，此时，数据不需要复制到用户态空间。不过，mmap虽然避免了不必要的拷贝，但不一定就能保证很高的性能。在不同的操作系统下，mmap的创建和销毁成本可能是不一样的。很高的创建和销毁开销会抵消Zero Copy带来的性能优势。由于这种不确定性，在Kafka中，只有索引应用了mmap，最核心的日志并未使用mmap机制。</p><p>再说第二个。TransportLayer是Kafka传输层的接口。它的某个实现类使用了FileChannel的transferTo方法。该方法底层使用sendfile实现了Zero Copy。对Kafka而言，如果I&#x2F;O通道使用普通的PLAINTEXT，那么，Kafka就可以利用Zero Copy特性，直接将页缓存中的数据发送到网卡的Buffer中，避免中间的多次拷贝。相反，如果I&#x2F;O通道启用了SSL，那么，Kafka便无法利用Zero Copy特性了。</p><h2 id="18-为什么Kafka不支持读写分离？"><a href="#18-为什么Kafka不支持读写分离？" class="headerlink" title="18. 为什么Kafka不支持读写分离？"></a>18. 为什么Kafka不支持读写分离？</h2><p>在 Kafka 中，生产者写入消息、消费者读取消息的操作都是与 leader 副本进行交互的，从 而实现的是一种主写主读的生产消费模型。</p><p>Kafka 并不支持主写从读，因为主写从读有 2 个很明 显的缺点:</p><ul><li><strong>数据一致性问题</strong>。数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间 窗口会导致主从节点之间的数据不一致。某一时刻，在主节点和从节点中 A 数据的值都为 X， 之后将主节点中 A 的值修改为 Y，那么在这个变更通知到从节点之前，应用读取从节点中的 A 数据的值并不为最新的 Y，由此便产生了数据不一致的问题。</li><li><strong>延时问题</strong>。类似 Redis 这种组件，数据从写入主节点到同步至从节点中的过程需要经历<code>网络→主节点内存→网络→从节点内存</code>这几个阶段，整个过程会耗费一定的时间。而在 Kafka 中，主从同步会比 Redis 更加耗时，它需要经历<code>网络→主节点内存→主节点磁盘→网络→从节点内存→从节点磁盘</code>这几个阶段。对延时敏感的应用而言，主写从读的功能并不太适用。</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="http://dockone.io/article/10853">http://dockone.io/article/10853</a></p><p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000023716306">https://segmentfault.com/a/1190000023716306</a></p><p><a target="_blank" rel="noopener" href="https://dongzl.github.io/2020/03/16/13-Solve-MQ-Problem-With-Kafka/index.html">https://dongzl.github.io/2020/03/16/13-Solve-MQ-Problem-With-Kafka/index.html</a></p></div><footer class="post-footer"><div class="post-eof"></div></footer></article><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://enpsl.github.io/2021/02/13/2021-02-13-RocketMQ%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="enpsl"><meta itemprop="description" content="my blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="彭诗亮的博客"></span><header class="post-header"><h2 class="post-title" itemprop="name headline"> <a href="/2021/02/13/2021-02-13-RocketMQ%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">RocketMQ基础知识总结</a></h2><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2021-02-13 19:30:00" itemprop="dateCreated datePublished" datetime="2021-02-13T19:30:00+08:00">2021-02-13</time></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="消息队列扫盲"><a href="#消息队列扫盲" class="headerlink" title="消息队列扫盲"></a>消息队列扫盲</h2><p>消息队列顾名思义就是存放消息的队列，队列我就不解释了，别告诉我你连队列都不知道是啥吧？</p><p>所以问题并不是消息队列是什么，而是 <strong>消息队列为什么会出现？消息队列能用来干什么？用它来干这些事会带来什么好处？消息队列会带来副作用吗？</strong></p><h3 id="消息队列为什么会出现？"><a href="#消息队列为什么会出现？" class="headerlink" title="# 消息队列为什么会出现？"></a><a href="#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BC%9A%E5%87%BA%E7%8E%B0">#</a> 消息队列为什么会出现？</h3><p>消息队列算是作为后端程序员的一个必备技能吧，因为<strong>分布式应用必定涉及到各个系统之间的通信问题</strong>，这个时候消息队列也应运而生了。可以说分布式的产生是消息队列的基础，而分布式怕是一个很古老的概念了吧，所以消息队列也是一个很古老的中间件了。</p><h3 id="消息队列能用来干什么？"><a href="#消息队列能用来干什么？" class="headerlink" title="# 消息队列能用来干什么？"></a><a href="#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E8%83%BD%E7%94%A8%E6%9D%A5%E5%B9%B2%E4%BB%80%E4%B9%88">#</a> 消息队列能用来干什么？</h3><h4 id="异步"><a href="#异步" class="headerlink" title="# 异步"></a><a href="#%E5%BC%82%E6%AD%A5">#</a> 异步</h4><p>你可能会反驳我，应用之间的通信又不是只能由消息队列解决，好好的通信为什么中间非要插一个消息队列呢？我不能直接进行通信吗？</p><p>很好👍，你又提出了一个概念，<strong>同步通信</strong>。就比如现在业界使用比较多的 <code>Dubbo</code> 就是一个适用于各个系统之间同步通信的 <code>RPC</code> 框架。</p><p>我来举个🌰吧，比如我们有一个购票系统，需求是用户在购买完之后能接收到购买完成的短信。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef37fee7e09230.jpg" alt="img"></p><p>我们省略中间的网络通信时间消耗，假如购票系统处理需要 150ms ，短信系统处理需要 200ms ，那么整个处理流程的时间消耗就是 150ms + 200ms &#x3D; 350ms。</p><p>当然，乍看没什么问题。可是仔细一想你就感觉有点问题，我用户购票在购票系统的时候其实就已经完成了购买，而我现在通过同步调用非要让整个请求拉长时间，而短信系统这玩意又不是很有必要，它仅仅是一个辅助功能增强用户体验感而已。我现在整个调用流程就有点 <strong>头重脚轻</strong> 的感觉了，购票是一个不太耗时的流程，而我现在因为同步调用，非要等待发送短信这个比较耗时的操作才返回结果。那我如果再加一个发送邮件呢？</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef380429cf373e.jpg" alt="img"></p><p>这样整个系统的调用链又变长了，整个时间就变成了550ms。</p><p>当我们在学生时代需要在食堂排队的时候，我们和食堂大妈就是一个同步的模型。</p><p>我们需要告诉食堂大妈：“姐姐，给我加个鸡腿，再加个酸辣土豆丝，帮我浇点汁上去，多打点饭哦😋😋😋” 咦~~~ 为了多吃点，真恶心。</p><p>然后大妈帮我们打饭配菜，我们看着大妈那颤抖的手和掉落的土豆丝不禁咽了咽口水。</p><p>最终我们从大妈手中接过饭菜然后去寻找座位了…</p><p>回想一下，我们在给大妈发送需要的信息之后我们是 <strong>同步等待大妈给我配好饭菜</strong> 的，上面我们只是加了鸡腿和土豆丝，万一我再加一个番茄牛腩，韭菜鸡蛋，这样是不是大妈打饭配菜的流程就会变长，我们等待的时间也会相应的变长。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/006APoFYly1fvd9cwjlfrj30as0b03ym.jpg" alt="img"></p><p>那后来，我们工作赚钱了有钱去饭店吃饭了，我们告诉服务员来一碗牛肉面加个荷包蛋 <strong>(传达一个消息)</strong> ，然后我们就可以在饭桌上安心的玩手机了 <strong>(干自己其他事情)</strong> ，等到我们的牛肉面上了我们就可以吃了。这其中我们也就传达了一个消息，然后我们又转过头干其他事情了。这其中虽然做面的时间没有变短，但是我们只需要传达一个消息就可以干其他事情了，这是一个 <strong>异步</strong> 的概念。</p><p>所以，为了解决这一个问题，聪明的程序员在中间也加了个类似于服务员的中间件——消息队列。这个时候我们就可以把模型给改造了。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef38124f55eaea.jpg" alt="img"></p><p>这样，我们在将消息存入消息队列之后我们就可以直接返回了(我们告诉服务员我们要吃什么然后玩手机)，所以整个耗时只是 150ms + 10ms &#x3D; 160ms。</p><blockquote><p>但是你需要注意的是，整个流程的时长是没变的，就像你仅仅告诉服务员要吃什么是不会影响到做面的速度的。</p></blockquote><h4 id="解耦"><a href="#解耦" class="headerlink" title="# 解耦"></a><a href="#%E8%A7%A3%E8%80%A6">#</a> 解耦</h4><p>回到最初同步调用的过程，我们写个伪代码简单概括一下。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef381a505d3e1f.jpg" alt="img"></p><p>那么第二步，我们又添加了一个发送邮件，我们就得重新去修改代码，如果我们又加一个需求：用户购买完还需要给他加积分，这个时候我们是不是又得改代码？</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef381c4e1b1ac7.jpg" alt="img"></p><p>如果你觉得还行，那么我这个时候不要发邮件这个服务了呢，我是不是又得改代码，又得重启应用？</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef381f273a66bd.jpg" alt="img"></p><p>这样改来改去是不是很麻烦，那么 <strong>此时我们就用一个消息队列在中间进行解耦</strong> 。你需要注意的是，我们后面的发送短信、发送邮件、添加积分等一些操作都依赖于上面的 <code>result</code> ，这东西抽象出来就是购票的处理结果呀，比如订单号，用户账号等等，也就是说我们后面的一系列服务都是需要同样的消息来进行处理。既然这样，我们是不是可以通过 <strong>“广播消息”</strong> 来实现。</p><p>我上面所讲的“广播”并不是真正的广播，而是接下来的系统作为消费者去 <strong>订阅</strong> 特定的主题。比如我们这里的主题就可以叫做 <code>订票</code> ，我们购买系统作为一个生产者去生产这条消息放入消息队列，然后消费者订阅了这个主题，会从消息队列中拉取消息并消费。就比如我们刚刚画的那张图，你会发现，在生产者这边我们只需要关注 <strong>生产消息到指定主题中</strong> ，而 <strong>消费者只需要关注从指定主题中拉取消息</strong> 就行了。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef382674b66892.jpg" alt="img"></p><blockquote><p>如果没有消息队列，每当一个新的业务接入，我们都要在主系统调用新接口、或者当我们取消某些业务，我们也得在主系统删除某些接口调用。有了消息队列，我们只需要关心消息是否送达了队列，至于谁希望订阅，接下来收到消息如何处理，是下游的事情，无疑极大地减少了开发和联调的工作量。</p></blockquote><h4 id="削峰"><a href="#削峰" class="headerlink" title="# 削峰"></a><a href="#%E5%89%8A%E5%B3%B0">#</a> 削峰</h4><p>我们再次回到一开始我们使用同步调用系统的情况，并且思考一下，如果此时有大量用户请求购票整个系统会变成什么样？</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef382a9756bb1c.jpg" alt="img"></p><p>如果，此时有一万的请求进入购票系统，我们知道运行我们主业务的服务器配置一般会比较好，所以这里我们假设购票系统能承受这一万的用户请求，那么也就意味着我们同时也会出现一万调用发短信服务的请求。而对于短信系统来说并不是我们的主要业务，所以我们配备的硬件资源并不会太高，那么你觉得现在这个短信系统能承受这一万的峰值么，且不说能不能承受，系统会不会 <strong>直接崩溃</strong> 了？</p><p>短信业务又不是我们的主业务，我们能不能 <strong>折中处理</strong> 呢？如果我们把购买完成的信息发送到消息队列中，而短信系统 <strong>尽自己所能地去消息队列中取消息和消费消息</strong> ，即使处理速度慢一点也无所谓，只要我们的系统没有崩溃就行了。</p><p>留得江山在，还怕没柴烧？你敢说每次发送验证码的时候是一发你就收到了的么？</p><h4 id="消息队列能带来什么好处？"><a href="#消息队列能带来什么好处？" class="headerlink" title="# 消息队列能带来什么好处？"></a><a href="#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E8%83%BD%E5%B8%A6%E6%9D%A5%E4%BB%80%E4%B9%88%E5%A5%BD%E5%A4%84">#</a> 消息队列能带来什么好处？</h4><p>其实上面我已经说了。<strong>异步、解耦、削峰。</strong> 哪怕你上面的都没看懂也千万要记住这六个字，因为他不仅是消息队列的精华，更是编程和架构的精华。</p><h4 id="消息队列会带来副作用吗？"><a href="#消息队列会带来副作用吗？" class="headerlink" title="# 消息队列会带来副作用吗？"></a><a href="#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E4%BC%9A%E5%B8%A6%E6%9D%A5%E5%89%AF%E4%BD%9C%E7%94%A8%E5%90%97">#</a> 消息队列会带来副作用吗？</h4><p>没有哪一门技术是“银弹”，消息队列也有它的副作用。</p><p>比如，本来好好的两个系统之间的调用，我中间加了个消息队列，如果消息队列挂了怎么办呢？是不是 <strong>降低了系统的可用性</strong> ？</p><p>那这样是不是要保证HA(高可用)？是不是要搞集群？那么我 <strong>整个系统的复杂度是不是上升了</strong> ？</p><p>抛开上面的问题不讲，万一我发送方发送失败了，然后执行重试，这样就可能产生重复的消息。</p><p>或者我消费端处理失败了，请求重发，这样也会产生重复的消息。</p><p>对于一些微服务来说，消费重复消息会带来更大的麻烦，比如增加积分，这个时候我加了多次是不是对其他用户不公平？</p><p>那么，又 <strong>如何解决重复消费消息的问题</strong> 呢？</p><p>如果我们此时的消息需要保证严格的顺序性怎么办呢？比如生产者生产了一系列的有序消息(对一个id为1的记录进行删除增加修改)，但是我们知道在发布订阅模型中，对于主题是无顺序的，那么这个时候就会导致对于消费者消费消息的时候没有按照生产者的发送顺序消费，比如这个时候我们消费的顺序为修改删除增加，如果该记录涉及到金额的话是不是会出大事情？</p><p>那么，又 <strong>如何解决消息的顺序消费问题</strong> 呢？</p><p>就拿我们上面所讲的分布式系统来说，用户购票完成之后是不是需要增加账户积分？在同一个系统中我们一般会使用事务来进行解决，如果用 <code>Spring</code> 的话我们在上面伪代码中加入 <code>@Transactional</code> 注解就好了。但是在不同系统中如何保证事务呢？总不能这个系统我扣钱成功了你那积分系统积分没加吧？或者说我这扣钱明明失败了，你那积分系统给我加了积分。</p><p>那么，又如何 <strong>解决分布式事务问题</strong> 呢？</p><p>我们刚刚说了，消息队列可以进行削峰操作，那如果我的消费者如果消费很慢或者生产者生产消息很快，这样是不是会将消息堆积在消息队列中？</p><p>那么，又如何 <strong>解决消息堆积的问题</strong> 呢？</p><p>可用性降低，复杂度上升，又带来一系列的重复消费，顺序消费，分布式事务，消息堆积的问题，这消息队列还怎么用啊😵？</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef382d709abc9d.png" alt="img"></p><p>别急，办法总是有的。</p><h2 id="RocketMQ是什么？"><a href="#RocketMQ是什么？" class="headerlink" title="# RocketMQ是什么？"></a><a href="#rocketmq%E6%98%AF%E4%BB%80%E4%B9%88">#</a> RocketMQ是什么？</h2><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef383014430799.jpg" alt="img"></p><p>哇，你个混蛋！上面给我抛出那么多问题，你现在又讲 <code>RocketMQ</code> ，还让不让人活了？！🤬</p><p>别急别急，话说你现在清楚 <code>MQ</code> 的构造吗，我还没讲呢，我们先搞明白 <code>MQ</code> 的内部构造，再来看看如何解决上面的一系列问题吧，不过你最好带着问题去阅读和了解喔。</p><p><code>RocketMQ</code> 是一个 <strong>队列模型</strong> 的消息中间件，具有<strong>高性能、高可靠、高实时、分布式</strong> 的特点。它是一个采用 <code>Java</code> 语言开发的分布式的消息系统，由阿里巴巴团队开发，在2016年底贡献给 <code>Apache</code>，成为了 <code>Apache</code> 的一个顶级项目。 在阿里内部，<code>RocketMQ</code> 很好地服务了集团大大小小上千个应用，在每年的双十一当天，更有不可思议的万亿级消息通过 <code>RocketMQ</code> 流转。</p><p>废话不多说，想要了解 <code>RocketMQ</code> 历史的同学可以自己去搜寻资料。听完上面的介绍，你只要知道 <code>RocketMQ</code> 很快、很牛、而且经历过双十一的实践就行了！</p><h2 id="队列模型和主题模型"><a href="#队列模型和主题模型" class="headerlink" title="# 队列模型和主题模型"></a><a href="#%E9%98%9F%E5%88%97%E6%A8%A1%E5%9E%8B%E5%92%8C%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B">#</a> 队列模型和主题模型</h2><p>在谈 <code>RocketMQ</code> 的技术架构之前，我们先来了解一下两个名词概念——<strong>队列模型</strong> 和 <strong>主题模型</strong> 。</p><p>首先我问一个问题，消息队列为什么要叫消息队列？</p><p>你可能觉得很弱智，这玩意不就是存放消息的队列嘛？不叫消息队列叫什么？</p><p>的确，早期的消息中间件是通过 <strong>队列</strong> 这一模型来实现的，可能是历史原因，我们都习惯把消息中间件成为消息队列。</p><p>但是，如今例如 <code>RocketMQ</code> 、<code>Kafka</code> 这些优秀的消息中间件不仅仅是通过一个 <strong>队列</strong> 来实现消息存储的。</p><h3 id="队列模型"><a href="#队列模型" class="headerlink" title="# 队列模型"></a><a href="#%E9%98%9F%E5%88%97%E6%A8%A1%E5%9E%8B">#</a> 队列模型</h3><p>就像我们理解队列一样，消息中间件的队列模型就真的只是一个队列。。。我画一张图给大家理解。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef3834ae653469.jpg" alt="img"></p><p>在一开始我跟你提到了一个 <strong>“广播”</strong> 的概念，也就是说如果我们此时我们需要将一个消息发送给多个消费者(比如此时我需要将信息发送给短信系统和邮件系统)，这个时候单个队列即不能满足需求了。</p><p>当然你可以让 <code>Producer</code> 生产消息放入多个队列中，然后每个队列去对应每一个消费者。问题是可以解决，创建多个队列并且复制多份消息是会很影响资源和性能的。而且，这样子就会导致生产者需要知道具体消费者个数然后去复制对应数量的消息队列，这就违背我们消息中间件的 <strong>解耦</strong> 这一原则。</p><h3 id="主题模型"><a href="#主题模型" class="headerlink" title="# 主题模型"></a><a href="#%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B">#</a> 主题模型</h3><p>那么有没有好的方法去解决这一个问题呢？有，那就是 <strong>主题模型</strong> 或者可以称为 <strong>发布订阅模型</strong> 。</p><blockquote><p>感兴趣的同学可以去了解一下设计模式里面的观察者模式并且手动实现一下，我相信你会有所收获的。</p></blockquote><p>在主题模型中，消息的生产者称为 <strong>发布者(Publisher)</strong> ，消息的消费者称为 <strong>订阅者(Subscriber)</strong> ，存放消息的容器称为 <strong>主题(Topic)</strong> 。</p><p>其中，发布者将消息发送到指定主题中，订阅者需要 <strong>提前订阅主题</strong> 才能接受特定主题的消息。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef3837887d9a54sds.jpg" alt="img"></p><h3 id="RocketMQ中的消息模型"><a href="#RocketMQ中的消息模型" class="headerlink" title="# RocketMQ中的消息模型"></a><a href="#rocketmq%E4%B8%AD%E7%9A%84%E6%B6%88%E6%81%AF%E6%A8%A1%E5%9E%8B">#</a> RocketMQ中的消息模型</h3><p><code>RocketMQ</code> 中的消息模型就是按照 <strong>主题模型</strong> 所实现的。你可能会好奇这个 <strong>主题</strong> 到底是怎么实现的呢？你上面也没有讲到呀！</p><p>其实对于主题模型的实现来说每个消息中间件的底层设计都是不一样的，就比如 <code>Kafka</code> 中的 <strong>分区</strong> ，<code>RocketMQ</code> 中的 <strong>队列</strong> ，<code>RabbitMQ</code> 中的 <code>Exchange</code> 。我们可以理解为 <strong>主题模型&#x2F;发布订阅模型</strong> 就是一个标准，那些中间件只不过照着这个标准去实现而已。</p><p>所以，<code>RocketMQ</code> 中的 <strong>主题模型</strong> 到底是如何实现的呢？首先我画一张图，大家尝试着去理解一下。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef383d3e8c9788.jpg" alt="img"></p><p>我们可以看到在整个图中有 <code>Producer Group</code> 、<code>Topic</code> 、<code>Consumer Group</code> 三个角色，我来分别介绍一下他们。</p><ul><li><code>Producer Group</code> 生产者组： 代表某一类的生产者，比如我们有多个秒杀系统作为生产者，这多个合在一起就是一个 <code>Producer Group</code> 生产者组，它们一般生产相同的消息。</li><li><code>Consumer Group</code> 消费者组： 代表某一类的消费者，比如我们有多个短信系统作为消费者，这多个合在一起就是一个 <code>Consumer Group</code> 消费者组，它们一般消费相同的消息。</li><li><code>Topic</code> 主题： 代表一类消息，比如订单消息，物流消息等等。</li></ul><p>你可以看到图中生产者组中的生产者会向主题发送消息，而 <strong>主题中存在多个队列</strong>，生产者每次生产消息之后是指定主题中的某个队列发送消息的。</p><p>每个主题中都有多个队列(分布在不同的 <code>Broker</code>中，如果是集群的话，<code>Broker</code>又分布在不同的服务器中)，集群消费模式下，一个消费者集群多台机器共同消费一个 <code>topic</code> 的多个队列，<strong>一个队列只会被一个消费者消费</strong>。如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。就像上图中 <code>Consumer1</code> 和 <code>Consumer2</code> 分别对应着两个队列，而 <code>Consumer3</code> 是没有队列对应的，所以一般来讲要控制 <strong>消费者组中的消费者个数和主题中队列个数相同</strong> 。</p><p>当然也可以消费者个数小于队列个数，只不过不太建议。如下图。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef3850c808d707.jpg" alt="img"></p><p><strong>每个消费组在每个队列上维护一个消费位置</strong> ，为什么呢？</p><p>因为我们刚刚画的仅仅是一个消费者组，我们知道在发布订阅模式中一般会涉及到多个消费者组，而每个消费者组在每个队列中的消费位置都是不同的。如果此时有多个消费者组，那么消息被一个消费者组消费完之后是不会删除的(因为其它消费者组也需要呀)，它仅仅是为每个消费者组维护一个 <strong>消费位移(offset)</strong> ，每次消费者组消费完会返回一个成功的响应，然后队列再把维护的消费位移加一，这样就不会出现刚刚消费过的消息再一次被消费了。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef3857fefaa079.jpg" alt="img"></p><p>可能你还有一个问题，<strong>为什么一个主题中需要维护多个队列</strong> ？</p><p>答案是 <strong>提高并发能力</strong> 。的确，每个主题中只存在一个队列也是可行的。你想一下，如果每个主题中只存在一个队列，这个队列中也维护着每个消费者组的消费位置，这样也可以做到 <strong>发布订阅模式</strong> 。如下图。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef38600cdb6d4b.jpg" alt="img"></p><p>但是，这样我生产者是不是只能向一个队列发送消息？又因为需要维护消费位置所以一个队列只能对应一个消费者组中的消费者，这样是不是其他的 <code>Consumer</code> 就没有用武之地了？从这两个角度来讲，并发度一下子就小了很多。</p><p>所以总结来说，<code>RocketMQ</code> 通过<strong>使用在一个 <code>Topic</code> 中配置多个队列并且每个队列维护每个消费者组的消费位置</strong> 实现了 <strong>主题模式&#x2F;发布订阅模式</strong> 。</p><h2 id="RocketMQ的架构图"><a href="#RocketMQ的架构图" class="headerlink" title="# RocketMQ的架构图"></a><a href="#rocketmq%E7%9A%84%E6%9E%B6%E6%9E%84%E5%9B%BE">#</a> RocketMQ的架构图</h2><p>讲完了消息模型，我们理解起 <code>RocketMQ</code> 的技术架构起来就容易多了。</p><p><code>RocketMQ</code> 技术架构中有四大角色 <code>NameServer</code> 、<code>Broker</code> 、<code>Producer</code> 、<code>Consumer</code> 。我来向大家分别解释一下这四个角色是干啥的。</p><ul><li><p><code>Broker</code>： 主要负责消息的存储、投递和查询以及服务高可用保证。说白了就是消息队列服务器嘛，生产者生产消息到 <code>Broker</code> ，消费者从 <code>Broker</code> 拉取消息并消费。</p><p>这里，我还得普及一下关于 <code>Broker</code> 、<code>Topic</code> 和 队列的关系。上面我讲解了 <code>Topic</code> 和队列的关系——一个 <code>Topic</code> 中存在多个队列，那么这个 <code>Topic</code> 和队列存放在哪呢？</p><p><strong>一个 <code>Topic</code> 分布在多个 <code>Broker</code>上，一个 <code>Broker</code> 可以配置多个 <code>Topic</code> ，它们是多对多的关系</strong>。</p><p>如果某个 <code>Topic</code> 消息量很大，应该给它多配置几个队列(上文中提到了提高并发能力)，并且 <strong>尽量多分布在不同 <code>Broker</code> 上，以减轻某个 <code>Broker</code> 的压力</strong> 。</p><p><code>Topic</code> 消息量都比较均匀的情况下，如果某个 <code>broker</code> 上的队列越多，则该 <code>broker</code> 压力越大。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef38687488a5a4.jpg" alt="img"></p><blockquote><p>所以说我们需要配置多个Broker。</p></blockquote></li><li><p><code>NameServer</code>： 不知道你们有没有接触过 <code>ZooKeeper</code> 和 <code>Spring Cloud</code> 中的 <code>Eureka</code> ，它其实也是一个 <strong>注册中心</strong> ，主要提供两个功能：<strong>Broker管理</strong> 和 <strong>路由信息管理</strong> 。说白了就是 <code>Broker</code> 会将自己的信息注册到 <code>NameServer</code> 中，此时 <code>NameServer</code> 就存放了很多 <code>Broker</code> 的信息(Broker的路由表)，消费者和生产者就从 <code>NameServer</code> 中获取路由表然后照着路由表的信息和对应的 <code>Broker</code> 进行通信(生产者和消费者定期会向 <code>NameServer</code> 去查询相关的 <code>Broker</code> 的信息)。</p></li><li><p><code>Producer</code>： 消息发布的角色，支持分布式集群方式部署。说白了就是生产者。</p></li><li><p><code>Consumer</code>： 消息消费的角色，支持分布式集群方式部署。支持以push推，pull拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制。说白了就是消费者。</p></li></ul><p>听完了上面的解释你可能会觉得，这玩意好简单。不就是这样的么？</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef386c6d1e8bdb.jpg" alt="img"></p><p>嗯？你可能会发现一个问题，这老家伙 <code>NameServer</code> 干啥用的，这不多余吗？直接 <code>Producer</code> 、<code>Consumer</code> 和 <code>Broker</code> 直接进行生产消息，消费消息不就好了么？</p><p>但是，我们上文提到过 <code>Broker</code> 是需要保证高可用的，如果整个系统仅仅靠着一个 <code>Broker</code> 来维持的话，那么这个 <code>Broker</code> 的压力会不会很大？所以我们需要使用多个 <code>Broker</code> 来保证 <strong>负载均衡</strong> 。</p><p>如果说，我们的消费者和生产者直接和多个 <code>Broker</code> 相连，那么当 <code>Broker</code> 修改的时候必定会牵连着每个生产者和消费者，这样就会产生耦合问题，而 <code>NameServer</code> 注册中心就是用来解决这个问题的。</p><blockquote><p>如果还不是很理解的话，可以去看我介绍 <code>Spring Cloud</code> 的那篇文章，其中介绍了 <code>Eureka</code> 注册中心。</p></blockquote><p>当然，<code>RocketMQ</code> 中的技术架构肯定不止前面那么简单，因为上面图中的四个角色都是需要做集群的。我给出一张官网的架构图，大家尝试理解一下。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef386fa3be1e53.jpg" alt="img"></p><p>其实和我们最开始画的那张乞丐版的架构图也没什么区别，主要是一些细节上的差别。听我细细道来🤨。</p><p>第一、我们的 <code>Broker</code> <strong>做了集群并且还进行了主从部署</strong> ，由于消息分布在各个 <code>Broker</code> 上，一旦某个 <code>Broker</code> 宕机，则该<code>Broker</code> 上的消息读写都会受到影响。所以 <code>Rocketmq</code> 提供了 <code>master/slave</code> 的结构， <code>salve</code> 定时从 <code>master</code> 同步数据(同步刷盘或者异步刷盘)，如果 <code>master</code> 宕机，<strong>则 <code>slave</code> 提供消费服务，但是不能写入消息</strong> (后面我还会提到哦)。</p><p>第二、为了保证 <code>HA</code> ，我们的 <code>NameServer</code> 也做了集群部署，但是请注意它是 <strong>去中心化</strong> 的。也就意味着它没有主节点，你可以很明显地看出 <code>NameServer</code> 的所有节点是没有进行 <code>Info Replicate</code> 的，在 <code>RocketMQ</code> 中是通过 <strong>单个Broker和所有NameServer保持长连接</strong> ，并且在每隔30秒 <code>Broker</code> 会向所有 <code>Nameserver</code> 发送心跳，心跳包含了自身的 <code>Topic</code> 配置信息，这个步骤就对应这上面的 <code>Routing Info</code> 。</p><p>第三、在生产者需要向 <code>Broker</code> 发送消息的时候，<strong>需要先从 <code>NameServer</code> 获取关于 <code>Broker</code> 的路由信息</strong>，然后通过 <strong>轮询</strong> 的方法去向每个队列中生产数据以达到 <strong>负载均衡</strong> 的效果。</p><p>第四、消费者通过 <code>NameServer</code> 获取所有 <code>Broker</code> 的路由信息后，向 <code>Broker</code> 发送 <code>Pull</code> 请求来获取消息数据。<code>Consumer</code> 可以以两种模式启动—— <strong>广播（Broadcast）和集群（Cluster）</strong>。广播模式下，一条消息会发送给 <strong>同一个消费组中的所有消费者</strong> ，集群模式下消息只会发送给一个消费者。</p><h2 id="如何解决-顺序消费、重复消费"><a href="#如何解决-顺序消费、重复消费" class="headerlink" title="# 如何解决 顺序消费、重复消费"></a><a href="#%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3-%E9%A1%BA%E5%BA%8F%E6%B6%88%E8%B4%B9%E3%80%81%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9">#</a> 如何解决 顺序消费、重复消费</h2><p>其实，这些东西都是我在介绍消息队列带来的一些副作用的时候提到的，也就是说，这些问题不仅仅挂钩于 <code>RocketMQ</code> ，而是应该每个消息中间件都需要去解决的。</p><p>在上面我介绍 <code>RocketMQ</code> 的技术架构的时候我已经向你展示了 <strong>它是如何保证高可用的</strong> ，这里不涉及运维方面的搭建，如果你感兴趣可以自己去官网上照着例子搭建属于你自己的 <code>RocketMQ</code> 集群。</p><blockquote><p>其实 <code>Kafka</code> 的架构基本和 <code>RocketMQ</code> 类似，只是它注册中心使用了 <code>Zookeeper</code> 、它的 <strong>分区</strong> 就相当于 <code>RocketMQ</code> 中的 <strong>队列</strong> 。还有一些小细节不同会在后面提到。</p></blockquote><h3 id="顺序消费"><a href="#顺序消费" class="headerlink" title="# 顺序消费"></a><a href="#%E9%A1%BA%E5%BA%8F%E6%B6%88%E8%B4%B9">#</a> 顺序消费</h3><p>在上面的技术架构介绍中，我们已经知道了 <strong><code>RocketMQ</code> 在主题上是无序的、它只有在队列层面才是保证有序</strong> 的。</p><p>这又扯到两个概念——<strong>普通顺序</strong> 和 <strong>严格顺序</strong> 。</p><p>所谓普通顺序是指 消费者通过 <strong>同一个消费队列收到的消息是有顺序的</strong> ，不同消息队列收到的消息则可能是无顺序的。普通顺序消息在 <code>Broker</code> <strong>重启情况下不会保证消息顺序性</strong> (短暂时间) 。</p><p>所谓严格顺序是指 消费者收到的 <strong>所有消息</strong> 均是有顺序的。严格顺序消息 <strong>即使在异常情况下也会保证消息的顺序性</strong> 。</p><p>但是，严格顺序看起来虽好，实现它可会付出巨大的代价。如果你使用严格顺序模式，<code>Broker</code> 集群中只要有一台机器不可用，则整个集群都不可用。你还用啥？现在主要场景也就在 <code>binlog</code> 同步。</p><p>一般而言，我们的 <code>MQ</code> 都是能容忍短暂的乱序，所以推荐使用普通顺序模式。</p><p>那么，我们现在使用了 <strong>普通顺序模式</strong> ，我们从上面学习知道了在 <code>Producer</code> 生产消息的时候会进行轮询(取决你的负载均衡策略)来向同一主题的不同消息队列发送消息。那么如果此时我有几个消息分别是同一个订单的创建、支付、发货，在轮询的策略下这 <strong>三个消息会被发送到不同队列</strong> ，因为在不同的队列此时就无法使用 <code>RocketMQ</code> 带来的队列有序特性来保证消息有序性了。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef3874585e096e.jpg" alt="img"></p><p>那么，怎么解决呢？</p><p>其实很简单，我们需要处理的仅仅是将同一语义下的消息放入同一个队列(比如这里是同一个订单)，那我们就可以使用 <strong>Hash取模法</strong> 来保证同一个订单在同一个队列中就行了。</p><h3 id="重复消费"><a href="#重复消费" class="headerlink" title="# 重复消费"></a><a href="#%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9">#</a> 重复消费</h3><p>emmm，就两个字—— <strong>幂等</strong> 。在编程中一个<em>幂等</em> 操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。比如说，这个时候我们有一个订单的处理积分的系统，每当来一个消息的时候它就负责为创建这个订单的用户的积分加上相应的数值。可是有一次，消息队列发送给订单系统 FrancisQ 的订单信息，其要求是给 FrancisQ 的积分加上 500。但是积分系统在收到 FrancisQ 的订单信息处理完成之后返回给消息队列处理成功的信息的时候出现了网络波动(当然还有很多种情况，比如Broker意外重启等等)，这条回应没有发送成功。</p><p>那么，消息队列没收到积分系统的回应会不会尝试重发这个消息？问题就来了，我再发这个消息，万一它又给 FrancisQ 的账户加上 500 积分怎么办呢？</p><p>所以我们需要给我们的消费者实现 <strong>幂等</strong> ，也就是对同一个消息的处理结果，执行多少次都不变。</p><p>那么如何给业务实现幂等呢？这个还是需要结合具体的业务的。你可以使用 <strong>写入 <code>Redis</code></strong> 来保证，因为 <code>Redis</code> 的 <code>key</code> 和 <code>value</code> 就是天然支持幂等的。当然还有使用 <strong>数据库插入法</strong> ，基于数据库的唯一键来保证重复数据不会被插入多条。</p><p>不过最主要的还是需要 <strong>根据特定场景使用特定的解决方案</strong> ，你要知道你的消息消费是否是完全不可重复消费还是可以忍受重复消费的，然后再选择强校验和弱校验的方式。毕竟在 CS 领域还是很少有技术银弹的说法。</p><p>而在整个互联网领域，幂等不仅仅适用于消息队列的重复消费问题，这些实现幂等的方法，也同样适用于，<strong>在其他场景中来解决重复请求或者重复调用的问题</strong> 。比如将HTTP服务设计成幂等的，<strong>解决前端或者APP重复提交表单数据的问题</strong> ，也可以将一个微服务设计成幂等的，解决 <code>RPC</code> 框架自动重试导致的 <strong>重复调用问题</strong> 。</p><h2 id="分布式事务"><a href="#分布式事务" class="headerlink" title="# 分布式事务"></a><a href="#%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1">#</a> 分布式事务</h2><p>如何解释分布式事务呢？事务大家都知道吧？<strong>要么都执行要么都不执行</strong> 。在同一个系统中我们可以轻松地实现事务，但是在分布式架构中，我们有很多服务是部署在不同系统之间的，而不同服务之间又需要进行调用。比如此时我下订单然后增加积分，如果保证不了分布式事务的话，就会出现A系统下了订单，但是B系统增加积分失败或者A系统没有下订单，B系统却增加了积分。前者对用户不友好，后者对运营商不利，这是我们都不愿意见到的。</p><p>那么，如何去解决这个问题呢？</p><p>如今比较常见的分布式事务实现有 2PC、TCC 和事务消息(half 半消息机制)。每一种实现都有其特定的使用场景，但是也有各自的问题，<strong>都不是完美的解决方案</strong>。</p><p>在 <code>RocketMQ</code> 中使用的是 <strong>事务消息加上事务反查机制</strong> 来解决分布式事务问题的。我画了张图，大家可以对照着图进行理解。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef38798d7a987f.png" alt="img"></p><p>在第一步发送的 half 消息 ，它的意思是 <strong>在事务提交之前，对于消费者来说，这个消息是不可见的</strong> 。</p><blockquote><p>那么，如何做到写入消息但是对用户不可见呢？RocketMQ事务消息的做法是：如果消息是half消息，将备份原消息的主题与消息消费队列，然后 <strong>改变主题</strong> 为RMQ_SYS_TRANS_HALF_TOPIC。由于消费组未订阅该主题，故消费端无法消费half类型的消息，<strong>然后RocketMQ会开启一个定时任务，从Topic为RMQ_SYS_TRANS_HALF_TOPIC中拉取消息进行消费</strong>，根据生产者组获取一个服务提供者发送回查事务状态请求，根据事务状态来决定是提交或回滚消息。</p></blockquote><p>你可以试想一下，如果没有从第5步开始的 <strong>事务反查机制</strong> ，如果出现网路波动第4步没有发送成功，这样就会产生 MQ 不知道是不是需要给消费者消费的问题，他就像一个无头苍蝇一样。在 <code>RocketMQ</code> 中就是使用的上述的事务反查来解决的，而在 <code>Kafka</code> 中通常是直接抛出一个异常让用户来自行解决。</p><p>你还需要注意的是，在 <code>MQ Server</code> 指向系统B的操作已经和系统A不相关了，也就是说在消息队列中的分布式事务是——<strong>本地事务和存储消息到消息队列才是同一个事务</strong>。这样也就产生了事务的<strong>最终一致性</strong>，因为整个过程是异步的，<strong>每个系统只要保证它自己那一部分的事务就行了</strong>。</p><h2 id="消息堆积问题"><a href="#消息堆积问题" class="headerlink" title="# 消息堆积问题"></a><a href="#%E6%B6%88%E6%81%AF%E5%A0%86%E7%A7%AF%E9%97%AE%E9%A2%98">#</a> 消息堆积问题</h2><p>在上面我们提到了消息队列一个很重要的功能——<strong>削峰</strong> 。那么如果这个峰值太大了导致消息堆积在队列中怎么办呢？</p><p>其实这个问题可以将它广义化，因为产生消息堆积的根源其实就只有两个——生产者生产太快或者消费者消费太慢。</p><p>我们可以从多个角度去思考解决这个问题，当流量到峰值的时候是因为生产者生产太快，我们可以使用一些 <strong>限流降级</strong> 的方法，当然你也可以增加多个消费者实例去水平扩展增加消费能力来匹配生产的激增。如果消费者消费过慢的话，我们可以先检查 <strong>是否是消费者出现了大量的消费错误</strong> ，或者打印一下日志查看是否是哪一个线程卡死，出现了锁资源不释放等等的问题。</p><blockquote><p>当然，最快速解决消息堆积问题的方法还是增加消费者实例，不过 <strong>同时你还需要增加每个主题的队列数量</strong> 。</p><p>别忘了在 <code>RocketMQ</code> 中，<strong>一个队列只会被一个消费者消费</strong> ，如果你仅仅是增加消费者实例就会出现我一开始给你画架构图的那种情况。</p></blockquote><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef387d939ab66d.jpg" alt="img"></p><h2 id="回溯消费"><a href="#回溯消费" class="headerlink" title="# 回溯消费"></a><a href="#%E5%9B%9E%E6%BA%AF%E6%B6%88%E8%B4%B9">#</a> 回溯消费</h2><p>回溯消费是指 <code>Consumer</code> 已经消费成功的消息，由于业务上需求需要重新消费，在<code>RocketMQ</code> 中， <code>Broker</code> 在向<code>Consumer</code> 投递成功消息后，<strong>消息仍然需要保留</strong> 。并且重新消费一般是按照时间维度，例如由于 <code>Consumer</code> 系统故障，恢复后需要重新消费1小时前的数据，那么 <code>Broker</code> 要提供一种机制，可以按照时间维度来回退消费进度。<code>RocketMQ</code> 支持按照时间回溯消费，时间维度精确到毫秒。</p><p>这是官方文档的解释，我直接照搬过来就当科普了😁😁😁。</p><h2 id="RocketMQ-的刷盘机制"><a href="#RocketMQ-的刷盘机制" class="headerlink" title="# RocketMQ 的刷盘机制"></a><a href="#rocketmq-%E7%9A%84%E5%88%B7%E7%9B%98%E6%9C%BA%E5%88%B6">#</a> RocketMQ 的刷盘机制</h2><p>上面我讲了那么多的 <code>RocketMQ</code> 的架构和设计原理，你有没有好奇</p><p>在 <code>Topic</code> 中的 <strong>队列是以什么样的形式存在的？</strong></p><p><strong>队列中的消息又是如何进行存储持久化的呢？</strong></p><p>我在上文中提到的 <strong>同步刷盘</strong> 和 <strong>异步刷盘</strong> 又是什么呢？它们会给持久化带来什么样的影响呢？</p><p>下面我将给你们一一解释。</p><h3 id="同步刷盘和异步刷盘"><a href="#同步刷盘和异步刷盘" class="headerlink" title="# 同步刷盘和异步刷盘"></a><a href="#%E5%90%8C%E6%AD%A5%E5%88%B7%E7%9B%98%E5%92%8C%E5%BC%82%E6%AD%A5%E5%88%B7%E7%9B%98">#</a> 同步刷盘和异步刷盘</h3><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef387fba311cda.jpg" alt="img"></p><p>如上图所示，在同步刷盘中需要等待一个刷盘成功的 <code>ACK</code> ，同步刷盘对 <code>MQ</code> 消息可靠性来说是一种不错的保障，但是 <strong>性能上会有较大影响</strong> ，一般地适用于金融等特定业务场景。</p><p>而异步刷盘往往是开启一个线程去异步地执行刷盘操作。消息刷盘采用后台异步线程提交的方式进行， <strong>降低了读写延迟</strong> ，提高了 <code>MQ</code> 的性能和吞吐量，一般适用于如发验证码等对于消息保证要求不太高的业务场景。</p><p>一般地，<strong>异步刷盘只有在 <code>Broker</code> 意外宕机的时候会丢失部分数据</strong>，你可以设置 <code>Broker</code> 的参数 <code>FlushDiskType</code> 来调整你的刷盘策略(ASYNC_FLUSH 或者 SYNC_FLUSH)。</p><h3 id="同步复制和异步复制"><a href="#同步复制和异步复制" class="headerlink" title="# 同步复制和异步复制"></a><a href="#%E5%90%8C%E6%AD%A5%E5%A4%8D%E5%88%B6%E5%92%8C%E5%BC%82%E6%AD%A5%E5%A4%8D%E5%88%B6">#</a> 同步复制和异步复制</h3><p>上面的同步刷盘和异步刷盘是在单个结点层面的，而同步复制和异步复制主要是指的 <code>Borker</code> 主从模式下，主节点返回消息给客户端的时候是否需要同步从节点。</p><ul><li>同步复制： 也叫 “同步双写”，也就是说，<strong>只有消息同步双写到主从节点上时才返回写入成功</strong> 。</li><li>异步复制： <strong>消息写入主节点之后就直接返回写入成功</strong> 。</li></ul><p>然而，很多事情是没有完美的方案的，就比如我们进行消息写入的节点越多就更能保证消息的可靠性，但是随之的性能也会下降，所以需要程序员根据特定业务场景去选择适应的主从复制方案。</p><p>那么，<strong>异步复制会不会也像异步刷盘那样影响消息的可靠性呢？</strong></p><p>答案是不会的，因为两者就是不同的概念，对于消息可靠性是通过不同的刷盘策略保证的，而像异步同步复制策略仅仅是影响到了 <strong>可用性</strong> 。为什么呢？其主要原因<strong>是 <code>RocketMQ</code> 是不支持自动主从切换的，当主节点挂掉之后，生产者就不能再给这个主节点生产消息了</strong>。</p><p>比如这个时候采用异步复制的方式，在主节点还未发送完需要同步的消息的时候主节点挂掉了，这个时候从节点就少了一部分消息。但是此时生产者无法再给主节点生产消息了，<strong>消费者可以自动切换到从节点进行消费</strong>(仅仅是消费)，所以在主节点挂掉的时间只会产生主从结点短暂的消息不一致的情况，降低了可用性，而当主节点重启之后，从节点那部分未来得及复制的消息还会继续复制。</p><p>在单主从架构中，如果一个主节点挂掉了，那么也就意味着整个系统不能再生产了。那么这个可用性的问题能否解决呢？<strong>一个主从不行那就多个主从的呗</strong>，别忘了在我们最初的架构图中，每个 <code>Topic</code> 是分布在不同 <code>Broker</code> 中的。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef38687488a5a4.jpg" alt="img"></p><p>但是这种复制方式同样也会带来一个问题，那就是无法保证 <strong>严格顺序</strong> 。在上文中我们提到了如何保证的消息顺序性是通过将一个语义的消息发送在同一个队列中，使用 <code>Topic</code> 下的队列来保证顺序性的。如果此时我们主节点A负责的是订单A的一系列语义消息，然后它挂了，这样其他节点是无法代替主节点A的，如果我们任意节点都可以存入任何消息，那就没有顺序性可言了。</p><p>而在 <code>RocketMQ</code> 中采用了 <code>Dledger</code> 解决这个问题。他要求在写入消息的时候，要求<strong>至少消息复制到半数以上的节点之后</strong>，才给客⼾端返回写⼊成功，并且它是⽀持通过选举来动态切换主节点的。这里我就不展开说明了，读者可以自己去了解。</p><blockquote><p>也不是说 <code>Dledger</code> 是个完美的方案，至少在 <code>Dledger</code> 选举过程中是无法提供服务的，而且他必须要使用三个节点或以上，如果多数节点同时挂掉他也是无法保证可用性的，而且要求消息复制半数以上节点的效率和直接异步复制还是有一定的差距的。</p></blockquote><h3 id="存储机制"><a href="#存储机制" class="headerlink" title="# 存储机制"></a><a href="#%E5%AD%98%E5%82%A8%E6%9C%BA%E5%88%B6">#</a> 存储机制</h3><p>还记得上面我们一开始的三个问题吗？到这里第三个问题已经解决了。</p><p>但是，在 <code>Topic</code> 中的 <strong>队列是以什么样的形式存在的？队列中的消息又是如何进行存储持久化的呢？</strong> 还未解决，其实这里涉及到了 <code>RocketMQ</code> 是如何设计它的存储结构了。我首先想大家介绍 <code>RocketMQ</code> 消息存储架构中的三大角色——<code>CommitLog</code> 、<code>ConsumeQueue</code> 和 <code>IndexFile</code> 。</p><ul><li><code>CommitLog</code>： <strong>消息主体以及元数据的存储主体</strong>，存储 <code>Producer</code> 端写入的消息主体内容,消息内容不是定长的。单个文件大小默认1G ，文件名长度为20位，左边补零，剩余为起始偏移量，比如00000000000000000000代表了第一个文件，起始偏移量为0，文件大小为1G&#x3D;1073741824；当第一个文件写满了，第二个文件为00000000001073741824，起始偏移量为1073741824，以此类推。消息主要是<strong>顺序写入日志文件</strong>，当文件满了，写入下一个文件。</li><li><code>ConsumeQueue</code>： 消息消费队列，<strong>引入的目的主要是提高消息消费的性能</strong>(我们再前面也讲了)，由于<code>RocketMQ</code> 是基于主题 <code>Topic</code> 的订阅模式，消息消费是针对主题进行的，如果要遍历 <code>commitlog</code> 文件中根据 <code>Topic</code> 检索消息是非常低效的。<code>Consumer</code> 即可根据 <code>ConsumeQueue</code> 来查找待消费的消息。其中，<code>ConsumeQueue</code>（逻辑消费队列）<strong>作为消费消息的索引</strong>，保存了指定 <code>Topic</code> 下的队列消息在 <code>CommitLog</code> 中的**起始物理偏移量 <code>offset</code> *<em>，消息大小 <code>size</code> 和消息 <code>Tag</code> 的 <code>HashCode</code> 值。*</em><code>consumequeue</code> 文件可以看成是基于 <code>topic</code> 的 <code>commitlog</code> 索引文件**，故 <code>consumequeue</code> 文件夹的组织方式如下：topic&#x2F;queue&#x2F;file三层组织结构，具体存储路径为：$HOME&#x2F;store&#x2F;consumequeue&#x2F;{topic}&#x2F;{queueId}&#x2F;{fileName}。同样 <code>consumequeue</code> 文件采取定长设计，每一个条目共20个字节，分别为8字节的 <code>commitlog</code> 物理偏移量、4字节的消息长度、8字节tag <code>hashcode</code>，单个文件由30W个条目组成，可以像数组一样随机访问每一个条目，每个 <code>ConsumeQueue</code>文件大小约5.72M；</li><li><code>IndexFile</code>： <code>IndexFile</code>（索引文件）提供了一种可以通过key或时间区间来查询消息的方法。这里只做科普不做详细介绍。</li></ul><p>总结来说，整个消息存储的结构，最主要的就是 <code>CommitLoq</code> 和 <code>ConsumeQueue</code> 。而 <code>ConsumeQueue</code> 你可以大概理解为 <code>Topic</code> 中的队列。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef3884c02acc72.png" alt="img"></p><p><code>RocketMQ</code> 采用的是 <strong>混合型的存储结构</strong> ，即为 <code>Broker</code> 单个实例下所有的队列共用一个日志数据文件来存储消息。有意思的是在同样高并发的 <code>Kafka</code> 中会为每个 <code>Topic</code> 分配一个存储文件。这就有点类似于我们有一大堆书需要装上书架，<code>RockeMQ</code> 是不分书的种类直接成批的塞上去的，而 <code>Kafka</code> 是将书本放入指定的分类区域的。</p><p>而 <code>RocketMQ</code> 为什么要这么做呢？原因是 <strong>提高数据的写入效率</strong> ，不分 <code>Topic</code> 意味着我们有更大的几率获取 <strong>成批</strong> 的消息进行数据写入，但也会带来一个麻烦就是读取消息的时候需要遍历整个大文件，这是非常耗时的。</p><p>所以，在 <code>RocketMQ</code> 中又使用了 <code>ConsumeQueue</code> 作为每个队列的索引文件来 <strong>提升读取消息的效率</strong>。我们可以直接根据队列的消息序号，计算出索引的全局位置（索引序号*索引固定⻓度20），然后直接读取这条索引，再根据索引中记录的消息的全局位置，找到消息。</p><p>讲到这里，你可能对 <code>RockeMQ</code> 的存储架构还有些模糊，没事，我们结合着图来理解一下。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/16ef388763c25c62.jpg" alt="img"></p><p>emmm，是不是有一点复杂🤣，看英文图片和英文文档的时候就不要怂，硬着头皮往下看就行。</p><blockquote><p>如果上面没看懂的读者一定要认真看下面的流程分析！</p></blockquote><p>首先，在最上面的那一块就是我刚刚讲的你现在可以直接 **把 <code>ConsumerQueue</code> 理解为 <code>Queue</code>**。</p><p>在图中最左边说明了红色方块代表被写入的消息，虚线方块代表等待被写入的。左边的生产者发送消息会指定 <code>Topic</code> 、<code>QueueId</code> 和具体消息内容，而在 <code>Broker</code> 中管你是哪门子消息，他直接 <strong>全部顺序存储到了 CommitLog</strong>。而根据生产者指定的 <code>Topic</code> 和 <code>QueueId</code> 将这条消息本身在 <code>CommitLog</code> 的偏移(offset)，消息本身大小，和tag的hash值存入对应的 <code>ConsumeQueue</code> 索引文件中。而在每个队列中都保存了 <code>ConsumeOffset</code> 即每个消费者组的消费位置(我在架构那里提到了，忘了的同学可以回去看一下)，而消费者拉取消息进行消费的时候只需要根据 <code>ConsumeOffset</code> 获取下一个未被消费的消息就行了。</p><p>上述就是我对于整个消息存储架构的大概理解(这里不涉及到一些细节讨论，比如稀疏索引等等问题)，希望对你有帮助。</p><p>因为有一个知识点因为写嗨了忘讲了，想想在哪里加也不好，所以我留给大家去思考🤔🤔一下吧。</p><p><img src="https://my-blog-to-use.oss-cn-beijing.aliyuncs.com/2019-11/e314ee45gy1g05zgr67bbj20gp0b3aba.jpg" alt="img"></p><p>为什么 <code>CommitLog</code> 文件要设计成固定大小的长度呢？提醒：<strong>内存映射机制</strong>。</p><h2 id="总结"><a href="#总结" class="headerlink" title="# 总结"></a><a href="#%E6%80%BB%E7%BB%93">#</a> 总结</h2><p>总算把这篇博客写完了。我讲的你们还记得吗😅？</p><p>这篇文章中我主要想大家介绍了</p><ol><li>消息队列出现的原因</li><li>消息队列的作用(异步，解耦，削峰)</li><li>消息队列带来的一系列问题(消息堆积、重复消费、顺序消费、分布式事务等等)</li><li>消息队列的两种消息模型——队列和主题模式</li><li>分析了 <code>RocketMQ</code> 的技术架构(<code>NameServer</code> 、<code>Broker</code> 、<code>Producer</code> 、<code>Comsumer</code>)</li><li>结合 <code>RocketMQ</code> 回答了消息队列副作用的解决方案</li><li>介绍了 <code>RocketMQ</code> 的存储机制和刷盘策略。</li></ol><p>等等。。。</p><blockquote><p>如果喜欢可以点赞哟👍👍👍。</p></blockquote></div><footer class="post-footer"><div class="post-eof"></div></footer></article><nav class="pagination"><a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="上一页"></i></a> <a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="下一页"></i></a></nav></div><script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">enpsl</p><div class="site-description" itemprop="description">my blog</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">50</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">9</span> <span class="site-state-item-name">标签</span></a></div></nav></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2023</span><span class="with-love"><i class="fa fa-heart"></i></span> <span class="author" itemprop="copyrightHolder">enpsl</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动</div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script></body></html>